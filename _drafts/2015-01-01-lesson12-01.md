---
layout: slide
title: Lesson 12 - SVM - Decision Trees - Random forests
description: none
transition: slide
permalink: /12-decision-trees-random-forests.html
theme: white
---
<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>12. Support Vector Machines, Decision trees, Random Forests</p>
</section>

<section data-markdown>
# Lesson 12
## LEARNING OBJECTIVES

* SVM
* Decision Trees
* Random Forests


* Project 2 - Caravan Dataset => Tuesday 8/2

* [Final Project part 2 and 3](https://github.com/alexperrier/gads/blob/master/final_project/Final%20Project%20-%20Design%20Writeup%20and%20Data%20Exploration.md) - Design Writeup and Exploratory Data Analysis
For => Tuesday 8/9

</section>

<!-- Prework and review -->
<section  data-background-color="#DA0A13">
    <h1>Lesson #12</h1>
    <p class = 'big_title'>Review of Lesson 11</p>
</section>


<section data-markdown>
# Last Lesson Review

* Accuracy Paradox
* Imbalanced datasets strategies
    * 3 strategies we saw?

</section>


<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Part I Support Vector Machines</p>
</section>

<section data-markdown>
# Hyperplane

Classification separating an hyperplane

* 1D => point
* 2D => line
* 3D => surface
* etc ...

A hyperplane in 2 dimension is defined by a line equation \\( \beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0  \\)


In **p dimensions** a hyperplane is defined by a **linear** equation $$\beta_0 + \beta_1 X_1 + ... + \beta_p X_p = 0 \qquad  p \gt 0 $$

</section>

<section data-markdown>

# Linearly separable

We have n samples in p dimensions/features  \\( X = \\{ x_{i}  \\}  \quad i \in [1,n]  \\).

These observations belong to two classes \\( y_i \in \\{-1, +1 \\} \\)

The classes are **linearly separable**: there is an  hyperplane that fully separates the points according to their classes.

* All \\( x\_i  \\) such that \\( \qquad \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j}  < 0  \qquad \\) belong to class -1
* All \\( x\_i^\prime  \\) such that  \\( \qquad \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j}^\prime  > 0  \qquad \\) belong to class +1

</section>

<section data-markdown>
# Linear decision boundary
We predict the class of a new point \\( x \\)

by calculating
$$ f(x) = \beta\_0+ \sum\_{j=1}^{n} \beta\_j x\_{j}  $$

* if \\( f(x) >0 => +1 \\)
* if \\( f(x) <0 => -1 \\)

A classifier that is based on a separating hyperplane has a **linear decision boundary**.

</section>

<section data-markdown>
# One line

Note that since \\( y\_i \\) and  \\( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j}  \\) have the same sign:

$$ y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) > 0 $$

</section>

<section data-markdown>
# Infinity of Hyperplanes

If our data is linearly separable then there exists an infinity of hyperplanes that can separate it

![](assets/11/hyperplane-classification.png)


=> We need to introduce a margin to separate the classes

Fig from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/getbook.html)
</section>

<section data-markdown>
# Maximal Margin Classifier

We want to find the Hyperplane that will maximise the distance to all the points.
Best separation of the classes

![Maximal Margin Classifier](assets/11/Maximal_Margin_Classifier.png)

Fig from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/getbook.html)
</section>

<section data-markdown>
# Maximal Margin Classifier

Fnding a simple linear boundary is equivalent to solving
$$ y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) > 0 \quad \forall i \in [1,..,n] $$

Finding **the largest margin** that separates the classes is equivalent to solving

$$ \max\_{\beta\_i} M \quad \text{such that} \quad y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) > M \quad \forall i \in [1,..,n] $$

with \\( \sum\_{j=0}^{p}  \beta^2\_j = 1\\)

</section>

<section data-markdown>
# Support Vectors

* The observations that are on the margin lines are called **support vectors**

* They *support* the maximal margin hyperplane in the sense that if these points were moved slightly then the **maximal margin hyperplane** would move as well.

* The maximal margin hyperplane depends directly on the support vectors, but not on the other observations.
    A movement to any of the other observations would not affect the separating hyperplane, provided that the observationâ€™s movement does not cause it to cross the boundary set by the margin.

A change in the support vector impacts the margin a lot => **over fitting**

</section>

<section data-markdown>
# Support Vector Classifier

What if we allow some points to be either wrongly classified or at least within the margin boundaries?

### Non linearly separable
This is the case when the classes are not separable, we still want the best margin possible. But some points will be on the wrong side

Add a tuning parameter C that dictates the *severity of the violation* of the margin.

The bigger C is the more points are **within the margin** or **misclassified**. C is some budget for violation of the margin which is determined during cross validation.


=> Even in the case of linearly separable classes adding some flexibility to the margin will make the classfier more robust, more flexible (less overfitting)


</section>

<section data-markdown>
# Support Vector Classifier

\\( C\\) is a non negative tuning parameter

$$ \max\_{\beta\_i, \epsilon\_i} M \quad \text{such that} \quad y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) > M - \epsilon\_i \quad \forall i \in [1,..,n]  $$
with

* \\( \sum\_j  \beta^2\_j = 1 \\)
* \\(   \epsilon\_i > 0 \\)
* \\( \sum\_{i} \epsilon\_i \leq C  \\)

</section>

<section data-markdown>
# LAB SVC

[L12 N1 Support Vector Classifier - Linear Case.py](https://github.com/alexperrier/gads/blob/master/12_decision_trees/py/L12%20Support%20Vector%20Classifier%20-%20Linear.ipynb)

</section>

<section data-markdown>
# Support Vector Machine

What if the data is not even close to be linearly separable?

![](assets/11/non_linearly_separable.png)

Fig from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/getbook.html)

</section>

<section data-markdown>
# Introducing Kernels

The optimization equation for the linear support vector classifier can be rewritten as such

$$ f(x) =  \beta\_0 + \sum\_{i=1}^{n} \alpha\_i \langle x, x\_i \rangle   $$
$$ f(x) =  \beta\_0 + \sum\_{i=1}^{n} \alpha\_i K(x,x\_i)    $$

with \\( K(x,x\_i) = \langle x, x\_i \rangle \\) the vector dot product.

But we could use a different Kernel function


**Note**:This involves \\(p * \frac{n(n-1)}{2} \\) multiplications! However only the support vectors are useful. So we can limit the sum to \\(S\\) support vectors.


</section>

<section data-markdown>
# Kernels

Here are some classic Kernel functions

* Linear Kernel \\( \quad K(x,x_i) = \langle x, x_i \rangle\\)
* Polynomial Kernel (d): \\( \quad K(x,x\_i) =  (1 + \sum\_{j=1}^{p} x\_{j}x\_{i,j} )^d   \\)
* Radial Kernel \\( \quad K(x,x\_i) =   \exp(-\gamma    \sum\_{j=1}^{p} (x\_{j} - x\_{i,j})^2 )   \\)

</section>
<section data-markdown>
# SVM Lab
[L12 N2 Support Vector Classifier - Non-Linear Case.py](https://github.com/alexperrier/gads/blob/master/12_decision_trees/py/L12%20Support%20Vector%20Machines.ipynb)

</section>

<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Part II Decision Tree</p>
</section>


<section data-markdown>
# Decision Trees

Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.

Rule based models

        if x < a
            if y < c
                ...
            else
                ...
        elif x > a & a < b
            ...
        else
            if z < d
                ...
            else
                0

</section>

<section data-markdown>
# Iris dataset

![](assets/12/L12-tree-iris.png)

</section>

<section data-markdown>
# Decision Trees

* Simple to understand and to interpret. Trees can be visualised.
* Requires little data preparation. (missing values, scaling, dummy variables, ...)
* Low  cost for prediction \\( O(log(n)) \\) with n number of data points used to train the tree.
* Can handle both numerical and categorical data.
* Uses a white box model. An observed situation can simply be explained by boolean logic.
* Possible to validate a model using statistical tests.
* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.

</section>


<section data-markdown>
# Decision Trees


* high overfitting for over-complex trees that do not generalise the data well.
* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated.
* no globally optimal decision tree

</section>




<section data-markdown>
# Lab: Controlling the tree

Set these params to control the tree complexity

* max_depth
* min_samples_split
* min_samples_leaf
* max_features


Lab: [Simple Decision tree](https://github.com/alexperrier/gads/blob/master/12_decision_trees/py/L12%20Simple%20Decision%20Tree%20-%20Iris%20dataset.ipynb)

Weak Learner: What if we forcefully prune the tree?

</section>

<section data-markdown>
# Classification vs Regression

Different Metric

* MSE

* Gini

</section>


<section data-markdown>
# Bootstrap aggregation aka Bagging

We used Boostrapping to estimate the mean of a sample.

Sampling with replacement.

The idea is the same with trees or any other classifier.

</section>


<section data-markdown>
# Bagging for trees

* Generate B different bootstrapped training data sets.
* Train a new tree on each training set

The predictions of all the trees are averaged

=> significantly reduces over fitting

</section>


<section data-markdown>
# Out Of Bag - OOB

When boostrapping, in each experiment will use only approx. 2/3rd of the available samples.

Which leaves 1/3rd that we can use to estimate the validation error of each tree.

This is called OOB Out of Bag error.

It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error.

</section>


<section data-markdown>
# Feature importance

* When the *max_features < total number of features*.

    => Some features are left out of the splitting decision in each node.

* Relative Feature importance can be deduced from the delta in MSE associated to the features included vs left out.

</section>


<section data-markdown>
# Bagging in scikit

Use the [Bagging Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)

A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.

</section>


<section data-markdown>
# Bagging in scikit
Very flexible

        bagging = BaggingClassifier(DecisionTreeClassifier(),  boostrap = True)


class sklearn.ensemble.BaggingClassifier

* base_estimator: The base estimator, decision tree by default
* n_estimators: How many estimators will be ensembled
* max_samples: The number of samples to draw from X to train each base estimator
* max_features: The number of features to draw from X to train each base estimator
* bootstrap: True / False
* bootstrap_features: True / False
* oob_score: True / False

</section>


<section data-markdown>
# Bagging in scikit

[Lab - bagging with Tree - regression](https://github.com/alexperrier/gads/blob/master/12_decision_trees/py/L12%20Bagging.ipynb)

</section>


<section data-markdown>
# Random Forests

Extension of the bootstrapping to features

* In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.

* In addition, when splitting a node during the construction of the tree, the split that is picked is the best split among **a random subset of the features**.

=> The bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.

(see RandomForestClassifier and RandomForestRegressor classes),

</section>


<section data-markdown>
# Random Forest Lab

[Random Forests Lab](https://github.com/alexperrier/gads/blob/master/12_decision_trees/py/L12%20Random%20Forests.ipynb)
</section>
<section data-markdown>
# Recap

* Support Vector Classifiers
* Decision Trees
* Bagging
* Random Forests


* Project 2 for next Tuesday
* Final Project part 2 and 3 for 8/9


</section>
<section data-markdown>
# Questions

### Questions?

</section>
