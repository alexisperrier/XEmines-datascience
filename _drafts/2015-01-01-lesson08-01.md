---
layout: slide
title: Lesson 08 - Classification - K-Nearest Neighbor
description: none
transition: slide
permalink: /08-classification-knn.html
theme: white
---
<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>8. Classification, KNN, Curse of dimensionality, Scaling, Grid search, PCA, Iris, Digits  </p>
</section>

<section data-markdown>
# Lesson 8
## LEARNING OBJECTIVES

* Classification
* KNN
* Optimizing meta parameters with Grid search
* Visualization of higher dimensions
* Curse of dimensionality

</section>

<!-- Prework and review -->
<section  data-background-color="#DA0A13">
    <h1>Lesson #8</h1>
    <p class = 'big_title'>Review of Lesson 7</p>
</section>


<section data-markdown>
# Last Lesson Review

* Bias - Variance decomposition
* Cross validation
* Sampling, Boostrapping
* Strategies to deal with Overfitting and Underfitting

</section>

<section data-markdown>
# Last session
### Any Questions?

* How to reduce Bias / Underfitting?
* How to reduce Variance / Overfitting?

Note:
Exit tickets
</section>

<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Classification</p>
</section>


<section data-markdown>
# Classification

### Regression
*  Predict Continuous values

### Classification
Predict a finite set of values aka labels

* Binary classification (sex of babies, survival on Titanic)
* Multiclass classification (user segmentation, flower type Iris dataset )
* Ordinal classification: classes are ordered (Grades of students, stars in user reviews, ...)

</section>


<section data-markdown>
# Regression or Classification?
Review the following situations and decide if each one is a regression problem, classification problem, or neither:

* Using the total number of explosions in a movie, predict if the movie is by JJ Abrams or Michael Bay.
* Determine how many tickets will be sold to a concert given who is performing, where, and the date and time.
* Given the temperature over the last year by day, predict tomorrow's temperature outside.
* Using data from four cell phone microphones, reduce the noisy sounds so the voice is crystal clear to the receiving phone.
* With customer data, determine if a user will return or not in the next 7 days to an e-commerce website.

</section>

<section data-markdown>
# Regression to Classification

How can regression models be used for classification?

</section>

<section data-markdown>
# Regression to Classification

How can regression models be used for classification?

=> Using Threshold(s)

        if probability < 0.5:
            return A
        else:
            return B

</section>

<section data-markdown>
# Let's build a classifier!

[Binary classifier](http://localhost:8889/notebooks/08_classification_knn/py/L8%20Binary%20Classifier%20on%20Iris.ipynb) to differentiate between class 0 and other classes

* Load the iris dataset and Boxplot the petal length and the petal width
* Write a simple function that returns 0 or 2 depending on the Petal length and a threshold
* Write a function that returns the ratio of properly predicted samples vs the total number of samples
* For different thresholds calculate the number of estimated positives (1s) over the true number of samples in that class

What happens to the accuracy if you were to look at sepal length for your simple classifier? Try it.

How would you use this simple classifier to expand to 3 classes

</section>

<section data-markdown>
# K Nearest Neighbors

You have a set of samples already labeled

1. For each new sample, calculate the *distance* to **all** other points.
2. Given those distances, pick the **K** closest samples.
2. Count the number of samples in each class
4. The new sample is classified as the class label with the largest number ("votes").

</section>

<section data-markdown>
# K Nearest Neighbors

![](assets/08/apples-oranges-k3.png)
![](assets/08/apples-oranges-k5.png)
</section>

<section data-markdown>
# Demo: K Nearest Neighbors


* Load the iris dataset
* Shuffle it
* Split 100 samples for training and 50 for testing
* See how the accuracy on the test set evolves with K ranging from 2 to 20
* Plot the training error and the test error vs K

        from sklearn import datasets, neighbors, metrics
        import pandas as pd

        iris = datasets.load_iris()
        knn = neighbors.KNeighborsClassifier(n_neighbors=K)
        knn.fit(iris.data, iris.target)
        ...
        metrics.accuracy_score(y_test, y_hat )

</section>

<section data-markdown>
# Your turn

Plot Accuracy for training and test error as a function of K

* set the seed
* Load iris dataset and shuffle it
* split X and y in train / test dataset (100, 50 samples)
* (loop over K) for each K
    * Initialize the classifier with K
    * Fit
    * Predict
    * print K and the accuracy score

[Notebook 1 - solution]()

</section>

<section data-markdown>
# Grid search: Searching for estimator parameters

How do you find the best hyperparameter for your model?

* \\(\alpha\\) for Ridge or Lasso
* K for K-NN
* ...

[Grid search](http://scikit-learn.org/stable/modules/grid_search.html) and [Randomized Search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn.grid_search.RandomizedSearchCV) allow you to specify a parameter space and find which value(s) of the hyperparameter(s) results in the best performance of your model.

</section>
<section data-markdown>
# Grid search: Searching for estimator parameters

### [Metric](http://scikit-learn.org/stable/modules/grid_search.html#specifying-an-objective-metric)
* Accuracy for classification
* \\(R^2\\) for regression
* Specify another metric via the *scoring* function

</section>

<section data-markdown>
# K-NN + Grid search

Now Find K with Grid Search on Fraudulent dataset

* fraud: cases
* student: is the person a student?
* balance and income


### Walkthrough
* knn = neighbors.KNeighborsClassifier()
* parameters = {'n_neighbors' : np.arange(2,20) }
* clf = GridSearchCV(knn, parameters)
* clf.best_estimator_,  clf.best_params_, clf.best_score_

</section>
<section data-markdown>
# K-NN + Grid search
### Your turn
* Fraud as function of balance
* Looking at the accuracy, what's the best K?
* What's happening here?

</section>
<section data-markdown>
# Other KNN hyper parameters

by default

**KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')**

</section>
<section data-markdown>
# Minkowski Distance

for 2 points  \\(   X=(x_1,x_2,\ldots,x_n)\text{ and }Y=(y_1,y_2,\ldots,y_n) \in \mathbb{R}^n\\)

The Minkowski distance is defined by $$ d(X,Y) = \left(\sum_{i=1}^n |x_i-y_i|^p\right)^{1/p}  \text{ with } p \geq 1 $$

* For p = 1:[Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry)
$$   d(X,Y) = \sum_{i=1}^n |x_i-y_i| $$

* For p = 2: [Euclidian distance](https://en.wikipedia.org/wiki/Euclidean_distance)
$$ d(X,Y) =  \sqrt{\sum_{i=1}^n (x_i-y_i)^2}. $$

</section>
<section data-markdown>
# Grid search with K and distance

* parameters = {'n_neighbors' : np.arange(2,20), 'p': np.linspace(1,2,5) }
* What's the best distance parameter p, what's the best K?
* What's the next best score parameters?

</section>

<section data-markdown>
# Pre processing the data with scikit


[preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html)

* scaling
* normalizing
* remove linear correlation with PCA
* binarization
* encoding categorical features
* missing values

with

* fit
* transform

</section>

<section data-markdown>
# Scaling and centering

Let's scale and normalize the income and balance and see if that helps

</section>

<section data-markdown>
# Make classification

[make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification)

n_features = n_informative + n_redundant + n_repeated + noisy feature

* what's the influence of n_redundant
* what's the influence of noisy features,
* does repeating features help? why?
* what's the catch?

</section>


<section data-markdown>
# Dimensionality reduction with PCA
### Principal Component Analysis

Problem: Can't visualize n Dimensions

Goal: Project the data space to a 2 dimension space to visualize it

PCA: orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components

* [by Sebastian Raschka](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)
* [In scikit](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)

</section>

<section data-markdown>
# Visualize Higher Dimensions Datasets
Let's use the [digits](http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html) dataset in scikit

        digits = datasets.load_digits()
        X = digits.data
        y = digits.target

* Dimension of the feature space?
* use PCA to project on 2 dimensions

        XX = pca.fit(X).transform(X)

* Plot with color as a target

        plt.scatter(XX[:,0], XX[:,1], c = y, cmap = plt.cm.coolwarm)

[L8 N4 PCA](http://localhost:8889/edit/08_classification_knn/py/L8%20N4%20PCA.py)
</section>

<section data-markdown>

![the curse](assets/08/cod_thecurse.png)
![of](assets/08/cod_of.png)

![The Curse](assets/08/cod_dimensionality.png)

</section>

<section data-markdown>
# Curse of dimensionality

* Create a set a classifications set with 100 samples and 5 classes
* Use only significant features (n_features = n_significant)
* Increase the number of features from 4 to 25

Plot the error vs the number of dimension.

What do you get?

Why?

[L8 N5 - Curse of Dimensionality](http://localhost:8889/edit/08_classification_knn/py/l8_N5_curse.py)
</section>
<section data-markdown>
# Curse of dimensionality

![curse](assets/08/the_curse_of_dimensionality.png)

The more dimensions the more it is difficult for the classifier to measure the distance significantly

</section>

<section data-markdown>
# Curse of dimensionality

You want your training dataset to cover 20% of the range of the feature space

* in 1D you need 20% of the population
* in 2D you need 45% of the population ( \\(0.45^2 = 0.2 \\) )
* in 3D you need 58% of the population ( \\(0.58^3 = 0.2 \\) ) => overfitting

[![](assets/08/curse_1to3.png)](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)

</section>
<section data-markdown>
# Curse of dimensionality

Sample density drops, data gets scarce, distance between points is less discriminative

Let's say your features all range from 0 to 5 (5 units). and you have 10 samples

* in 1D, density = 10 / 5 = 2 samples per unit
* in 2D, density = 10 / 25 = 0.5 samples per unit
* in 3D, density = 10 / 125 = 0.08 samples per unit

</section>

<section data-markdown>
# KNN

* Predict probability
* KNN for regression


http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html
</section>


<section data-markdown>
# Links

* [K-NN mathematicalmonk's channel](https://www.youtube.com/watch?v=4ObVzTuFivY&index=6&list=PLD0F06AA0D2E8FFBA)

* [Implementing your own k-nearest neighbour algorithm using Python](https://blog.cambridgecoding.com/2016/01/16/machine-learning-under-the-hood-writing-your-own-k-nearest-neighbour-algorithm/)

* [Centering, scaling and k-Nearest Neighbours](https://www.datacamp.com/community/tutorials/the-importance-of-preprocessing-in-data-science-and-the-machine-learning-pipeline-i-centering-scaling-and-k-nearest-neighbours)

* [Scikit Classification Example](http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py)

* [Scikit Nearest Neighbors](http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors)

* [Classification in Python with scikit-learn](http://nbviewer.jupyter.org/urls/s3.amazonaws.com/datarobotblog/notebooks/classification_in_python.ipynb)

* [The Curse of Dimensionality in classification](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)

</section>