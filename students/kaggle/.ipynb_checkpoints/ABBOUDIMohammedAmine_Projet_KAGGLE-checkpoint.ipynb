{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Kaggle: House Price par ABBOUDI Mohammed Amine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Librairies a importer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import skew,norm\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, Lasso, SGDRegressor, Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR,SVR\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lecture des donnees\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.drop(columns = 'Id', inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"train: {} \\ntest: {})\".format(train.shape,test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le volume de donnees de train et de test sont presque equivalents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_columns', None):\n",
    "    display(train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque la presence de quelque outliers, il faudra plotter ces variables susceptibles afin de conclure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(x = train['1stFlrSF'],y = train.SalePrice, c='Black')\n",
    "plt.title('1stFlrSF', size = 15)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(x = train.TotalBsmtSF,y = train.SalePrice, c='red')\n",
    "plt.title('TotalBsmtSF', size = 15)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(x = train.GrLivArea,y = train.SalePrice, c='green')\n",
    "plt.title('GrLivArea', size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppression des outliers\n",
    "train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice']<300000)].index,inplace = True)\n",
    "train.drop(train[train['TotalBsmtSF'] > 5000].index,inplace = True)\n",
    "train.drop(train[train['1stFlrSF'] > 4000].index,inplace = True)\n",
    "#Taille apres suppresion\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a elimine 2 observations seulements, cela peut etre du au fait que plusieurs variables partagent le meme point aberrant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = train.corr()\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns,\n",
    "        cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Heatmap des variables les plus correlees. On choisit un threshold de 0.75\n",
    "zoomCorr = corr.loc[['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea'], ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea']]\n",
    "f , ax = plt.subplots(figsize = (14,12))\n",
    "plt.title('Correlation des variables numeriques',size=15)\n",
    "sns.heatmap(zoomCorr, square = True, linewidths=0.01, vmax=0.75, annot=True,cmap='viridis', linecolor=\"white\", annot_kws = {'size':12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -GarageCars et GarageArea sont tres correlees (0.89)\n",
    "    -SalePrice avec OverallQual (0.8)\n",
    "    -TotalBsmtSF avec 1stFlrSF d'ou l'idee de creer une nouvelle variable qui combine toutes les surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppression de GarageCars\n",
    "\n",
    "train.drop(columns = 'GarageCars', inplace =True)\n",
    "test.drop(columns = 'GarageCars', inplace =True)\n",
    "print(\"train: {} \\ntest: {})\".format(train.shape,test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=train['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par visualiser les colonnes numeriques avec des valeurs manquantes a l'aide de la librairie missingno , Merci MAJDOUBI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenation du train et test sets\n",
    "df = pd.concat([train,test],ignore_index=True)\n",
    "df.drop(['Id','SalePrice'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Variables numeriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(df.select_dtypes(include=[np.number]).sample(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(test.select_dtypes(include=[np.number]).sample(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les memes trois variables du testing et training sets ont des valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=[np.number]).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remplacement par la moyenne de la variable MasVnrArea\n",
    "df['MasVnrArea'].fillna(df['MasVnrArea'].mean(), inplace=True)\n",
    "#Puisuqe YearBuilt et GarageYrBlt sont tres correlees on remplace toute observation manquante par YearBlt correspondente\n",
    "df['GarageYrBlt'].fillna(df['YearBuilt'], inplace=True)\n",
    "\n",
    "for col in ['BsmtFullBath','BsmtHalfBath','BsmtUnfSF','TotalBsmtSF','BsmtFinSF2','BsmtFinSF1','GarageArea']:\n",
    "    df[col].fillna(0,inplace= True)\n",
    "# Remplacement de LotFrontage par la mediane en la groupant avec Neighborhood puisqu'elles sont tres correlees\n",
    "df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(\n",
    "lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Variables Categorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(df.select_dtypes(include=[np.object]).sample(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(test.select_dtypes(include=[np.object]).sample(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour une grande partie des variables categorielles, une valeur manquante signifie que la maison ne possede pas cette propriete, comme pour PoolQC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=[np.object]).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# On les remplace par 'None'\n",
    "for col in ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageQual','GarageCond','GarageFinish','GarageType','BsmtExposure','BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1','MasVnrType']:\n",
    "    df[col].fillna('None',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les autres on remplace par la valeur la plus frequente. donne par la fonction mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in ['MSZoning','Functional','Utilities','KitchenQual','SaleType','Exterior2nd','Exterior1st','Electrical']:\n",
    "    df[col].fillna(df[col].mode()[0],inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, le dataset n'a plus de valeurs manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing de Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une grande partie des variables numeriques, tel que YrBuilt ou MSSubClass n'ont pas de raison pour etre percu par le model comme numeriques, il faut dont y remedier en les rendant de type object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['YrSold','YearRemodAdd','YearBuilt','MoSold','MSSubClass','GarageYrBlt']\n",
    "for col in cols:\n",
    "    df[col]=df[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'un autre cote plusieurs variables categorielles ont un sense hierarchique, un rating Excellent n'est pas la meme chose qu'un rating Poor, il est donc important de remedier cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"ExterQual_\"] = df.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "df[\"BsmtQual_\"] = df.BsmtQual.map({'None':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "df[\"BsmtExposure_\"] = df.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n",
    "df[\"HeatingQC_\"] = df.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "df[\"KitchenQual_\"] = df.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "df[\"FireplaceQu_\"] = df.FireplaceQu.map({'None':1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n",
    "df[\"GarageFinish_\"] = df.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n",
    "df[\"PavedDrive_\"] = df.PavedDrive.map({'N':1, 'P':2, 'Y':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ajout de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['TotalSF'] = df['1stFlrSF'] + df['2ndFlrSF'] + df['TotalBsmtSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()\n",
    "# La ditribution est skewed des deux cotes, un peu plus du coté droit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "sns.distplot(train.SalePrice,fit=norm)\n",
    "\n",
    "# On peut le voir plus clairement ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skewness = pd.DataFrame({'Skew' :df[df.dtypes[df.dtypes != \"object\"].index].apply(lambda x : skew (x.dropna())).sort_values(ascending=False)})\n",
    "skewness = skewness[abs(skewness) > 1] # On prend 0.75 comme threshold\n",
    "print (\"{} variables necessitent une transformation.\".format(skewness.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SalesPrices plot with three different fitted distributions\n",
    "plt.figure(1); plt.title('Johnson')\n",
    "sns.distplot(y, kde=False, fit=stats.johnsonsu)\n",
    "plt.figure(2); plt.title('Normale')\n",
    "sns.distplot(y, kde=False, fit=stats.norm)\n",
    "plt.figure(3); plt.title('Log-Normale')\n",
    "sns.distplot(y, kde=False, fit=stats.lognorm)\n",
    "\n",
    "# Je n'ai pas pu trouver le fit pour une transformation box cox, pour cela je vais utiliser \n",
    "# la transformation Normale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_trans = np.log(y)\n",
    "skewness = df.select_dtypes(include=[np.number]).apply(lambda x: skew(x))\n",
    "skewness_features = skewness[abs(skewness) >= 1].index\n",
    "df[skewness_features] = np.log1p(df[skewness_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalisation des Variables Numeriques\n",
    "df_scaled= df\n",
    "cols = ['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BedroomAbvGr', 'BsmtFinSF1',\n",
    "        'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'EnclosedPorch', \n",
    "        'Fireplaces', 'FullBath', 'GarageArea', 'GrLivArea',\n",
    "        'HalfBath', 'KitchenAbvGr', 'LotArea', 'LotFrontage', 'LowQualFinSF',\n",
    "        'MasVnrArea', 'MiscVal', 'OpenPorchSF', 'OverallCond', 'OverallQual',\n",
    "        'PoolArea', 'ScreenPorch', 'TotRmsAbvGrd', 'TotalBsmtSF', 'WoodDeckSF','TotalSF']\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "df_scaled[cols] = robust_scaler.fit(df[cols]).transform(df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodage des variables categorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "df_scaled['YrSold']=labelencoder.fit_transform(df_scaled['YrSold'])\n",
    "df_scaled['YearRemodAdd']=labelencoder.fit_transform(df_scaled['YearRemodAdd'])\n",
    "df_scaled['YearBuilt']=labelencoder.fit_transform(df_scaled['YearBuilt'])\n",
    "df_scaled['MoSold']=labelencoder.fit_transform(df_scaled['MoSold'])\n",
    "df_scaled['GarageYrBlt']=labelencoder.fit_transform(df_scaled['GarageYrBlt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_scaled = pd.get_dummies(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_scaled[:train.shape[0]]\n",
    "X_test = df_scaled[train.shape[0]:]\n",
    "\n",
    "print(\"train: {} \\ntest: {} \\ny: {}\".format(X_train.shape,X_test.shape,y_trans.shape))\n",
    "np.isnan(X_test.values).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des Variables les plus importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilisation du model de regression XGBoost pour la detection des variables les plus importantes\n",
    "\n",
    "model = xgboost.XGBRegressor(colsample_bytree=0.4,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.07,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=10000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,y_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OrderedDict(sorted(model._Booster.get_fscore().items(), key=lambda t: t[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_vars= list( dict((k, v) for k, v in model._Booster.get_fscore().items() if v >= 10).keys())\n",
    "print(best_vars)\n",
    "X_train = X_train[best_vars]\n",
    "X_test = X_test[best_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement de Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSLE (y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "# Fonction de Cross Validation\n",
    "\n",
    "def RMSLE_CV(model):\n",
    "    kf = KFold(5, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_trans, scoring=\"neg_mean_squared_error\",\n",
    "cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# creating the models\n",
    "models = [\n",
    "             SVR(),\n",
    "             xgboost.XGBRegressor(loss='huber', learning_rate=0.05, n_estimators=3000,min_samples_split=10, min_samples_leaf=15,max_depth=4,random_state=5,max_features='sqrt'),\n",
    "             GradientBoostingRegressor(),\n",
    "             RandomForestRegressor(),\n",
    "             Lasso(alpha=0.01,max_iter=10000),\n",
    "             Ridge(),\n",
    "             BayesianRidge(),\n",
    "             lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11),\n",
    "             ElasticNet(),\n",
    "             ElasticNet(alpha = 0.001,max_iter=10000),  \n",
    "             ]\n",
    "\n",
    "names = ['Support vector regression','XGBoost','Gradient boosting','Random Forest','Custom Lasso','Ridge','Bayesian Ridge','LightGBM','Elastic Net Regularization','Elastic Net Regularization Custom']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Perform 5-folds cross-calidation to evaluate the models \n",
    "for model, name in zip(models, names):\n",
    "    # Root mean square error\n",
    "    score = RMSLE_CV(model)\n",
    "    print(\"- {} : moyenne : {:.4f}, ecart-type : {:4f}\".format(name, score.mean(),score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class gridSearch():\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "    def grid_get(self,param_grid):\n",
    "        grid_search = GridSearchCV(self.model,param_grid,cv=5,scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train,y_trans)\n",
    "        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n",
    "        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])\n",
    "        print('\\nBest parameters : {}, best score : {}'.format(grid_search.best_params_,np.sqrt(-grid_search.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gridSearch(ElasticNet()).grid_get(\n",
    "        {'alpha':[0.006,0.0065,0.007,0.0075,0.008],'l1_ratio':[0.070,0.075,0.080,0.085,0.09,0.095],'max_iter':[10000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gridSearch(BayesianRidge()).grid_get(\n",
    "        {'alpha_1':[0.01,0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009],'n_iter':[100000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gridSearch(SVR()).grid_get(\n",
    "        {'C':[13,15,17,19,21],'kernel':['rbf'],'gamma':[0.0005,0.001,0.002,0.01],'epsilon':[0.01,0.02,0.03,0.1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gridSearch(Lasso()).grid_get(\n",
    "       {'alpha':[0.01,0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009],'max_iter':[10000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gridSearch(GradientBoostingRegressor()).grid_get(\n",
    "       {'learning_rate':[0.05,0.1,0.15,0.025,0.012],'n_estimators':[1000,2000,3000,4000,5000,6000],'loss':['ls', 'lad', 'huber', 'quantile']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit les meilleurs parametres de tous ces modeles, et definit les versions a utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elsnt = ElasticNet(alpha=0.006,l1_ratio=0.07,max_iter=100000)\n",
    "bayes = BayesianRidge(alpha_1 = 0.0001, n_iter = 100000)\n",
    "svr = SVR(C = 13, epsilon= 0.03, gamma = 0.001, kernel = 'rbf')\n",
    "lasso = Lasso(alpha= 0.0005, max_iter= 100000)\n",
    "GBoost = GradientBoostingRegressor(loss='huber', learning_rate=0.05, n_estimators=3000,\n",
    "                                   min_samples_split=10, min_samples_leaf=15,max_depth=4,\n",
    "                                   random_state=5,max_features='sqrt')\n",
    "lgbm = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # Clonage des modeles afin de les fitter\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Entrainement des modeles\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # On predit le resultat et on prend la moyenne de tous les modeles\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)\n",
    "    \n",
    "\n",
    "model_final = AveragingModels(models = (elsnt, bayes, lasso, GBoost))\n",
    "\n",
    "score = RMSLE_CV(model_final)\n",
    "print(\" La moyenne des modeles est: {:.4f}\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final.fit(X_train.values, y_trans) \n",
    "y_train_pred = model_final.predict(X_train.values)\n",
    "print(\"Score du modele sur le train set:\") \n",
    "print(RMSLE(y_trans,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = pd.read_csv(\"test.csv\")['Id']\n",
    "submission['SalePrice'] = np.exp(model_final.predict(X_test.values))\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
