# Cours de data science - School of Industrial Management - Emines - UM6P - Sept. 2018

Cours appliqué d'analyse prédictive supervisée



Jour 1 : Intro, data science et régression linéaire

Présentation du programme, de la compétition Kaggle, du travail attendu
Mise en place de l'environnement de travail (Jupyter notebook) et des outils (python, scikit-learn, numpy, pandas, ...),
Pratiques python, et fonctionnalités pandas
Qu'est-ce que la data science ?
Data Science, Machine Learning, Intelligence Artificielle, Deep Learning,
Finalités et étapes d'un projet,
Supervisée ou non-supervisée,
Régression ou classification.
Jeux de données classiques
Après-midi :
LAB : Régression(s) sur des datasets simples
Régression linéaire,
Régression logistique,
Conditions sur les données, cas d'application, p-value, R^2, intervalles de confiance, ...

Jour 2 : Modèles ou Algorithmes
Matin :
Pratique du maximum de vraisemblance
Importance de la linéarité : définition et tests statistiques associés
Impact de la multi-colinéarité
Régression polynomiale
Après-midi :
Présentation du projet final : compétition Kaggle.
LAB : On continue sur les différents types de régression avec des datasets non linéaires
Jour 3 : Data visualisation et python

Matin :
Visualisation des données, principaux graphs, matplotlib, pyplot
Best practice en python pour la data science; structure de données avec pandas.
Après-midi :
LAB: travaux pratiques de visualisation d’un dataset

Jour 4 : Gradient stochastique
Matin :
Recap : l'analyse prédictive supervisée : Modèle ou Algorithme, minimisation d'une fonction de coût, score, prédicteurs et outcome, ...
Découpage en test, train validation ; validation croisée
Algorithme du gradient stochastique, théorie, pratique et applications
Après-midi :
LAB : Gradient stochastique, convergence, paramétrisation, visualisation, scikit-learn, batch mode, conditions
Cas : L’utilisation du gradient stochastique dans le service AWS ML
Jour 5 : Arbres et Forêts
Matin :
Arbre de décisions, forêts aléatoires, XGBoost
Ensembling et Boosting
Score de classification : matrice de confusion, AUC, F1, ...
Après-midi :
Présentation d’un cas : classification d’échantillons de musique électronique pour Splice.com
LAB : forêts aléatoires et XGBoost
Jour 6 : Support Vector Machines et overfitting
Matin :
Support Vector Machines avec différents kernels
Overfitting, compromis biais-variance, régularisations L1 et L2, Ridge et Lasso
Courbes d’apprentissage : détecter et corriger l'overfitting
Après-midi :
LAB : SVM
LAB : Régularisation
Jour 7 : Création et sélection des variables
Matin :
Modèle bayésien naïf
Feature engineering approche manuelle, brute ou approche bayésienne
Améliorer les variables : Box-Cox, valeurs manquantes ou aberrantes
Bootstrapping
Sélection des variables
Curse of dimensionality

Après-midi :
LAB : nettoyage des données, feature engineering et sélection
Jour 8 : Classification déséquilibrée
Matin :
Le paradoxe de la précision
Méthodes de sous-échantillonnage et de sur-échantillonnage
SMOTE
Après-midi :
Cas : prédiction de résiliation dans l’assurance
LAB : sur dataset caravan
Point projet
Jour 9 :Séries temporelles
Matin :
Exponential smoothing
Modèles linéaires AR, MA, ARIMA
Décomposition en tendance, saisonnalité et résidus
Après-midi :
Cas : Prédiction de la demande pour un producteur d'électroménager
LAB sur séries temporelles
Jour 10 : Présentations des projets
Matin : session ouverte à l’initiative des étudiants, questions / réponses, intro aux outils cloud AWS ou Google.
Après-midi : Présentations des projets étudiants
