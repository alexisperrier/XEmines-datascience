<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Emines - UM6P - Data Science</title>
    <description>Cours de data science.
</description>
    <link>http://localhost:4001/</link>
    <atom:link href="http://localhost:4001/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 10 Sep 2018 16:20:28 -0400</pubDate>
    <lastBuildDate>Mon, 10 Sep 2018 16:20:28 -0400</lastBuildDate>
    <generator>Jekyll v3.7.2</generator>
    
      <item>
        <title>Biais, variance et gradient stochastique</title>
        <description>&lt;section&gt;
&lt;div style=&quot;display: table; height: 100px; width: 80%; text-align: center; border: 1px solid   #ccc; margin:auto; margin-top: 50px; box-shadow: 5px 10px 8px #888888; &quot;&gt;
&lt;div style=&quot;margin: 100px 0 100px 0 ;&quot;&gt;
&lt;p style=&quot;font-size:44px;&quot;&gt; Biais, variance et gradient stochastique&lt;/p&gt;
&lt;p style=&quot;font-size:28px;&quot;&gt; avec scikit-learn&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* Régression logistique
* Approche statistique ou approche Machine Learning
* Metrique de classification:
    * Matrice de confusion
    * ROC-AUC
* One hot encoding
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    ## Lab:
    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:40%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Aujourd'hui

* Scikit-learn
* Biais - Variance
* Stochastic Gradient Descent
* Outliers, detection and impact
* Skewness: Box cox and Kurtosis

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;

    * [scikit-learn.org](http://scikit-learn.org/stable/)
    * [Eco-système](http://scikit-learn.org/stable/related_projects.html)

    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;

# Scikit-learn

* Open source, 2010
* Basé sur numpy, scipy, …
* Olivier Grisel, Andreas Muller, Gael Varoquaux, Jake Vanderplas
* [INRIA](http://www.inria.fr), [Telecom ParisTech ](http://www.telecom-paristech.fr/), NYU

### Algorithmes et modèles:
* Classification: Identifying to which category an object belongs to.
    * Applications: Spam detection, Image recognition.
    * Algorithms: SVM, nearest neighbors, random forest, ...

* Regression: Predicting a continuous-valued attribute associated with an object.
    * Applications: Drug response, Stock prices.
    *  Algorithms: SVR, ridge regression, Lasso, ...

* Clustering: Automatic grouping of similar objects into sets.
    * Applications: Customer segmentation, Grouping experiment outcomes
    * Algorithms: k-Means, spectral clustering, mean-shift, ...
* Dimensionality reduction: Reducing the number of random variables to consider.
    * Applications: Visualization, Increased efficiency
    * Algorithms: PCA, feature selection, non-negative matrix factorization.
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;

    * [scikit-learn.org](http://scikit-learn.org/stable/)
    * [Eco-système](http://scikit-learn.org/stable/related_projects.html)

    &lt;/div&gt;
&lt;/div&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# suite
* Model selection: Comparing, validating and choosing parameters and models.
    * Goal: Improved accuracy via parameter tuning
    * Modules: grid search, cross validation, metrics.

* Preprocessing: Feature extraction and normalization.
    * Application: Transforming input data such as text for use with machine learning algorithms.
    * Modules: preprocessing, feature extraction.

* Fantastic documentation, plein d'exemples
* Input: Images, text, numerique, ...

&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# scikit-learn API
Could not be simpler

1. Instancier un modèle
Par exemple une regression linéaire:
    * ```mdl = linear_model.LinearRegression( meta-params, loss)```
2. Train le modèle
    * ```mdl.fit(X, y)```
3. Faire des predictions sur de nouvelles données
    * ```y_hat = mdl.predict(Some New Data)```.
    * ```y_hat = mdl.predict_proba(Some New Data)```
4. Obtenir un score
    * ```mdl.score(X,y)```

&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# scikit-learn model input

* les meta parametres du modele: \\(\alpha, \epsilon, \beta, \cdots \\) *learning_rate*
* la regularization: *penalty, l1, l2*
* la fonction de cout: *loss*
* gestion des iterations: *max_iter, n_iter*
* data processing: *normalize, shuffle*

[![Lin reg params](/assets/04/scikit_linear_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)
[![SGD reg params](/assets/04/scikit_sgd_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Demo

[linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Part II Biais - Variance

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Bias Variance decomposition
The prediction error of your model can be decomposed in 2 terms:

* The bias
* The Variance

Total Error = Bias Error + Variance Error

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Bias Variance decomposition

**Error due to Bias**: Error due to bias is taken as the difference between the average prediction of our model and the correct value which we are trying to predict.

Bias measures how far off in general your models' predictions are from the correct value.

**Error due to Variance**: The error due to variance is taken as the variability of a model prediction for a given data point.

The variance is how much the predictions for a given point vary between different realizations of the model.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Bias Variance decomposition
![Bias-variance](assets/04/bias-variance-targets.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Bias Variance and overfitting
![Bias-variance](assets/04/bias_Variance_Under_overfitting.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Bias Variance decomposition
L'erreur quadritique est définie par:

$$ \operatorname{MSE}( \hat{y} ) = \frac {1}{n} \sum_{i=1}^{n}( \hat{y_i}-y_i )^2  $$

que l'on peut réecrire de la façon suivante:

$$ \operatorname{MSE}( \hat{y} )= \mathbb{E} \big[ (\hat{y} - y)^2   \big]  =\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] + \left(\mathbb{E}(\hat{y})-y\right)^2 $$

Soit
$$ \operatorname{MSE}( \hat{y} )= \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2 $$

où

* \\( \operatorname{Var}(\hat{y}) =  \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] \\)
* \\( \operatorname{Bias}(\hat{y},y)  = \mathbb{E}(\hat{y})-y  \\)
&lt;/section&gt;

&lt;section&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Le calcul
&lt;/div&gt;
$$
\begin{align} \mathbb{E}((\hat{y}-y)^2)&amp;amp;=
 \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})+\mathbb{E}(\hat{y})-y\right)^2\right]
\\ &amp;amp; =
\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2 +2\left((\hat{y}-\mathbb{E}(\hat{y}))(\mathbb{E}(\hat{y})-y)\right)+\left( \mathbb{E}(\hat{y})-y \right)^2\right]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\mathbb{E}\Big[(\hat{y}-\mathbb{E}(\hat{y}))(\overbrace{\mathbb{E}(\hat{y})-y}^{\begin{smallmatrix} \text{This is} \\  \text{a constant,} \\ \text{so it can be} \\  \text{pulled out.} \end{smallmatrix}}) \,\Big] + \mathbb{E}\Big[\,\overbrace{\left(\mathbb{E}(\hat{y})-y\right)^2}^{\begin{smallmatrix} \text{This is a} \\  \text{constant, so its} \\  \text{expected value} \\  \text{is itself.} \end{smallmatrix}}\,\Big]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\underbrace{\mathbb{E}(\hat{y}-\mathbb{E}(\hat{y}))}_{=\mathbb{E}(\hat{y})-\mathbb{E}(\hat{y})=0}(\mathbb{E}(\hat{y})-y)+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2
\end{align}

$$
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
![underfit - overfit](/assets/04/underfit_overfit.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
Pour réduire l'erreur quadratique, il faut minimiser à la fois le **biais** et la **variance**.

* Biais important &amp;lt;-&amp;gt; sous estimation - Underfitting
    * L'estimateur, le modele est pas bon
    * Mauvais score

* Variance importante &amp;lt;-&amp;gt; Overfitting
    * Le modele est trop **sensible** au training dataset
    * pouvoir d'extrapolation faible, peu de capacité à etre appliqué sur des nouvelles données.
    * comment détecter l'overfit ?

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# split: train vs test dataset

* On entraine le modele sur un **dataset de training**
* Mais le but est d'avoir un modele capable de traiter des **données nouvelles**, données qu'il n'a pas vu auparavant
* et on veut surtout éviter **l'Overfit**: *Le modele est trop **sensible** au training dataset*

Donc on va mettre de coté  une partie des données comme données nouvelles: dataset de test.
&amp;lt;img src=/assets/04/split_train_test.png style='width: 500px;  margin:auto; ' &amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# train - test split

&amp;gt; Demo sur iris dataset

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Comment detecter l'overfit - learning curves

* set aside a test set
* train your model on increasing number of training samples
* for each model, calculate the training error and the test error

[scikit example 1](http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve)

[scikit example 2](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py)

The learning curve traces the error on training and the error on the validation set as the sample sizes increases.

* For small sample sizes, the model does not have enough data to learn and both the training error and the testing error are high.

* As the sample size increases the learning rate decreases. The model is learning the data. And the testing error is also decreasing. The model becomes a better predictor.

* As you increase the sample size, there comes a point where the training error keeps on decreasing but the testing error stops decreasing and may instead starts increasing.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# detecter l'overfit -

# TODO
Demo sur ames housing avec SGD

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Que faire si on a peu de données

test - train split ne laisse pas assez de données pour un bon train du modele

=&amp;gt; validation croisée

k-fold

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# K-fold


# K-fold cross validation

Train, valid, test: You're wasting a lot of data

### [K-fold cross validation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)

![K-fold cross validation](assets/04/k-fold-cross-validation.png)


1. Split your data into train (80%) / test (20%), leave the test alone

2. Then further split your training set on K subsets (K = 4)
    * train on 1,2,3, validate on 4
    * train on 1,2,4, validate on 3
    * train on 1,3,4, validate on 2
    * train on 2,3,4, validate on 1

The average of the errors obtained on the validations subsets is a better estimation on the performance of your model than if you had just one validation set.
&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# cross validation
Many other ways to do [cross validation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation) in Scikit learn.

* Stratified K-Fold: preserving the percentage of samples for each class.
* Leave one out:  Each sample is used once as a test set (singleton) while the remaining samples form the training set.
* Shuffle Split: Random permutation cross-validation iterator. Yields indices.
* cross_val_score, see [this example](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_digits.html#example-exercises-plot-cv-digits-py)

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# K-fold cross validation
Exercise

On the diabetes dataset, find the optimal regularization parameter alpha.

Bonus: How much can you trust the selection of alpha?

http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold

http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;display: table; height: 100px; width: 80%; text-align: center; border: 1px solid   #ccc; margin:auto; margin-top: 50px; box-shadow: 5px 10px 8px #888888; &quot;&gt;
&lt;div style=&quot;margin: 100px 0 100px 0 ;&quot;&gt;
&lt;p style=&quot;font-size:44px;&quot;&gt;Gradient Stochastique - SGD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Gradient Stochastique aka SGD


The SGD algorithm is low on computation, has good convergence behavior and is applicable to many different situations through its many available variants. The SGD is implemented in a wide variety of languages (python, R, Java, scala, matlab, ...) and platforms (weka, rapidminer, ...) as well as online services big (Amazon ML, Google, Azure) and small (Dataiku, ....)

 The idea of iterative stochastic approximation Robbins and Monro in 1951 in a seminal paper titled A Stochastic approximation Method

The literature related to the SGD algorithm is abundant. With the resurgence of depp learning and neural networks, no less than 6000 academic papers about Stochastic Gradient Descent were published in 2016 according to google scholar (from around a few 100s in the early 2000s).

&lt;section data-markdown=&quot;&quot;&gt;
&lt;/section&gt;
The general idea is to aproximate an unknown function through iterations of unbiased estimates of the function's gradient. Knowing that the expectation of the gradient estimates equal the gradient.


Soit une function \\(f, {\bf w}_{t+1} = {\bf w}_t - \alpha_t \hat{\nabla} f({\bf x}_t) \\) given some condition on \\(\alpha_t  \text{and} \hat{\nabla} f\\), alors \\(w_t \text{converges vers} f\\).

Dans notre contexte, la fonction \\(f\\) est une fonction de régression linéaire d'ordre N avec les coefficients \\(w_k \text{with} k \in [0..N]\\)

$$f(x) = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_N x_N $$

On veut donc minimiser l'erreur

$$ e = Y - f(X) = Y - W^T X $$

cette equation admet une solution exacte que l'on a vu precedemment

$$\hat{W} = ()() $$
On peut montrer que cette la solution de cette equation est l'esimateur le moins biaisé de tous le estimqteurs possibles! En d'autre termes il fit les donn´és d'entrainement du mieux possible.

https://towardsdatascience.com/predicting-housing-prices-using-advanced-regression-techniques-8dba539f9abe
According to the Gauss-Markov theorem, the model fit by the Ordinary Least Squares (OLS) is the least biased estimator of all possible estimators. In other words, it fits the data it has seen better than all possible models.

Calculer la solution a partir de l'equation ci dessus est couteuse en calcul

On va donc approximer la solution de facon iterative

* on choisit un vercteor W_0 pour initialiser l'algo
et a chaque iteration on corrige W par le gradient de la fonction de cout


see [raschka](https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html)
The cost function J(⋅), the sum of squared errors (SSE), can be written as:

The magnitude and direction of the weight update is computed by taking a step in the opposite direction of the cost gradient

where η is the learning rate. The weights are then updated after each epoch via the following update rule:

=&amp;gt; learning rate

~[gradient as ball](/assets/04/gradient_ball.png)

see file:///Users/alexis/amcp/packt-B05028/B05028_07_draft.html

In case of very large datasets, using GD can be quite costly since we are only taking a single step for one pass over the training set – thus, the larger the training set, the slower our algorithm updates the weights and the longer it may take until it converges to the global cost minimum

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;

La dimenion stichastique consiste a actualiser le W non plus avec l'integralité du gradient sur toutes les donnees mais avec une estimation du gradient echantillon par echantillon.
Cela marche parce que dans notre contexte: E(gradient) = gradient

Instead, we update the weights after each training sample:

et on passe plusieurs fois sur le dataset en shufflant le dataset a chaque fois (epoch)

“stochastic” comes from the fact that the gradient based on a single training sample is a “stochastic approximation” of the “true” cost gradient

it has been shown that SGD almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)


## Mini-Batch Gradient Descent (MB-GD)
milieu entre le GD et le SGD
On utilise K echantillons pour estimer le gradient a chaque iteration

Accroitre K entraine

* on converge plus vite et plus directement. moins de zig zag
* on converge moins profondement. le biais est plus important.


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Outliers, detection and impact

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Skewness: Box cox and Kurtosis

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

* Data Split
    train, test, valid http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/18
    k-fold http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/20
    http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/21
    * stratified k-fold http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/22
&lt;/section&gt;
</description>
        <pubDate>Mon, 01 Jan 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4001/04-biais-variance-gradient-stochastique</link>
        <guid isPermaLink="true">http://localhost:4001/04-biais-variance-gradient-stochastique</guid>
        
        
      </item>
    
      <item>
        <title>Régression Logistique</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
# Régression Logistique
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/03/questions.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* Régression linéaire - OLS
* Interpretation
    * p-value
    * R^2

* Correlation
* Statsmodel python
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    ## Lab: titanic
    &amp;lt;img src=/assets/03/titanic_photo.jpg style='width:300px; border:0'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Programme

* Regression logistique
* odds ratio, log odds ratio
* Maximum de vraisemblance
* encoding categorical values
* Metriques de classification
    * confusion matrix
    * AUC and ROC
* Outliers, detection and impact
* Skewness: Box cox and Kurtosis

&lt;/div&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# More on Classification vs Regression

Why not use linear regression to predict some medical condition such as

* 0: Stroke,
* 1: Epileptic seizure
* 2: Overdose

Encoding it like that and using Linear Regression implies:

* order of the encoding
* equal distance between codes

# In the binary case:

* 0: Stroke,
* 1: Epileptic seizure


Possible to use linear regression as a proxy for a probability

* May end up with results outside the [0,1] range

So classification specific models better!

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Regression or Classification?
Review the following situations and decide if each one is a regression problem, classification problem, or neither:

* Using the total number of explosions in a movie, predict if the movie is by JJ Abrams or Michael Bay.
* Determine how many tickets will be sold to a concert given who is performing, where, and the date and time.
* Given the temperature over the last year by day, predict tomorrow's temperature outside.
* Using data from four cell phone microphones, reduce the noisy sounds so the voice is crystal clear to the receiving phone.
* With customer data, determine if a user will return or not in the next 7 days to an e-commerce website.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Logistic regression

also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier.

Au lieu de prédire la categorie auquelle appartient la variable cible. on va predire la probabilité que cette variable appartienne a la category en question.




$$ P(Y = 1 \bigg{/} X) $$

which we note \\( p(X) \\)

and similarly to Linear Regression we want a **simple linear model** for that probability

$$ P(Y=1 / X) =  p(X) = \beta_0 + \beta_1 X $$

but that still does not give us values between [0, 1]

&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right;&quot;&gt;
&lt;h1&gt; Sigmoid function &lt;/h1&gt;
&lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/03/sigmoid.svg style='width:300px; border:0'&amp;gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;

# Logistic regression

So instead we feed the linear model to the sigmoid function

$$ f(z) = \frac{e^{z} }{1 + e^{z}} =  \frac{1 }{1 + e^{-z}} $$

We feed $$ z = P(Y=1 / X) =  p(X) = \beta\_0 + \beta\_1 X $$ to the sigmoid function

$$ p(X) = \frac{e^{(\beta\_0 + \beta\_1 X)} }{1 + e^{(\beta\_0 + \beta\_1 X)}}  $$

because this function shrinks \\( \mathbb{R} \\)  to \\( [0,1] \\)
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# logistique regression en python
avec statsmodel

### appliqué au *default* dataset

4 colonnes

* student: étudiant?
* balance: compte en banque
* income: revenues

prédiction : va défaulter sur son crédit ou non

Using:

1. default vs balance
2. default vs balance, income and student

* Calculate the probability of default for
    * a student with a credit card balance of \\$1500 and income of \\$40k
    * a non-student, same balance and income

Why is the coefficient for student positive when student is the only factor and negative in the case of multilinomial logistic regression?

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Evenement et categorie

Question de vocabulaire:
On parle d'evenement le fait que la variable cible appartienne a une categorie.

La probabilité que la variable cible soit dans la categorie 1 = probabilité de l'evenement 1
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Odds ratio

Aussi appelé  rapport des chances, rapport des cotes1 ou risque relatif rapproché

Comment quantifier l'impact d'une variable predicteur sur la probabilité de la catégorie ?

On a:

$$ p(X) = \frac{e^{\beta\_0 + \beta\_1 X} }{1 + e^{\beta\_0 + \beta\_1 X}}  $$

Le **odds ratio**: est le rapport entre la probabilité de l'evenement sur la probabilité du non evenement.

$$
\frac{p(X)}{ 1 -p(X)} = e^{\beta\_0 + \beta\_1*X}
$$

* Odds ratio  \\( \in [0, +\infty] \\)
* Odds close to 0: low probability of the event happening
* Odds close to \\( +\infty \\) : low probability of the event happening

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Log-Odds ratio
Si on prends le log du *odds-ratio* on a le log odds ration

$$
log(\frac{p(X)}{ 1 -p(X)}) = \beta\_0 + \beta\_1 X
$$

Increase in \\( \beta\_1 \\) =&amp;gt; results in increase in \\( p(X)\\).

Not as direct and linear as in the case of linear regression.
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Log-Odds ratio: application
Sur le data set default:

* On accroit / décroit le compte en banque de 10k
* On accroit / décroit le revenu de 10k

## Exemple in the default dataset

\\( p(X) = 0.2 \iff  \frac{0.2}{1 -0.2} = 0.25 \\)

* 1/5 people with ods 1/4 will default

\\( p(X) = 0.9 \iff  \frac{0.9}{1 -0.9} = 9 \\)

* 9 out of 10 people (90%)  with ods 9 will default


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Maximum de vraisemblance
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Metriques de classification&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/03/classification_metrics.png style='width:400px;border:0'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Metriques de regression
&amp;lt;img src=/assets/03/regression_metrics.png style='width:400px;'&amp;gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right;&quot;&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/03/pregnant.jpg style='width:400px;border:0'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;


&lt;div data-markdown=&quot;&quot;&gt;
# Metrics
Correctly identified:

* TP = True Positive
* TN = True Negatives

Incorrectly identified:

* FP = False Positive
* FN = False Negatives
## Accuracy

How to you define accuracy?

$$ Accuracy = \frac{ TP + TN  }{TP + FP + TN + FN}   $$

&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Confusion matrix

![confusion matrix](/assets/03/confusion_matrix.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# ca se complique assez rapidement

&amp;lt;img src=/assets/03/confusion_matrix_wikipedia.png style='border:0'&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Confusion matrix
Avec scikit:

        from sklearn.metrics import confusion_matrix
        y_true = [0,0,0,0,0,1,1,1,1,1]
        y_pred = [0,0,0,1,1,0,1,1,1,1]
        confusion_matrix(y_true, y_pred)

&lt;/section&gt;
&lt;section&gt;
&lt;div style=&quot;float:right; width:30%&quot;&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/03/stats-vs_ml.jpeg&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left;width:50%;&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;


# quittons les stats pour rejoindre sur le machine learning

* Statsmodel est dans une approche statistique classique qui favorise l'interpretabilité

* Scikit est dans une approche machine learning plus orientée robustesse et prediction

    * une regression est un modele parmi d'autres

Au niveau du modele, la difference est que scikit ajoute une contrainte sur le modele au niveau de la fonction de cout. cette contrainte est appelé regularization et sert a accroitre la capacité du modele a &quot;marcher&quot; sur des donnees nouvelles. On verra cela en detail dans 2 jours.

On est donc dans une transition de la modelisation statistique vers la modelisation machine learning.
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Regression avec scikit-learn

On va avoir des meta parametres. Par exemple:

* acces a differents algo pour trouver les coefficients et un certain controle sur leur fonctionnement
* differentes façon de traiter le multi-class: ovr, multinomial
* differents mode de regularization

et en output

* un modele que l'on peut appliquer a de nouvelles donnees
* les intervals de confiance
* la ou les categories predites
* les proba de prediction
et surtout un model qye


On n'aura plus:
- les p-value
- le R^2
- les tests statistiques
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Scikit-learn LogisticRegression

http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

![scikit LogisticRegression](/assets/03/scikit-logistic-regression.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Demo Scikit-learn LogisticRegression

* default dataset
* score
* plot proba hist
* trouver le meilleur threshold (Acc, max P, min Neg, TPR, ...)
Use predict_proba and a different threshold =&amp;gt; you should find a different confusion matrix


* QQ plot residuals
* matrice de confusion

Essayer plusieurs regularization L2 et L1 avec differents C

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# AUX and ROC Curve

* TPR = \\( \frac{  TP }{ P}  = \frac{  TP }{ TP + FN} \\) aka **Sensitivity** or **Recall**
* FPR = \\( \frac{  FP }{ N}  = \frac{  FP }{ FP + TN} \\)  aka **Fall-out**

Le TPR / recall et le FPR varient en fonction du seuil. on obtient donc

### Receiver operating characteristic

ROC = TPR vs FPR pour different seuils

        sklearn.metrics.roc_curve returns TPR, FPR

plot to get the ROC Curve

### AUX and ROC Curve
The AUX is Area under the Curve

        sklearn.metrics.roc_auc_score

So what's your best model LR according to AUC?

voir aussi F1-score

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# One hot encoding

Comment traduire les variables quantitative en variables numeriques

Binaires
* est ce un etudiant
* fille / garcon
*

Multinomiales
* liste de villes, pays, destinations,
* tranche d'age
* niveau d'etude
* marques de voiture

Par exemple: Audi, Renault, Ford, Fiat
Si on assigne un numero arbitraire a chqaue marque de voitue on crée une hierarchie
Audi =&amp;gt;1 , Renault =&amp;gt; 2, Ford =&amp;gt; 3, Fiat =&amp;gt; 4

* chien,chat,souris,poulet =&amp;gt; {1,2,3,4}
pourquoi le poulet est 4 fois le chien ? ca ne fait pas sense


Mais parfois on peut quand meme assigner un chiffre a chaque categorie, catégories ordonnées

* enfant, jeune, adulte, vieux =&amp;gt; {1,2,3,4}
* negatif, neutre, positif =&amp;gt; {-1, 0, 1}

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# One hot encoding

[One Hot Encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), ou pandas.get_dummies

Si on a N classes, on crée N-1 variables binaires
par exemple negatif, neutre, positif: est_neutre, est_positif (est_negatif est deduite des 2 autres variables pas besoin de la specifier)


# LabelEncoder
[LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) associe un chiffre a chaque classe, on garde l'ordonnancement

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Recap
* regression logistique
* approche stats vs approche ML
* matrice de confusion
* ROC-AUC
* One hot encoding
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Resources
* Logistic Regression: Why sigmoid function?
https://github.com/rasbt/python-machine-learning-book/blob/master/faq/logistic-why-sigmoid.md

* scikit-learn documentation: Logistic regression,
http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression

* No, Machine Learning is not just glorified Statistics
https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3

* on stackexchange When to use One Hot Encoding vs LabelEncoder vs DictVectorizor? https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor


&lt;/section&gt;
</description>
        <pubDate>Mon, 01 Jan 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4001/03-regression-logistique</link>
        <guid isPermaLink="true">http://localhost:4001/03-regression-logistique</guid>
        
        
      </item>
    
      <item>
        <title>Regression lineaire</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
# Régression Linéaire

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Cours précédent

* Supervisé vs non- Supervisé
* Vue genérale de l'analyse predictive
* Python

# Questions ?

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Programme
Régression ou classification ?
Régression linéaire
OLS, Moindres carrés
Modélisation
Univariable &amp;amp; multivariables
Interpretation
MSE
R^2, P-value, Interval de confidence
Confounders et multi collinearité
Hypotheses et leur verification

Statsmodel
Linéarité:
Definition
Tests de linearite
Régression polynomiale
Anscombe quartet
Kaggle projet

# Lab : régression sur le boston housing dataset

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Regression ou classification
## Qualitatif
Variable a predire est continue
Age, taille, poids,
Salaire,
Notes,
Probabilité d'une action

## Quantitatif
Variable a predire est discrete
Binaire
Achat, resiliation, click
Survie, sexe, succes examen, admission,
Positif ou negatif
Spam
Multi class
Categories, types (A,B,C),
Positif, neutre ou negatif
Especes, ….
Pays


&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;

&amp;lt;img src=/assets/02/height_vs_weight.png style='float:right; width:400px;border:0px'&amp;gt;

# Taille en fonction de l'age des enfants

En vrai

$$ \text{Taille} = f(\text{Age}) + \epsilon $$


Regression univariable
On suppose que on peut predire la taille en fonction de l'age avec la relation lineaire suivante

$$ \hat{\text{T}}\text{aille} = a * \text{Age} + b $$

Et on cherche a connaitre les parametres (a,b) qui donnent la meilleure approximation de la vrai relation


&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Intro
Linear regression, also called Ordinary Least-Squares (OLS) Regression, is probably the most commonly used technique in Statistical Learning. It is also the oldest, dating back to the eighteenth century and the work of Carl Friedrich Gauss and Adrien-Marie Legendre. It is also one of the easier and more intuitive techniques to understand, and it provides a good basis for learning more advanced concepts and techniques.

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
Nous avons \\( n  \\) échantillons:

* Une variable \\( x = [x_1, ... , x_n]  \\)

* Et un outcome \\( y = [y_1, ... , y_n]  \\)

On veut trouver les *meilleurs*  \\(a\\) et \\(b\\) pour lesquels
&amp;lt;br \&amp;gt;

$$ \hat{y_i} = a * x_i +b  $$

de telle façon que l'erreur de prediction \\( \vert y_i - \hat{y_i} \vert   \\)  soit minimale.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
Les residus sont:

$$ e_i = y_i - \hat{y_i} $$
$$ e_i = y_i - (a * x_i +b) $$

pour  \\( i = [1, ... , n]  \\)

Les residus sont une **distance** entre les vrai valeurs  \\( y_i  \\) et leur estimations \\( \hat{y_i}  \\). On veut reduire cette distance.

Pour cela on chercher a reduire la norme  \\( L^2  \\) des residus

$$  || y - \hat{y} || =  || y - (ax +b) || $$


$$  || y - \hat{y} ||^2 =  \sum_{i=0}^n [y_i - (a*x_i + b)]^2   $$
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Petit rappel des normes

### Norme quadratique  \\( L^2  \\)
$$  ||x|| = \sqrt{  x_1^2 + .... + x_n^2  } $$

### Norme \\( L^1  \\)
$$  |x| =  |x_1| + .... + |x_n|  $$

### Norme infinie \\( L^\infty  \\)

$$  |x|_{\infty} =  max [ |x_1|, ... , |x_n| ]  $$

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Fonction de cout

$$ L(a,b)  = || y - \hat{y} ||^2 =  \sum_{i=0}^n [y_i - (a*x_i + b)]^2   $$

* fonction convexe
* pour trouver son minima il suffit de prendre la dérivé et de trouver les valeurs de \\( a \\) et \\( b \\) en \\( 0 \\).
* Cela donne 2 équations a 2 inconnues  dont la solution est

$$ \hat{\beta} =  (x^T.x)^{-1} x^T y \quad \text{où}  \quad  \hat{\beta} = \\{ a,b \\}^T   $$

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Regression multinomial : plusieurs predicteurs

On a \\(m\\) variables predicteurs et  \\(n\\)  échantillons.

Pour chaque échantillon la relation suivante
$$ \hat{y}_i  = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_m x_m $$

On peut écrire ça de la façon suivante:

* \\( n\\) échantillons  \\( y = [y_1, ... , y_n]  \\)
* \\( m\\) predicteurs \\( X = \[ (x_{i,j}) \]  \\)

où \\(X\\) est une matrice de taille (\\(n\\) par \\(m\\))

et on veut trouver les n+1 coefficients \\( \beta = [\beta_0, \beta_1, ...., \beta_n] \\) qui minimize

$$  L(\beta) = || y - X\beta ||  $$

La solution de cette équation est : $$ \hat{\beta} =  (X^T.X)^{-1} X^T y  $$
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Python

=&amp;gt; notebook
Generer plusieurs datasets de [regression](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression)

* N samples avec M variables: \\( \hat{y}_i = \sum_k \beta_k * X[i,k]  + \sigma^2 \\)

        X, y = make_regression(n_samples=N,
                                n_features=M,
                                noise =10)

* Regression weights: \\( \quad \hat{\beta} = (X^T . X)^{-1} X^T y \\)

        beta_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

* prediction

        yhat = X[:, 0]* beta[0] + X[:,1] * beta[1]

* ou si \\(M &amp;gt; 2\\):

        yhat = [0 for i in range(N)]
        for k in range(M):
            yhat += X[:, k]* beta[k]


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Erreur absolu (MAE) (L1)

Valeur absolue de la difference entre la prédiction et les vraies valeurs

$$  MAE = \sum_{i=1}^n \|\hat{y_i} - y_i \| $$

```e = np.mean( np.abs(y - yhat) )```

# Erreur quadratique (MSE) (L2)

$$  MSE = \sum_{i=1}^n (\hat{y_i} - y_i)^2 $$

```e = np.mean( (y - yhat)**2 )```

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

&amp;lt;img src=/assets/02/advertising.csv.png style='width:400px; float:right; '&amp;gt;


# Next

Maintenant on va estimer les coefficients avec une methode

Obtenir  plus d'information sur les coefficients de regression:

* leur importance relative
* leur fiabilité
* leur impact quantitatif

Sur un vrai dataset: [advertising](https://www.kaggle.com/ishaanv/ISLR-Auto), 200 echantillons
* 3 variables: TV, Radio, Newspaper: sommes dépenséee pour chaque média (k$)
* outcome: Sales: ventes réalisées
&amp;lt;/div&amp;gt;



&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Statsmodel
* [Statsmodel](http://www.statsmodels.org/stable/index.html) is a Python library designed for more statistically-oriented approaches to data analysis, with an emphasis on econometric analyses.

* It integrates well with the pandas and numpy libraries

* It also has built in support for many of the statistical tests to check the quality of the fit

* Dedicated set of plotting functions to visualize and diagnose the fit.

* Scikit-learn also has support for linear regression but it lacks the rich set of statistical tests and diagnostics that have been developed for linear models.

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;

# Notebook python

[Regression lineaire sur le dataset advertising](https://github.com/alexisperrier/gads/blob/master/04_statistics_inference/py/Lesson%204%20-%20Notebook%202%20-%20Linear%20Regression%20for%20Causal%20Inference.ipynb)

1. sales = f(TV)
2. sales = f(TV + Radio + Newspaper)
3. sales = f(Newspaper)
2. sales = f(TV + TV^2)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Explication de l'output smf.ols

```lm = smf.ols(formula='Sales ~ TV', data=df).fit()```

* R^2 et R^2 adjusted
* Coefficients
* p-value

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

&amp;lt;img src=/assets/02/01_advsertising_results_01.png style=' width:600; '&amp;gt;


```lm = smf.ols(formula='Sales ~ TV', data=df).fit()```

* **Dep. Variable**: Which variable is the response in the model
* **Model**: What model you are using in the fit
* **Method**: How the parameters of the model were calculated
* **No. Observations**: The number of observations (examples)
* **DF Residuals**: Degrees of freedom of the residuals. Number of observations - number of parameters
* **DF Model**: Number of parameters in the model (not including the constant term if present)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/01_advsertising_results_02.png style='width:30%; float:right; clear: right;'&amp;gt;

&lt;div style=&quot;width:30%; float:left;&quot;&gt;
&amp;lt;table width:40%&amp;gt;
&lt;tr&gt;&lt;td&gt;
The right part of the first table shows the goodness of fit


* **R-squared**: The [coefficient of determination](http://en.wikipedia.org/wiki/Coefficient_of_determination). A statistical measure of how well the regression line approximates the real data points

* **Adj. R-squared**: The above value adjusted based on the number of observations and the degrees-of-freedom of the residuals

* F-statistic: A measure how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals

* Prob (F-statistic): The probability that you would get the above statistic, given the null hypothesis that they are unrelated

* Log-likelihood: The log of the likelihood function.

* AIC: The [Akaike Information Criterion](http://en.wikipedia.org/wiki/Akaike_information_criterion). Adjusts the log-likelihood based on the number of observations and the complexity of the model.

* BIC: The [Bayesian Information Criterion](http://en.wikipedia.org/wiki/Bayesian_information_criterion). Similar to the AIC, but has a higher penalty for models with more parameters.&lt;/td&gt;
&amp;lt;/td&amp;gt;&lt;/tr&gt;&amp;lt;/table&amp;gt;
&lt;/div&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# R^2

proportion of the variance in the dependent variable that is predictable from the independent variable(s)

$$ \bar{y} = \frac{1}{n} \sum\_{i} y_{i} $$


The total sum of squares (proportional to the variance of the data):

$$ SS\_{\text{tot}} = \sum\_{i} (y\_{i} - \bar{y} )^2 $$

The regression sum of squares, also called the explained sum of squares:
$$ SS\_{\text{reg}} = \sum\_{i} (f\_{i} - \bar{y} )^2 $$

The sum of squares of residuals, also called the residual sum of squares:
$$ SS\_{\text{res}} = \sum\_{i} (y\_{i} - f\_{i} )^2 = \sum\_{i}e\_{i}^{2} $$

The most general definition of the coefficient of determination is

$$ R^{2} = 1-{SS\_{\text{res}} \over SS\_{\text{tot}}}   $$

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;


# R2 does not indicate whether:

* the independent variables are a cause of the changes in the dependent variable;
* omitted-variable bias exists;
* the correct regression was used;
* the most appropriate set of independent variables has been chosen;
* there is collinearity present in the data on the explanatory variables;
* the model might be improved by using transformed versions of the existing set of independent variables;
* there are enough data points to make a solid conclusion.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# The adjusted R2 is defined as

On ajuste pour prendre en compte la complaexité du modele:

$$ \bar{R}^{2} = {1-(1-R^{2}){n-1 \over n-p-1}} $$

where p is the total number of explanatory variables in the model (not including the constant term), and n is the sample size.

En accroissant le nombre de predicteurs, on accroit souvent R^2.

Mais \\(\bar{R}^{2}\\) prendra en compte la cpmplexité du modele

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;

# Log likelihood
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# coefficient, intervals et p-value

![Info Generale](/assets/02/01_advsertising_results_03.png)

The second table reports for each of the coefficients


* **coef**: La valuer des coefficients
* **std err**: l'erreur d'estimation
* **t-statistic**: une mesure de la importance (significant) statistique de chaque coefficient.

* &lt;strong&gt;P &amp;gt; |t|&lt;/strong&gt;: P-value that the null-hypothesis that the coefficient = 0 is true.

* **[95.0% Conf. Interval]**: les limites lower and upper values of the 95% confidence interval

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/p_value.png  style='float:right; width:200px'&amp;gt;
# P-value

* You start with a **null hypothesis** and an **alternative hypothesis** - a hypothesis that is the opposite of the null.

* Then, you check whether the data supports rejecting the null hypothesis or failing to reject the null hypothesis.

As it relates to model coefficients, here is the conventional hypothesis test:

* null hypothesis: There is no relationship between TV ads and Sales
    * and thus a = 0 in \\( Sales = a * TV + b \\)

* alternative hypothesis: There is a relationship between TV ads and Sales (and thus a != 0 mais ca ne veut pas dire que a = la valeur du coefficient affichée, ca serait trop simple)

**How do we test this hypothesis?**

The p-value represents the probability that the coefficient is actually zero

* if \\( P_{value} &amp;gt; 0.05 \\) then there more than 5% chance that the null hypothesis is true (a = 0)
    * =&amp;gt; can't reject
* if \\( P_{value} &amp;lt; 0.05 \\) then there's less than 5% chance that the null hypothesis is true
    * =&amp;gt;  reject the null hypothesis
    * =&amp;gt; more than 95% chance the opposite is true

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/xkcd_p_value.png style='width:250px'&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/xkcd_p_value_02.png style='width:400px'&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Multinomiale

=&amp;gt; Dans le notebook comparer differents modeles

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Conditions sur les données
Pour qu'une regression linéaire soit possible et fiable, il faut que les données vérifient les conditions suivantes:

* **Linearite**: la relation entre les predicteurs et la cible est lineaire
    * On peut tester avec des scatter plots

* **Normality**: Les variables ont une distribution normale
    * test: [QQ plot](https://en.wikipedia.org/wiki/Q–Q_plot#Interpretation)
    * ou Kolmogorov-Smirnov test
    * correction: log ou box-cox

* **Independence**: no or little multicollinearity between variables
    * test: Correlation matrix

* **Homoscedasticity**: for a given variable the low and high range have the same statistical properties with respect to the residuals
    * test: Chunk data and Check Variance

* All **Confounders** accounted for

http://www.statisticssolutions.com/assumptions-of-linear-regression/

https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/

https://www.kaggle.com/questions-and-answers/57571
What does Linear Relationship means? :- You can check linearity with scatter plots, if there is little to no linearity in the scatter plot between your dependent and Independent variables then the assumption doesn't hold.
Linearity regression assumption requires all variables to be normal, how to check? :- You can check this assumption with a Q-Q plot, if your data deviates substantially from the line on the Q-Q plot, then this assumption doesn't hold.
check for little to no multicollinearity, why is multicollinearity a problem? :- Multicollinearity generally occurs when there are high correlations between two or more predictor variables. A principal danger of such data redundancy is that of overfitting in regression analysis models. The best regression models are those in which the predictor variables each correlate highly with the dependent (outcome) variable but correlate at most only minimally with each other. Such a model is often called &quot;low noise&quot; and will be statistically robust (that is, it will predict reliably across numerous samples of variable sets drawn from the same statistical population).

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation
# Rappel pearson coefficient

Different ways to calculate correlation.

Most common one is [Pearson Correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient)

formula for r is:

$$r=\frac{\sum\_{i=1}^{n} (x\_{i}-{\bar{x}}) (y\_{i}-{\bar{y}})} { {\sqrt {\sum\_{i=1}^{n}(x\_{i}-{\bar {x}})^{2}}}{\sqrt {\sum\_{i=1}^{n}(y\_{i}-{\bar {y}})^{2}}}}$$

where:
* \\(n\\)  is the sample size

* \\(x\_{i},y\_{i}\\) are the individual sample points indexed with i

* \\( \bar{x} = \frac{1}{n} \sum\_{i=1}^{n} x\_{i} \\) (the sample mean); and analogously for  \\({\bar {y}}\\)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation
![Correlation](/assets/02/pearson-correlation-coefficient-illustration.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation

On va regarder l'influence de la correlation entre les predicteurs


Demo: Creer un exemple de relation lineaire bruité et rajouter une variable = coef de la premiere + bruit

comparer les coefficients

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation \\(\neq\\) Causalité
http://www.tylervigen.com/spurious-correlations
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Regression and Causation:


For regression coefficients to have a causal interpretation we need both that
* the linear regression assumptions hold: linearity, normality,
independence, homoskedasticity
* and that all confounders of, e.g., the relationship between treatment A and Y be in the model.

Not the same thing

Calculating Correlation: easy

Demonstrating and Quantifying Causation: Causal Inference: Not so easy


=&amp;gt; However most common strategy is to find not causality but correlation through linear regression which can be interpreted as causality under strong assumptions on the covariates.

Works under VERY strong assumptions


For regression coefficients to have a causal interpretation we need both that

the linear regression assumptions hold: linearity, normality, independence, homoskedasticity
and that all confounders of, e.g., the relationship between treatment A and Y be in the model.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Confonders

facteurs potentiels de confusion

https://www.r-bloggers.com/how-to-create-confounders-with-regression-a-lesson-from-causal-inference/

http://www.statisticshowto.com/experimental-design/confounding-variable/

![Confounders](/assets/02/confounder.png)

* Relationship between **ice-cream consumption** and number of **drowning deaths** for a given period

**Confounding: ?**
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

![xkcd](/assets/02/xkcd_correlation.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Conclusion, ce qu'il faut retenir

* Regression lineaire, simple et explicite
* Attention a ce que les predicteurs soient decorrélés
* R^2 ajusté au lieu de R^2

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Lab de cette apres midi
Regresison lineaire sur le  housing dataset

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Questions


&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Liens et resources

* [Régression linéaire en python](http://www.xavierdupre.fr/app/mlstatpy/helpsphinx/notebooks/regression_lineaire.html) sur le site de Xavier Dupré

* [OLS sur wikipedia](https://en.wikipedia.org/wiki/Ordinary_least_squares): tres complet

&lt;/section&gt;
</description>
        <pubDate>Mon, 01 Jan 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4001/02-regression-lineaire</link>
        <guid isPermaLink="true">http://localhost:4001/02-regression-lineaire</guid>
        
        
      </item>
    
  </channel>
</rss>
