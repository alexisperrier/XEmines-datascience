<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Emines - UM6P - Data Science</title>
    <description>Cours de data science.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 03 Oct 2018 21:38:32 +0100</pubDate>
    <lastBuildDate>Wed, 03 Oct 2018 21:38:32 +0100</lastBuildDate>
    <generator>Jekyll v3.7.2</generator>
    
      <item>
        <title>1) Intro et Python</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Data science&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;Analyse prédictive et machine learning &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;&amp;nbsp;&lt;/p&gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;&amp;nbsp;&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:center;&quot;&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/01/presentation_alexis_perrier.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* Concepts et Methodes
    * biais, variance et overfitting
    * transformations des données
    * feature engineering et feature selection
    * métriques et techniques de scoring

* datasets
    * iris, titanic, housing
    * caravan, arbres, ...

* Python
    * notebook jupyter, anaconda
    * pandas, numpy
    * statsmodel et surtout scikit-learn
    &lt;/div&gt;
&lt;/div&gt;


&lt;hr class=&quot;vline&quot; /&gt;

&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Programme

* Machine learning avec scikit-learn
    * analyse prédictive
    * classification et régression

* Approches statistiques classiques:

    * régression linéaire,
    * régression logistique

* Modélisation machine learning
    * Random Forests, XGBoost
    * Support vector machines
    * Gradient stochastique
    * Adaboost, perceptron
    * Naive Bayes

&lt;/div&gt;
&lt;/div&gt;

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Déroulement

* Matin: théories, méthodes et démos

* Après-midi: Lab, workshop =&amp;gt; notebooks jupyter

* Quizzes

* Projet final: Kaggle

    * https://www.kaggle.com/c/house-prices-advanced-regression-techniques/

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;data science - machine learning - predictive analytics - intelligence artificielle - deep learning&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/tweet_when_youre_fundraising.png width = 800&amp;gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![data science sexiest job](/assets/01/data_science_sexiest_job.jpeg)
![demand](/assets/01/data-scientist-demand.jpg)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# data science - machine learning - predictive analytics - intelligence artificielle - deep learning

* **Data Analysis, Data Mining** : Exploration, trouver les tendances, les evolutions, les anomalies, etudier les corrélations.

* **Statistiques** : Trouver le modèle qui explique au mieux les données

* **Machine learning** : Le modèle apprend automatiquement à partir des données. Dimension importante d'apprentissage, de training

* **Analyse prédictive**: Construire ou entrainer des modèles qui peuvent *&quot;prédire&quot;* à partir de données passées.
* **Deep Learning** : Analyse prédictive supervisée avec des réseaux de neurones


[What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data?](https://www.quora.com/What-is-the-difference-between-Data-Analytics-Data-Analysis-Data-Mining-Data-Science-Machine-Learning-and-Big-Data-1)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Stats vs machine learning

# A tiny drop of History

Great article [Forbes: A Very Short History Of Data Science](http://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/#bbea13569fd2)

2001 Leo Breiman, Berkeley, publishes “[Statistical Modeling: The Two Cultures](https://projecteuclid.org/euclid.ss/1009213726)”:

“*There are two cultures in the use of statistical modeling to reach conclusions from data.*

**One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.**

*The statistical community has been committed to the almost exclusive use of data models.*

*This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.*”


&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/ds_meme.jpg&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Data Science: skills
&amp;lt;img src=/assets/01/ven_diagrams.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/data_science_close_up.png width = 650&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Champs d'applications

* **Predictions**: market, demand, supply prices, population, weather, earthquakes, ...

* **Patterns**: customer behavior patterns

* **Detection**: Spam, Fraud, Failures, Cyber attacks

* **Extracting meaning** from large sets of data: handwritten health records, exoplanets

* **NLP**: translation, speech to text, speech recognition, sentiment analysis, topic modeling, spell checking

* **Recommender systems**: Netflix, Spotify, Amazon

* **Ranking systems**: search results

* **Autonomous systems** (reinforcement learning / AI): playing games, self driving cars, drones

* **Time series**: algorithmic trading, signal processing, IoT
* **Image / Video**: automatic captionning, face and object recognition, ...

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Data science workflow&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

![](/assets/01/predicsis_data_science_workflow.png)

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## A) Les données

1.  Définir le problème
    * De quelles données disposent-on ?
    * Sont-elles accessibles ?
    * Que veut-on améliorer ?
    * Comment mesurer l'efficacité de la solution, du modèle ?
    * Choix des métriques

2. ETL: Extraction Transform Load

    * Constituer le dataset
    * Explorer et comprendre

## là commence le travail de modélisation

3. Travailler sur les variables
    * Nettoyer et transformer : outliers, missing values, distributions, correlations, ...
    * feature engineering
    * feature selection
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/aws_ml.png width=200&amp;gt;
&amp;lt;img src=/assets/01/gcloud_ml.png width=200&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## B) Machine Learning

4) Outils et plateforme
* Cloud (AWS, Google Cloud, Azure) ou local
* python (scikit-learn) ou R ou ...
* Modèles classiques
* Deep learning (TF, Keras, pytorch, ...)

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/ml_map.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## B) Machine Learning

5) Modélisation

* Choisir la bonne approche, le bon type de modèle
* *Train* le modèle
* Evaluer le modèle, scoring, ....
* Sélectionner les meilleurs parametres du modèle

6) Appliquer sur de nouvelles données

* on quitte un environnement controlé (laptop / labo) pour le monde réel

* le modèle generalise t il bien ?
* les données ont-elles changées ?


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;



## D) Mise en production

* ingénierie logicielle
* API
* streaming

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

## C) Nouvelle itération

7) Présentation des résultats

* cycle itératif court
* communication
* data visualization

8) reprendre le problème au niveau des données
* il en faut plus
* il faut de nouvelles variables
* ....

ou au niveau de la définition du problème

* qu'est ce qu'on veut optimiser
* comment le mesurer
* accessibilité des données
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;!-- ======================================================== --&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/predicsis_data_science_workflow.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## A) Les données

* 1)  Définir le problème

* 2) ETL: Extraction Transform Load

* 3) Travailler sur les variables

## B) Machine Learning

* 4) Outils et plateforme
* 5) Modélisation
* 6) Le test des nouvelles données

## C) Nouvelle itération

* 7) Présentation des résultats
* 8) reprendre le problème


## D) Mise en production

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Data science - machine learning - predictive analytics

[Can I learn Machine Learning completely with Kaggle?](https://www.quora.com/Can-I-learn-Machine-Learning-completely-with-Kaggle)

*While modeling is the sexy part of any machine learning project, it is also one of the parts that you will actually spend the least amount of time on.*

*In a business environment 80–90% of the time will be spent on defining problems worthwhile solving, defining **evaluation metrics**, procuring access to the **raw data**, **understanding** the data, generating **features**, presenting findings, and working with engineers to deploy the model to production via API or other automated approaches.*

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/your_plan_vs_reality.jpeg height=500&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Analyse prédictive

![](/assets/01/predictive_analytics.png)

[Predictive analytics](https://en.wikipedia.org/wiki/Predictive_analytics) : analyze current and historical facts to make predictions about **future or otherwise unknown events**. Predictive analytics provides a predictive score (probability).

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Non Supervisée

Le dataset d'apprentissage n'inclut pas de variable cible. Il n'y a pas de **ground truth**

* logique de clustering, de classification automatique des échantillons  sans connaitre a priori le nombre de classes
* notion de similarité et de distance entre les échantillons
* K-means, K-NN, ...


&amp;lt;img src=/assets/01/ch1_unsupervised_learning.png&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Supervisée

Le dataset d'apprentissage inclut la variable  à prédire  [cible]. On a un certain nombres d'exemples sur lesquels on peut entrainer un modèsle

* logique de scoring, de classification et de prediction
* Random forest, Regression lineaire ou logistique, SVM, ...
* Classification: On connait le nombre de classes

&amp;lt;img src=/assets/01/ch1_supervised_learning.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Classification

La variable cible est discrete, une catégorie, une classe

### Cas binaire
* Achat, resiliation, click
* Survie, maladie, succes examen, admission,
* Positif ou negatif
* Spam, fraude

### Multi class - multinomiale
* Catégories, types (A,B,C),
* Positif, neutre ou negatif
* Espèces de plantes d'animaux, ...
* Pays, planetes

### Ordinale

* Notes, satisfaction, ranking


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Regression

La variable cible est continue

* Age, taille, poids,

* nombre d'appels, de clicks, volume de vente, consommation

* Température, Salaire, ...

* Probabilité d'une action

* Temps, délai, retard

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Classification à regression

* Prédire une probabilité au lieu d'une classe

$$
0 &amp;lt; P( x \in A) &amp;lt; 1
$$


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Regression à classification

* discrétiser la variable

Age =&amp;gt;

* 0 - 12
* 12 - 24
* 25 - 49
* 50 - 65
* plus de 65



    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;Environnement&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;Python, anaconda et jupyter &lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/Python-Logo-PNG-Image.png width=400&amp;gt;
&amp;lt;img src=/assets/01/guidovanrossum.jpg width=400&amp;gt;


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Python

* Beaucoup d’applications: web, data science, scientific, …

* Créé en 1991 par Guido von Rossum! 30 ans déjà!

* 130.000 packages et librairies

* Duck typing, pas de compilation, pas de ; ou de {}

* Indentation =&amp;gt; le code est lisible

* Performances

* Mais il y a des surprises, des incoherences, des idioms, …

* Python 2.7 ou python 3.6


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/python_projections_on_stackoverflow.png height=650&amp;gt;
&lt;/section&gt;

&lt;section&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Python

* list comprehension

```liste_a = [n for n in range(100) if n % 2 ==0 ]```


* pandas dataframe

``` df = pd.read_csv(filename) ```

```df = df.groupby(by = 'age' ).reset_index(inplace = True)```

```df = df.age.apply(lambda a : une_fonction(a) )
```

* notebook jupyter


    &amp;gt; jupyter notebook


* exemple

[Python_Pandas_Demo.ipynb](Python_Pandas_Demo.ipynb)

&lt;/div&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Quel Python avez-vous?

Dans un terminal

```&amp;gt; python --version
```

![](/assets/01/python_version.png)

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/anaconda_logo.png&amp;gt;

[https://www.anaconda.com/](https://www.anaconda.com/)


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Anaconda

* Distribution Anaconda et package manager conda

```conda install package_name
```

* Data science en python:

    * Dataframe: pandas, dask

    * Math, science: numpy, scipy, statsmodel,

    * Dataviz: matplotlib, plot.ly, bokeh

    * Deep learning: Tensorflow, Keras, Mxnet, …

    * Text: Gensim, NLTK, Spacy.io

    * scikit-learn: http://scikit-learn.org/

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# [http://jupyter.org/](http://jupyter.org/)
&amp;lt;img src=/assets/01/jupyter_logo_ecosystem.png&amp;gt;
&amp;lt;img src=/assets/01/jupyter_architecture.png&amp;gt;


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Jupyter notebook

* Executer du code dans le navigateur
* Partage et reproductabilité
* Calcul et visualisation

* Multilingue: R, python, ...
* Local ou cloud
    * $ Jupyter notebook
    * AWS Sagemaker, Google datalab, Kaggle kernels
* A base de cellules
    * Documentation: markdown et latex
    * Kernels: Python, R, Julia, Scala, …
    * Shell terminal

* Alt: Beaker, Apache zeppelin

* Mais: Le code est séquentiel + State problems

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Editeurs de texte

&amp;lt;img src=/assets/01/atom_io-card.png height=150px&amp;gt;

&amp;lt;img src=/assets/01/sublime_text_logo.png height=150px&amp;gt;

&amp;lt;img src=/assets/01/spyder_readme_banner.png height=150px&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;Récapitulatif&amp;lt;/p&amp;gt;
&amp;lt;p class=top&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Récapitulatif

* Programme des 2 semaines
* Révisions de python
* Différence entre Data Science, Machine Learning et analyse prédictive
* Approche statistique vs approche machine learning
* Déroulement d'un projet de Data science
* Supervisée vs non-supervisée
* Regression vs Classification
* Anaconda, Python et Jupyter

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Notebook d'exploration et Pandas

* load dataset dans une dataframe pandas
* visualisation des variables
* statistiques des variables numériques et occurences des catégories
* trouver les outliers et les enlever

Arrondissements:

* quels arrondissements ont
    * le plus d'arbres
    * le plus de variétés d'arbres
    * les arbres les plus hauts, les plus larges
* hauteur et circonférence en fonctions des espèces d'arbres
* Comment sont définis les arbres dit remarquables?
    * comment traiter les valeurs manquantes de cette colonne

Domanialité
    * memes questions que pour les arrondissements: variétés, hauteur, ...

En joignant le dataset arrondissement qui donne la superficie des arrondissements

Creer une variable code_postale dans le dataset arbres, qui permette de joindre les 2 fichiers: PARIS 2E ARRDT =&amp;gt; 75102

Joindre les 2 fichiers

Calculer le nombre d'arbres par arrondissement en utilisant groupby et count()
enlever les arrondissement qui ne sont pas dans Paris



    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Lab

2 datasets

* 200.000 arbres de Paris
    * Espèces, genres, famille
    * Adresse, geolocalisation
    * Environnement: rue, jardin, ..
    * hauteur et circonférence

* Arrondissement de Paris
    * Superficie

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/1-intro-python</link>
        <guid isPermaLink="true">http://localhost:4000/1-intro-python</guid>
        
        
      </item>
    
      <item>
        <title>2) Régression linéaire</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
Régression Linéaire
&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;et Python&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent

* Révisions de python
* Différence entre Data Science, Machine Learning et analyse prédictive
* Approche statistique vs approche machine learning
* Déroulement d'un projet de Data science
* Supervisée vs non-supervisée
* Régression vs Classification
* Anaconda, Python et Jupyter

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Lab

Lab pandas et exploration sur le dataset les-arbres.csv

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Régression linéaire

* Régression linéaire
    * OLS, Moindres carrés
    * Modélisation
    * Univariable &amp;amp; multivariables
* Interprétation des résultats
    * Mean Square Error (MSE)
    * P-value, Interval de confiance, \\(R^2\\), \\(R^2_{adj}\\)
    * Confonders et multi-collinearité

* Hypothèses et vérification
    * Linéarité: Définition et tests

* Statsmodel

## Projet Kaggle

## Python

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
https://www.kaggle.com/c/house-prices-advanced-regression-techniques
&amp;lt;img src=/assets/02/kaggle_competition.png height=600&amp;gt;

&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

## Regression: Qualitatif

La variable à prédire est **continue**

* Age, taille, poids,

* nombre d'appels, de clicks, volume de vente, consommation

* Température, Salaire, ...

* Probabilité d'une action

* Temps, délai, retard

&lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:60%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

## Classification: Quantitatif
La variable à prédire est **discrète**

### Cas binaire
* Achat, resiliation, click
* Survie, maladie, succes examen, admission,
* Positif ou negatif
* Spam, fraude

### Multi class - multinomiale
* Catégories, types (A,B,C),
* Positif, neutre ou negatif
* Espèces de plantes d'animaux, ...
* Pays, planetes

### Ordinale

* Notes, satisfaction, ranking

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data=&quot;&quot;&gt;

&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

&amp;lt;img src=/assets/02/height_vs_weight.png style='float:right; width:400px;border:0px'&amp;gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

## Taille en fonction de l'age des enfants

On mesure la taille des enfants dans une ecole et leur age.
La taille croit avec l'age. On peut écrire

$$ \text{Taille} = f(\text{Age})  $$


## Regression univariable


On modélise cette fonction par une relation linéaire de la forme:

$$ \hat{\text{T}}\text{aille} = a * \text{Age} + b $$

où \\(\hat{\text{T}}\text{aille}\\) est la taille estimée.


On cherche à connaitre les paramètres \\((a,b)\\) qui donnent la meilleure approximation de la réalité entre la taille et l'age.

Pour trouver ces paramètres on utilise une méthode dite des **moindres carrés**  ou  **Ordinary Least-Squares (OLS)**.

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data=&quot;&quot;&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
Les résidus  \\( e_i  \\)  représentent une **distance** entre les vrais valeurs  \\( y_i  \\) et leur estimations \\( \hat{y_i}  \\). On veut réduire cette distance.

Pour cela on chercher à minimiser la somme des carrés des résidus (aussi appelé norme  \\( L^2  \\).)

$$  || y - \hat{y} ||^2 =  \sum_{i=0}^n (y_i - (a*x_i + b))^2   $$

&amp;lt;img src=/assets/02/Ordinary_Least_Squares_OLS.jpg&amp;gt;


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Regression linéaire
Nous avons \\( n  \\) échantillons:

* Une variable prédictrice \\( x = [x_1, ... , x_n]  \\)

* Et une variable cible \\( y = [y_1, ... , y_n]  \\)

On veut trouver les *meilleurs*  \\(a\\) et \\(b\\) pour lesquels

$$ \hat{y_i} = a * x_i +b  $$

l'erreur de prédiction \\( e_i  \\)  soit minimale:

$$ e_i = \vert  y_i - \hat{y_i} \vert  = \vert  y_i - (a * x_i +b)\vert  $$
&lt;/div&gt;
&lt;/div&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Petit rappel des normes

### Norme quadratique  \\( L^2  \\)
$$  ||x|| = \sqrt{  x_1^2 + .... + x_n^2  } $$

### Norme \\( L^1  \\) ou norme en valeur absolue
$$  |x| =  |x_1| + .... + |x_n|  $$

### Norme infinie \\( L^\infty  \\)

$$  |x|\_{\infty} = max [ |x_1|, ... , |x_n| ] $$

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

Cela donne 2 équations à 2 inconnues  dont la solution exacte est:

$$ \hat{\beta} =  (x^T.x)^{-1} x^T y   $$

avec

* \\( \hat{\beta} = \\{ a,b \\}^T \\)

*  \\( x = [x_1, ... , x_n]  \\)

*  \\( y = [y_1, ... , y_n]  \\)

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Fonction de cout

On a ce qu'on appelle une **fonction de cout** \\(L(a,b) \\):

$$ L(a,b)  = || y - \hat{y} ||^2 =  \sum_{i=0}^n [y_i - (a*x_i + b)]^2   $$

C'est fonction quadratique donc convexe.

Par conséquent pour trouver son minima, il faut trouver les valeurs de \\( a \\) et \\( b \\) qui annule la dérivée \\( 0 \\) .

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
On veut trouver les n+1 coefficients

$$ \beta = [\beta_0, \beta_1, ...., \beta_n] $$

qui minimisent la fonction de cout:

$$  L(\beta) = || y - X\beta ||  $$

La solution de cette équation est : $$ \hat{\beta} =  (X^T.X)^{-1} X^T y  $$
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;


# Regression multinomial

## plusieurs predicteurs

On a  maintenant \\(m\\) predicteurs et toujours \\(n\\)  échantillons.

Pour chaque échantillon, on a la modélisation suivante:
$$ \hat{y}_i  = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_m x_m $$
ou plus simplement
$$ \hat{y}  = \beta X  $$

avec

* \\( X = \[ (x_{i,j}) \]  \\) est une matrice de taille  \\(n\\) par \\(m\\)

* \\( y = [y_1, ... , y_n]  \\) vecteur de \\( n\\) échantillons



    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Python

A)

    X, y = make_regression(n_samples=N, n_features=M, noise =10)


B)

    beta_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

C)

    yhat = X[:, 0]* beta[0] + X[:,1] * beta[1]


* ou si \\(M &amp;gt; 2\\):


    yhat = [0 for i in range(N)]

    for k in range(M):
        yhat += X[:, k]* beta[k]


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Regression linéaire


A) N samples avec M variables:

$$ y_i = \sum_k \beta_k * X[i,k]  + \sigma^2 $$

$$ y =  \beta * X  + \sigma^2 $$

B) Regression weights:

$$\quad \hat{\beta} = (X^T . X)^{-1} X^T y $$

C) Prédiction

$$ \hat{y}_i = \sum_k \beta_k * X[i,k]  + \sigma^2 $$



    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Notebook - demo

02 Linear Regression Exact.ipynb

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Metriques de scoring
## Erreur absolu (MAE) (L1)

Valeur absolue de la difference entre la prédiction et les vraies valeurs


$$  MAE = \sum\_{i=1}^n \| \hat{y\_i} - y\_i \| $$

        e = np.mean( np.abs(y - yhat) )

## Erreur quadratique (MSE) (L2)

$$  MSE = \sum_{i=1}^n (\hat{y_i} - y_i)^2 $$

        ```e = np.mean( (y - yhat)**2 )
        ```

&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

Sur un vrai dataset: **Mileage per gallon performances of various cars** disponible sur https://www.kaggle.com/uciml/autompg-dataset

A prédire:
* mpg: continuous

Les variables

* cylinders: multi-valued discrete
* displacement: continuous
* horsepower: continuous
* weight: continuous
* acceleration: continuous

On ne prends pas en compte:

* model year: multi-valued discrete
* origin: multi-valued discrete
* car name: string (unique for each instance)

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Régression linéaire avec Statsmodel

On va estimer les coefficients non plus directement mais avec la méthode OLS.



On aura plus d'information sur les coefficients de régression:

* leur importance relative
* leur fiabilité
* leur impact quantitatif

On utilise la librairie

* [Statsmodel](http://www.statsmodels.org/stable/index.html) librairie Python
pour une approche statistique de l'analyse de données.

* Intégrée avec pandas et numpy


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# statsmodel


![](assets/02/statsmodel_functions.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Notebook python
    import pandas as pd
    import statsmodels.formula.api as smf

    df = pd.read_csv('../data/autos_mpg.csv')
    lm = smf.ols(formula='mpg ~ cylinders + displacement + horsepower + weight + acceleration + origin ', data=df).fit()
    lm.summary()

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Résultats
* **Dep. Variable**: La variable à prédire
* **Model**: Le modèle
* **Method**: La méthode utilisée
* **No. Observations**: Le nombre d'observations / échantillons
* **DF Residuals**: Degré de liberté des résidus = nombre d'échantillons - nombre de variables
* **DF Model**: Nombre de prédicteurs

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/02_linreg_autompg_01-left.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Goodness of fit

* **R-squared**: The [coefficient of determination](http://en.wikipedia.org/wiki/Coefficient_of_determination). A statistical measure of how well the regression line approximates the real data points

* **Adj. R-squared**: The above value adjusted based on the number of observations and the degrees-of-freedom of the residuals

* F-statistic: A measure how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals

* Prob (F-statistic): The probability that you would get the above statistic, given the null hypothesis that they are unrelated

* Log-likelihood: The log of the likelihood function.

* AIC: The [Akaike Information Criterion](http://en.wikipedia.org/wiki/Akaike_information_criterion). Adjusts the log-likelihood based on the number of observations and the complexity of the model.

* BIC: The [Bayesian Information Criterion](http://en.wikipedia.org/wiki/Bayesian_information_criterion). Similar to the AIC, but has a higher penalty for models with more parameters.&amp;lt;/td&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/02_linreg_autompg_01-right.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    # Définition

\\(R^2\\) est la proportion des variations de la variable cible qui est prédite grace aux prédicteurs.

On définit  \\(R^2\\) par

$$ R^{2} = 1-{SS\_{\text{res}} \over SS\_{\text{tot}}}   $$

On a

$$ 0 &amp;lt; R^2 &amp;lt; 1$$



    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    # \\(R^2\\)

Soit la moyenne de la variable cible :

$$ \bar{y} = \frac{1}{n} \sum\_{i} y_{i} $$

et la somme des carrés de la variable cible centrée :

$$ SS\_{\text{tot}} = \sum\_{i} (y\_{i} - \bar{y} )^2 $$

La somme des carrés des résidus :
$$ SS\_{\text{res}} = \sum\_{i}e\_{i}^{2} = \sum\_{i} (y\_{i} - \hat{y}\_{i} )^2  $$


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# R2 does not indicate whether:

* the independent variables are a cause of the changes in the dependent variable;
* omitted-variable bias exists;
* the correct regression was used;
* the most appropriate set of independent variables has been chosen;
* there is collinearity present in the data on the explanatory variables;
* the model might be improved by using transformed versions of the existing set of independent variables;
* there are enough data points to make a solid conclusion.

et surtout
* plus on ajoute de variable plus \\(R^2\\) augmente meme quand les variables ne sont pas vraiment significative.

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
En accroissant le nombre de predicteurs, on accroit souvent R^2.

Mais \\(R^{2}_{adj}\\) compense la complexité du modele
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# \\(R^2_{adj}\\)

On ajuste pour prendre en compte la complexité du modele:

$$ R^{2}_{adj} = {1-(1-R^{2}){n-1 \over n-p-1}} $$

avec

* \\(p\\) le nombre de prédicteurs
* \\(n\\) le nombre d'échantillons

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;


# Coefficients et p-value
La deuxième partie des résultats porte sur les coefficients et leur fiabilité.


* **coef**: La valeur estimée des coefficients
* **P &amp;gt; |t|**: la probabilité que l'on observe cette estimation alors qu'en fait le coefficient est nulle (=0) .
* **[95.0% Conf. Interval]**: l'interval de confiance de l'estimation du coefficient.
* **std err**: l'erreur d'estimation
* **t-statistic**: une mesure de l'importance (significant) statistique de chaque coefficient.

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/02_linreg_autompg_02.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;width : 40%;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

&amp;lt;img src=/assets/02/p_value.png height=250px &amp;gt;


The p-value represents the probability that the coefficient is actually zero

* Si \\( P_{value} &amp;gt; 0.05 \\) alors il y a plus de 5% de chance que l'hypothèse NULL soit vraie:=&amp;gt; **on ne peut pas la rejeter**.

* si \\( P_{value} &amp;lt; 0.05 \\) a lors il y a moins de 5% de chance pour que l'hypothèse NULL soit vraie: =&amp;gt; **on  peut la rejeter**
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# P-value

On a 2 hypothèses:

1. [NULL] ce que l'on observe est du au hasard
2. [ALT] ce que l'on observe n'est pas du au hasard (il y a une relation)

La p-value est la probabilité que ce que l'on observe est du au hasard.

Si la p-value est faible, on rejete l'hypothèse NULL.

Ce qui ne veut pas dire que la valeur du coefficient est la  bonne. (ca serait trop simple) mais simplement que il y a bien une relation entre le predicteur et la variable cible.

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/xkcd_p_value.png style='width:250px'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/xkcd_p_value_02.png style='width:300px'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Multinomiale

Que se passe t il quand on filtre certains predicteurs ?

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Conditions sur les données
Pour qu'une régression linéaire soit possible et fiable, il faut que les données vérifient les conditions suivantes:

* **Linearite**: la relation entre les predicteurs et la cible est lineaire
    * On peut tester avec des scatter plots

* **Normality**: Les variables ont une distribution normale
    * test: [QQ plot](https://en.wikipedia.org/wiki/Q–Q_plot#Interpretation)
    * ou Kolmogorov-Smirnov test
    * correction: log ou box-cox

* **Independence**: no or little multicollinearity between variables
    * test: Correlation matrix

* **Homoscedasticity**: for a given variable the low and high range have the same statistical properties with respect to the residuals
    * test: Chunk data and Check Variance

* All **Confounders** accounted for

http://www.statisticssolutions.com/assumptions-of-linear-regression/

https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/

https://www.kaggle.com/questions-and-answers/57571
What does Linear Relationship means? :- You can check linearity with scatter plots, if there is little to no linearity in the scatter plot between your dependent and Independent variables then the assumption doesn't hold.
Linearity regression assumption requires all variables to be normal, how to check? :- You can check this assumption with a Q-Q plot, if your data deviates substantially from the line on the Q-Q plot, then this assumption doesn't hold.
check for little to no multicollinearity, why is multicollinearity a problem? :- Multicollinearity generally occurs when there are high correlations between two or more predictor variables. A principal danger of such data redundancy is that of overfitting in regression analysis models. The best regression models are those in which the predictor variables each correlate highly with the dependent (outcome) variable but correlate at most only minimally with each other. Such a model is often called &quot;low noise&quot; and will be statistically robust (that is, it will predict reliably across numerous samples of variable sets drawn from the same statistical population).

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
Correlation
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Rappel pearson coefficient

Etudier la corrélation entre deux ou plusieurs variables aléatoires ou statistiques numériques, c’est étudier l'intensité de la liaison qui peut exister entre ces variables.

Il y a différentes façon de calculer la corrélation de 2 variables.

La plus commune est [Pearson Correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient)

Qui se calcule suivant :

$$r=\frac{\sum\_{i=1}^{n} (x\_{i}-{\bar{x}}) (y\_{i}-{\bar{y}})} { {\sqrt {\sum\_{i=1}^{n}(x\_{i}-{\bar {x}})^{2}}}{\sqrt {\sum\_{i=1}^{n}(y\_{i}-{\bar {y}})^{2}}}}$$

où :

* \\(n\\) nombre d'échantillons

* \\(x\_{i},y\_{i}\\) les échantillons

* \\( \bar{x} = \frac{1}{n} \sum\_{i=1}^{n} x\_{i} \\) la moyenne; de meme pour  \\({\bar {y}}\\)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation
![Correlation](/assets/02/pearson-correlation-coefficient-illustration.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation

On va regarder l'influence de la correlation entre les predicteurs

    df.corr()

Les prédicteurs ```horsepower``` et ```weight``` sont très corrélés, ```displacement``` et ```cylinders``` aussi.

&amp;lt;img src=/assets/02/autompg-correlation.png  height=400&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation \\(\neq\\) Causalité
http://www.tylervigen.com/spurious-correlations

&amp;lt;img src=/assets/02/spurious_correlations.png&amp;gt;


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Regression and Causation:


For regression coefficients to have a causal interpretation we need both that
* the linear regression assumptions hold: linearity, normality,
independence, homoskedasticity
* and that all confounders of, e.g., the relationship between treatment A and Y be in the model.

Not the same thing

Calculating Correlation: easy

Demonstrating and Quantifying Causation: Causal Inference: Not so easy


=&amp;gt; However most common strategy is to find not causality but correlation through linear regression which can be interpreted as causality under strong assumptions on the covariates.

**Works under VERY strong assumptions**

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Confonders

facteurs potentiels de confusion

https://www.r-bloggers.com/how-to-create-confounders-with-regression-a-lesson-from-causal-inference/

http://www.statisticshowto.com/experimental-design/confounding-variable/

![Confounders](/assets/02/confounder.png)

* Relationship between **ice-cream consumption** and number of **drowning deaths** for a given period

**Confounding: ?**
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

![xkcd](/assets/02/xkcd_correlation.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;Récapitulatif&amp;lt;/p&amp;gt;
&amp;lt;p class=top&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Récapitulatif

* Regression lineaire, simple et explicite
* Attention à ce que les predicteurs soient decorrélés
* R^2 ajusté au lieu de R^2

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Lab de cette apres midi
Regression lineaire sur le  dataset *advertising*

![advertising](/assets/02/advertisingscatterplots.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Questions

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Liens et resources

* [Régression linéaire en python](http://www.xavierdupre.fr/app/mlstatpy/helpsphinx/notebooks/regression_lineaire.html) sur le site de Xavier Dupré

* [OLS sur wikipedia](https://en.wikipedia.org/wiki/Ordinary_least_squares): tres complet

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/2-regression-lineaire</link>
        <guid isPermaLink="true">http://localhost:4000/2-regression-lineaire</guid>
        
        
      </item>
    
      <item>
        <title>3) Régression Logistique</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
Régression Logistique
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/03/questions.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* Régression linéaire - OLS
* Interpretation
    * p-value
    * R^2

* Correlation
* Statsmodel
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    ## Lab: titanic
    &amp;lt;img src=/assets/03/titanic_photo.jpg style='width:300px; border:0'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Programme

* Regression logistique
* odds ratio, log odds ratio
* Maximum de vraisemblance
* encoding categorical values
* Metriques de classification
    * confusion matrix
    * AUC and ROC
* Outliers, detection and impact
* Skewness: Box cox and Kurtosis

&lt;/div&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Classification ou Régression

Voici une variable à prédire, une condition médicale:

* 0: Attaque cardiaque
* 1: Crise d'épilepsie
* 2: Overdose

Pourquoi ne pas utiliser une regression linéaire pour prédire cette variable ?

L'encodage de la variable (ordre et continuité) implique que

* il y a un ordre entre les catégories: Attaque &amp;lt; Crise &amp;lt; Overdose
* Toutes ces catégories sont équidistantes

# Dans le cas binaire

* 0: Attaque cardiaque
* 1: Crise d'épilepsie

On pourrait utiliser une regression lineaire comme substitut de probabilité mais on obtiendrait peut etre des valeurs en dehors de [0,1]

Donc utiliser des modèles de classification est plus approprié!

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Regression ou Classification?
Review the following situations and decide if each one is a regression problem, classification problem, or neither:

* Using the total number of explosions in a movie, predict if the movie is by JJ Abrams or Michael Bay.
* Determine how many tickets will be sold to a concert given who is performing, where, and the date and time.
* Given the temperature over the last year by day, predict tomorrow's temperature outside.
* Using data from four cell phone microphones, reduce the noisy sounds so the voice is crystal clear to the receiving phone.
* With customer data, determine if a user will return or not in the next 7 days to an e-commerce website.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Régression Logistique


Aussi appelée **logit regression**, **maximum-entropy classification (MaxEnt)** ou log-linear classifier.

Au lieu de prédire la catégorie de la variable cible, on va prédire la probabilité que cette variable appartienne à la catégorie en question :


$$ P(Y = 1 \bigg{/} X) $$

que l'on note \\( p(X) \\)

et comme pour la regression linéaire on vuet avoir un modlèle linéaire simple pour estimer cette probabilité.

$$ P(Y=1 / X) =  p(X) = \beta_0 + \beta_1 X $$

mais pour que \\(p(X)\\) soit une probabilité il faut que ses valeurs soient comprises dans \\( [0, 1] \\) ce qui n
est pas forcement le cas avec la formule ci-dessus.

&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right; width:45%; &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    # Fonction Sigmoide
&amp;lt;img src=/assets/03/sigmoid.svg style='width:300px; border:0'&amp;gt;

Cette fonction réduit \\( \mathbb{R} \\)  à l'intervale \\( [0,1] \\)
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Régression logistique

Le modèle linéaire \\(p(X) = \beta\_0 + \beta\_1 X\\)  est pris comme attribut de la fonction sigmoide.

$$ f(z) = \frac{e^{z} }{1 + e^{z}} =  \frac{1 }{1 + e^{-z}} $$

ce qui donne

$$ p(X) = \frac{e^{(\beta\_0 + \beta\_1 X)} }{1 + e^{(\beta\_0 + \beta\_1 X)}}  $$


&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# logistique regression en python
avec statsmodel

### appliqué au *default* dataset

4 colonnes

* student: étudiant?
* balance: compte en banque
* income: revenues

prédiction : va défaulter sur son crédit ou non

En utilisant :

1. default vs balance
2. default vs balance, income et student

* calculer la probabilité de default pour

    * Un etudiant avec un solde debiteur de 1500 et un revenude 40000
    * un non etudiant avec le meme solde et meme revenue

Pourquoi est ce que le coefficient relatif a la variable student est positive quand student est la seule variable alors qu'elle est negative dans le cas multinomial ?


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Catégorie et évènement

Question de vocabulaire:

* un évènement = la variable appartient à la catégorie

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Odds ratio

Aussi appelé  rapport des chances, **rapport des cotes** ou **risque relatif rapproché**

Comment quantifier l'impact d'une variable predicteur sur la probabilité de la catégorie ?

On a:

$$ p(X) = \frac{e^{\beta\_0 + \beta\_1 X} }{1 + e^{\beta\_0 + \beta\_1 X}}  $$

Le **odds ratio**: est le rapport entre la probabilité de l'évènement sur la probabilité du non évènement.

$$
\frac{p(X)}{ 1 -p(X)} = e^{\beta\_0 + \beta\_1*X}
$$

* Odds ratio  \\( \in [0, +\infty] \\)
* Odds ratio proche de  0: probabilité faible que l'évènement survienne
* Odds ratio s'approchant de  \\( +\infty \\) : probabilité forte que l'évènement survienne

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Log-Odds ratio
Si on prends le log du *odds-ratio* on a le **log odds ratio**

$$
log(\frac{p(X)}{ 1 -p(X)}) = \beta\_0 + \beta\_1 X
$$

Cela mesure l'influence d'une variable sur la cible.

Si \\( \beta\_1 \\)  augmente, alors  \\( p(X)\\) augmente aussi.

C'est moins direct que dans le cas de la régression linéaire.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Log-Odds ratio: application
Sur le data set default:

* On accroit / décroit le compte en banque de 10k
* On accroit / décroit le revenu de 10k

## Exemple in the default dataset

\\( p(X) = 0.2 \iff  \frac{0.2}{1 -0.2} = 0.25 \\)

* 1/5 people with ods 1/4 will default

\\( p(X) = 0.9 \iff  \frac{0.9}{1 -0.9} = 9 \\)

* 9 out of 10 people (90%)  with odds 9 will default


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Maximum de vraisemblance
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Métriques de classification
&amp;lt;img src=/assets/03/classification_metrics.png style='width:450px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Métriques de régression
&amp;lt;img src=/assets/03/regression_metrics.png style='width:450px;'&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Faux Positifs et Faux Négatifs
&amp;lt;img src=/assets/03/pregnant.jpg style='width:400px;border:0'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Métrique: Accuracy ou Précision

Correctement identifiés

* TP = True Positive - Vrai positif
* TN = True Negatives  - Vrai négatifs

Incorrectement identifiés

* FP = False Positive
* FN = False Negatives

## Accuracy

On définit la précision par

$$ Accuracy = \frac{ TP + TN  }{TP + FP + TN + FN}   $$

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Confusion matrix

![confusion matrix](/assets/03/confusion_matrix.png)
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# classification
![cats and dogs](/assets/03/cats_dogs.png)


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Ca se complique assez rapidement

&amp;lt;img src=/assets/03/confusion_matrix_wikipedia.png style='border:0'&amp;gt;

[https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Confusion matrix
Avec scikit:

        from sklearn.metrics import confusion_matrix
        y_true = [0,0,0,0,0,1,1,1,1,1]
        y_pred = [0,0,0,1,1,0,1,1,1,1]
        confusion_matrix(y_true, y_pred)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
Machine learning
&amp;lt;/p&amp;gt;
&amp;lt;p class=mitop&amp;gt;Au revoir les statistiques :)&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:30%&quot;&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/03/stats-vs_ml.jpeg&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left;width:50%;&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;


# Des stats au machine learning

* Statsmodel est dans une approche **statistique classique** qui favorise l'interprétabilité et l'analyse des prédicteurs

* Scikit-learn est dans une approche **machine learning** plus orientée vers la  robustesse et la prédiction

Au niveau de l'implémentation de la regression logistique dans les 2 librairies, la difference est que scikit ajoute une **contrainte** sur le modele au niveau de la fonction de cout.

Cette contrainte est appelé **régularisation** et sert à accroitre la capacité du modele a extrapoler sur des donnees nouvelles. On verra cela en detail dans 2 jours.

On est donc dans une transition de la modélisation statistique vers la modélisation machine learning.
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Regression avec scikit-learn

On va avoir des meta parametres.

Par exemple:

* accès à differents algorithmes  pour trouver les coefficients + un certain controle sur leur fonctionnement
* différentes façon de traiter le multi-class: ovr, multinomial
* différents mode de régularisation

et en output

* un modele que l'on peut appliquer a de nouvelles donnees
* les intervals de confiance
* la ou les categories prédites
* les probabilités de prédiction (appartenance a la classe)



On n'aura plus:
- les p-value
- les tests statistiques
- le R^2 (pas directement en tout cas)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Scikit-learn LogisticRegression

http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

![scikit LogisticRegression](/assets/03/scikit-logistic-regression.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![](/assets/03/sklearn_01.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![](/assets/03/sklearn_02.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![](/assets/03/iris_screen_shot_01.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![](/assets/03/iris_screen_shot_02.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Demo Scikit-learn LogisticRegression

* Iris dataset
* Score
* ROC AUC
* Trouver le meilleur threshold (Acc, max P, min Neg, TPR, ...)
* Use predict_proba and a different threshold =&amp;gt; you should find a different confusion matrix


    import pandas as pd
    from sklearn import datasets, metrics
    from sklearn.linear_model import LogisticRegression


    iris = datasets.load_iris()
    clf = LogisticRegression()
    clf.fit(iris.data, iris.target)

    metrics.accuracy_score(y_test, y_hat )


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# AUX and ROC Curve

* TPR = \\( \frac{  TP }{ P}  = \frac{  TP }{ TP + FN} \\) aka **Sensitivity** or **Recall**
* FPR = \\( \frac{  FP }{ N}  = \frac{  FP }{ FP + TN} \\)  aka **Fall-out**

Le TPR / recall et le FPR varient en fonction du seuil. on obtient donc

### Receiver operating characteristic

ROC = TPR vs FPR pour different seuils

        sklearn.metrics.roc_curve returns TPR, FPR

plot to get the ROC Curve

### AUX and ROC Curve
The AUX is Area under the Curve

        sklearn.metrics.roc_auc_score

So what's your best model LR according to AUC?

voir aussi F1-score

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Exemples

* Marque de voiture: Audi, Renault, Ford, Fiat

Si on assigne un numero arbitraire à chaque marque de voiture on crée une hiérarchie:

Audi =&amp;gt;1 , Renault =&amp;gt; 2, Ford =&amp;gt; 3, Fiat =&amp;gt; 4

De meme:

* chien, chat, souris, poulet =&amp;gt; {1,2,3,4}

pourquoi le poulet serait *4* fois le chien ? Ca ne fait pas sens.


Parfois on peut quand meme assigner un chiffre à chaque categorie, catégories ordonnées

* enfant, jeune, adulte, vieux =&amp;gt; {1,2,3,4}
* negatif, neutre, positif =&amp;gt; {-1, 0, 1}


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# One hot encoding et label encoding

Comment traduire les variables quantitative en variables numeriques

Binaires
* Oui / Non ; 1 /0
* Homme / Femme
* Spam / legit
* Action: Achete, enregistre,
* Identification


Multinomiales
* liste de villes, pays, destinations,
* tranche d'age
* niveau d'etude
* marques de voiture

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# One hot encoding

[One Hot Encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), ou pandas.get_dummies

Si on a **N** classes, on crée **N-1** variables binaires:

par exemple la variable ```animal_type: chien, chat, souris, poulet``` sera transformée en 3 variables binaires

* est_ce_chien : 1/0
* est_ce_chat : 1/0
* est_ce_souris : 1/0

La variable *est_ce_poulet* étant redondante et automatiquement déduite des 3 autres.

# LabelEncoder
[LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) associe un chiffre a chaque classe, on garde l'ordonnancement

* enfant, jeune, adulte, vieux =&amp;gt; {1,2,3,4}

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Recap
* regression logistique
* approche stats vs approche ML
* matrice de confusion
* ROC-AUC
* One hot encoding
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Resources
* Logistic Regression: Why sigmoid function?
https://github.com/rasbt/python-machine-learning-book/blob/master/faq/logistic-why-sigmoid.md

* scikit-learn documentation: Logistic regression,
http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression

* No, Machine Learning is not just glorified Statistics
https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3

* on stackexchange When to use One Hot Encoding vs LabelEncoder vs DictVectorizor? https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor


&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/3-regression-logistique</link>
        <guid isPermaLink="true">http://localhost:4000/3-regression-logistique</guid>
        
        
      </item>
    
      <item>
        <title>4) Biais, variance et gradient stochastique</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Biais, variance et gradient stochastique&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt; avec scikit-learn&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* Régression logistique
* Approche statistique ou approche Machine Learning
* Métriques de classification:
    * TP, TN, FP, FN
    * Matrice de confusion
    * ROC-AUC
* One hot encoding
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    ## Lab:
    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:60%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Aujourd'hui

* Scikit-learn
* Gradient Stochastique; Stochastic Gradient Descent
* Décomposition Biais - Variance

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;

    * [scikit-learn.org](http://scikit-learn.org/stable/)
    * [Eco-système](http://scikit-learn.org/stable/related_projects.html)

    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:60%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Scikit-learn

Libraries open source pour le machine learning créée 2010 dans le cadre du summer of code de Google
* Librairie basée sur numpy, scipy
* Contributeurs principaux :  Olivier Grisel, Andreas Muller, Gael Varoquaux, Jake Vanderplas =&amp;gt; [lien github](github.com)
* Un projet soutenu par
    * [INRIA](http://www.inria.fr),
    * [Telecom ParisTech ](http://www.telecom-paristech.fr/),
    * NYU
* Largement utilisé dans la communauté ML
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Algorithmes et modèles:
Grand choix d'algorithmes et de modèles

## Supervisé
* Classification:
    * SVM, nearest neighbors, random forest, XGBoost, AdaBoost, ...

* Regression:
    * SVM, SGD, ridge et Lasso, regression lineaire, naive bayes, ...

## Non supervisé
* Clustering: Grouper des échantillons *similaires* ou *proches*
    * k-Means, spectral clustering, mean-shift, ...

* Reduction de dimension: Réduire le nombre de variables
    * Applications: Visualization, Performance
    * PCA, feature selection, non-negative matrix factorization.
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:70%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Scikit learn

Mais aussi :
* Sélectionner les modèles: comparer, valider et choisir les paramètres et modèles
    * But: trouver les paramètres qui offrent les meilleurs performances
    * Modules: grid search, cross validation, metrics.

* Pre-processing: Transformation des variables.
    * But: Transformer les variables brutes pour améliorer leur pertinence et les numériser.
    * Modules: preprocessing, feature extraction.

* Documentation très complete avec de nombreux exemples
* Capable de traiter différents types de données: images, textes, données numeriques

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# scikit-learn API
Simple et cohérente

1. Instancier un modèle, par exemple une regression linéaire:
    * ```from sklearn.Linear import LinearRegression```
    * ```mdl = LinearRegression( meta-params, loss function, ...)```
2. Entrainer le modèle
    * ```mdl.fit(X, y)```
3. Obtenir des prédictions sur de nouvelles données
    * ```y_hat = mdl.predict(Nouveaux échantillons}```.
    * ```y_hat = mdl.predict_proba(Nouveaux échantillons)```
4. Obtenir un score
    * ```mdl.score(X,y)```

&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# paramétrer les modèles

* les meta-paramètres du modèle: \\(\alpha, \epsilon, \beta, \gamma, \cdots \\)
* la regularisation: *penalty, l1, l2*
* la fonction de cout: *loss*
* la gestion des itérations: *max_iter, n_iter*
* data pre-processing: *normalize, shuffle, valeurs manquantes*

[![Lin reg params](/assets/04/scikit_linear_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)
[![SGD reg params](/assets/04/scikit_sgd_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: Biais - Variance
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=assets/04/bias-variance-targets.png style='width:500px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:50%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Décomposition Biais - Variance
L'erreur de prédiction peut etre décomposée en 2 termes

$$
\text{Erreur totale} = \text{Erreur du biais} + \text{Erreur de la variance}
$$

**Biais**: la différence entre les predictions du modele et la valeur cible. Le biais mesure la performance du modèle, la distance entre les predictions et les valeurs cibles.

* **Underfitting**: Un biais important indique que le modele n'arrive pas  à comprendre les données qui lui sont fournies

**Variance**: Il s'agit là de la variabilité des prédictions entre différentes *réalisations* du modèle pour un échantillon donné.

La variance mesure la sensibilité du modèle aux données d'apprentissages

* **Overfitting**: Une forte erreur de variance indique que le modèle ne pourra pas extrapoler ses prédictions sur des nouvelles données.
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width : 45%;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* **Biais**: L'espérance de  l'erreur de prédiction

* **Variance**: Variance des prédictions.

    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:45%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Décomposition Biais - Variance
L'erreur quadratique est définie par:

$$ \operatorname{MSE}( \hat{y} ) = \frac {1}{n} \sum_{i=1}^{n}( \hat{y_i}-y_i )^2 = \mathbb{E} \big[ (\hat{y} - y)^2   \big] $$


Et on peut réécrire cette équation de la façon suivante:

$$ \operatorname{MSE}( \hat{y} )   =\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] + \left(\mathbb{E}(\hat{y})-y\right)^2 $$

Soit
$$ \operatorname{MSE}( \hat{y} )= \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2 $$

avec

$$ \operatorname{Var}(\hat{y}) =  \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] $$
$$ \operatorname{Bias}(\hat{y},y)  = \mathbb{E}(\hat{y})-y  $$

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Le calcul
&lt;/div&gt;
$$
\begin{align} \mathbb{E}((\hat{y}-y)^2)&amp;amp;=
 \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})+\mathbb{E}(\hat{y})-y\right)^2\right]
\\ &amp;amp; =
\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2 +2\left((\hat{y}-\mathbb{E}(\hat{y}))(\mathbb{E}(\hat{y})-y)\right)+\left( \mathbb{E}(\hat{y})-y \right)^2\right]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\mathbb{E}\Big[(\hat{y}-\mathbb{E}(\hat{y}))(\overbrace{\mathbb{E}(\hat{y})-y}^{\begin{smallmatrix} \text{This is} \\  \text{a constant,} \\ \text{so it can be} \\  \text{pulled out.} \end{smallmatrix}}) \,\Big] + \mathbb{E}\Big[\,\overbrace{\left(\mathbb{E}(\hat{y})-y\right)^2}^{\begin{smallmatrix} \text{This is a} \\  \text{constant, so its} \\  \text{expected value} \\  \text{is itself.} \end{smallmatrix}}\,\Big]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\underbrace{\mathbb{E}(\hat{y}-\mathbb{E}(\hat{y}))}_{=\mathbb{E}(\hat{y})-\mathbb{E}(\hat{y})=0}(\mathbb{E}(\hat{y})-y)+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2
\end{align}

$$
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
![underfit - overfit](/assets/04/underfit_overfit.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
Pour réduire l'erreur quadratique, il faut minimiser à la fois le **biais** et la **variance**.

* Biais important &amp;lt;-&amp;gt; sous estimation - Underfitting
    * le modele n'est pas bon
    * On obtient de mauvais score

* Variance importante &amp;lt;-&amp;gt; Overfitting
    * Le modele est trop **sensible** au training dataset
    * pouvoir d'extrapolation faible, mauvaises performances sur des nouvelles données.

Mais comment détecter l'overfit ?

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Underftiing


* Ajouter des predicteurs
* Rendre le modele plus complexe
* Attenuer la regularisation.
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Overfitting

* Reduire le nombre de predicteurs
* Utiliser plus de données d'apprentissage
* Accroitre la reguilarisation
* Moyenner plusieurs modèles

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# split: train vs test dataset

* On entraine le modele sur un **dataset de training**
* Mais le but est d'avoir un modele capable de traiter des **données nouvelles**, données qu'il n'a pas vu auparavant
* et on veut surtout éviter **l'Overfit**: *Le modele est trop **sensible** au training dataset*

Donc on va mettre de coté  une partie des données comme données nouvelles: dataset de test.

Typiquement : une répartition  80/20 ou 70/30

&amp;lt;img src=/assets/04/split_train_test.png style='width: 500px;  margin:auto; ' &amp;gt;

Et c'est en évaluant le modele sur les données de test que l'on va pouvoir détecter l'overfit

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# train - test split

&amp;gt; Demo sur iris dataset

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    [scikit example 1](http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve)

    [scikit example 2](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py)
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:70%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;




# Comment détecter l'overfit ?
### Courbes d'apprentissages - Learning curves



* On met de coté un set d'apprentissage (20% des données)
* on entraine le modele sur un nombre croissant d'echantillons (10%, 20%, ...)
* Pour chaque réalisation on calcule
    * l'erreur sur le set d'apprentissage
    * l'erreur sur le set de test

* En accroissant le set d'apprentissage, le modele a de plus en plus d'info, le modele apprends le set d'apprentissage. On espere que ca va lui permettre de traiter aussi les données sur le set de test.

Ce que l'on observe:
* avec un set d'apprentissage petit, les 2 erreurs sont grandes
* avec plus de données, l'erreur d'apprentissage décroit
    * si l'erreur sur le set de test ne décroit pas: **overfit**!

Si l'erreur sur le set de test ne décroit pas, alors cela veut dire que le modele n'est pas capable d'extrapoler sur des nouvelles données

Note: si l'erreur ne décroit pas sur le set de training en premier lieu, alors cela veut dire que le modele est mauvais, que l'erreur de biais est forte.

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Illustration

### learning curve

* underfit
* overfit


&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Detecter l'overfit -

Demo sur ames housing avec SGD

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Validation croisée
Si on a peu de données, le split train - test *gaspille* des données pour le test. Données qui pourraient etre utile pour l'apprentissage du modele.

=&amp;gt; on va alterner le découpage train - test, 80% - 20%,

C'est la validation croisée et plus particulièrement **K-FOLD cross validation**

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# K-fold cross-validation

&amp;lt;img src=/assets/04/k-fold-cross-validation.png style='width: 600px;  margin:auto; float:right;' &amp;gt;

1. Mélanger le dataset
2. Puis découper le dataset en K (5) parties
3. Faire K (5) experiences:
    * apprentissage sur 1,2,3,4 et evaluation sur 5
    * apprentissage sur 1,2,3,**5** et evaluation sur **4**
    * ...
    * apprentissage sur 2,3,4,5 et evaluation sur **1**

La moyenne des scores obtenus ainsi est plus robuste qu'un score obtenu sur un unique découpage.

[K-fold cross validation - scikit-learn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Autres méthodes de validation croisée

* **Stratified K-Fold**: (classification) Chaque subset conserve la distribution des classes. Utile lorsque la repartition des classes est déséquilibrée.
* **Leave one out**:  Chaque échantillon est utilisé à son tour comme echantillon de test. Tous les autres sont laissé dans le set d'apprentissage.
* **Shuffle cross validation**: Decoupage aléatoire avec remise en place. rien n'oblige à fixer le découpage au début.


### scikit-learn
* [cross_val_score](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_digits.html#example-exercises-plot-cv-digits-py)
* [cross validation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)


&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# K-fold cross validation
Exercise

On the diabetes dataset, find the optimal regularization parameter alpha.

Bonus: How much can you trust the selection of alpha?

http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold

http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: Gradient Stochastique
&amp;lt;/p&amp;gt;
&amp;lt;p class=mitop&amp;gt;
Stochastic Gradient Descent
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Illustration
&amp;lt;img src=/assets/04/robins_monroe_1951.png style='width: 500px; margin: auto; float:right; '&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Methode du gradient

**1951!** Robbins - Monroe: A Stochastic approximation Method

Soit une function \\(  f \\) dont on souhaite trouver le minimum.

Sous certaines conditions sur  \\(  \alpha \\) et \\(\hat{\nabla} f\\) (gradient de f), alors

$$  {\bf w}_{t+1} = {\bf w}_t - \alpha_t \hat{\nabla} f({\bf x}_t) $$

si \\(  \alpha \\) assez petit et si f est différentiable

Alors \\( {\bf w}_t  \\) converge vers le / un minima de \\(f\\)

*decreases fastest in the direction of the negative gradient of f*

$$ f(\mathbf {w}\_{0})\geq f(\mathbf {w}\_{1}) \geq f(\mathbf {w}\_{2})\geq \cdots , $$

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En python

On veut trouver le minimum de la fonction  \\( f(x) =   \\)

    cur_x       = 6 # The algorithm starts at x=6
    gamma       = 0.01 # step size multiplier
    max_iters   = 10000 # maximum number of iterations
    iters       = 0 #iteration counter
    precision   = 0.00001
    previous_step_size = 1

    # dérivée de la fonction à minimiser

    fct = lambda x: 4 * x**3 - 9 * x**2
    x = []
    while (previous_step_size &amp;gt; precision) &amp;amp; (iters &amp;lt; max_iters):
        x.append(cur_x)
        prev_x = cur_x
        cur_x -= gamma * fct(prev_x)
        print(cur_x, previous_step_size)
        previous_step_size = abs(cur_x - prev_x)
        iters+=1

    print(&quot;Le minimum est {:.4f}&quot;, cur_x)

    print(&quot;En x = {:.4f}, la valeur de la fonction est {:.4f}  &quot;.format(cur_x, fct(cur_x)) )


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Stochastic Gradient

The general idea is to aproximate an unknown function through iterations of **unbiased estimates of the function's gradient.**

Knowing that the expectation of the gradient estimates equal the gradient.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Gradient Descent

**1951!** Robbins - Monroe: A Stochastic approximation Method

&amp;lt;img src=/assets/04/robins_monroe_1951.png style='width: 500px; margin: auto; float:right; '&amp;gt;


Soit une function \\(  f \\) que l'on souhaite approximer.
Sous certaines conditions sur  \\(  \alpha \\) et \\(\hat{\nabla} f\\) (gradient de f), alors \\(  \\)

$$  {\bf w}_{t+1} = {\bf w}_t - \alpha_t \hat{\nabla} f({\bf x}_t) $$
$$ {\bf w}_t -&amp;gt; f  $$

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

Dans notre contexte, la fonction \\(f\\) est une fonction de régression linéaire d'ordre N avec les coefficients \\(w_k \text{with} k \in [0..N]\\)

$$f(x) = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_N x_N $$

On veut donc minimiser l'erreur

$$ e = y - f(X) = t - W^T X $$

Cette equation admet une solution exacte que l'on a vu precedemment

$$\hat{W} = ()() $$

Au lieu de calculer la solution directement on va l'estimer par la methode du gradient:

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Methode du gradient

Equation

Neanmoins cela nécessite de calculer le gradient sur tous les échantillons disponibles a la fois. Pour un dataset grand, c'est couteux et long.

Donc on va utiliser le fait que sous certaines conditions

le gradient peut etre estimé par la moyenne des

The SGD algorithm is low on computation, has good convergence behavior and is applicable to many different situations through its many available variants. The SGD is implemented in a wide variety of languages (python, R, Java, scala, matlab, ...) and platforms (weka, rapidminer, ...) as well as online services big (Amazon ML, Google, Azure) and small (Dataiku, ....)

The idea of iterative stochastic approximation Robbins and Monro in 1951 in a seminal paper titled A Stochastic approximation Method

The literature related to the SGD algorithm is abundant. With the resurgence of depp learning and neural networks, no less than 6000 academic papers about Stochastic Gradient Descent were published in 2016 according to google scholar (from around a few 100s in the early 2000s).



According to the Gauss-Markov theorem, the model fit by the Ordinary Least Squares (OLS) is the least biased estimator of all possible estimators. In other words, it fits the data it has seen better than all possible models.

Calculer la solution a partir de l'equation ci dessus est couteuse en calcul

On va donc approximer la solution de facon iterative

* on choisit un vercteor W_0 pour initialiser l'algo
et a chaque iteration on corrige W par le gradient de la fonction de cout


see [raschka](https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html)
The cost function J(⋅), the sum of squared errors (SSE), can be written as:

The magnitude and direction of the weight update is computed by taking a step in the opposite direction of the cost gradient

where η is the learning rate. The weights are then updated after each epoch via the following update rule:

=&amp;gt; learning rate

~[gradient as ball](/assets/04/gradient_ball.png)

see file:///Users/alexis/amcp/packt-B05028/B05028_07_draft.html

In case of very large datasets, using GD can be quite costly since we are only taking a single step for one pass over the training set – thus, the larger the training set, the slower our algorithm updates the weights and the longer it may take until it converges to the global cost minimum

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

La dimenion stichastique consiste a actualiser le W non plus avec l'integralité du gradient sur toutes les donnees mais avec une estimation du gradient echantillon par echantillon.
Cela marche parce que dans notre contexte: E(gradient) = gradient

Instead, we update the weights after each training sample:

et on passe plusieurs fois sur le dataset en shufflant le dataset a chaque fois (epoch)

“stochastic” comes from the fact that the gradient based on a single training sample is a “stochastic approximation” of the “true” cost gradient

it has been shown that SGD almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)


## Mini-Batch Gradient Descent (MB-GD)
milieu entre le GD et le SGD
On utilise K echantillons pour estimer le gradient a chaque iteration

Accroitre K entraine

* on converge plus vite et plus directement. moins de zig zag
* on converge moins profondement. le biais est plus important.


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Outliers, detection and impact

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Skewness: Box cox and Kurtosis

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# resources
https://towardsdatascience.com/predicting-housing-prices-using-advanced-regression-techniques-8dba539f9abe



* Data Split
    train, test, valid http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/18
    k-fold http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/20
    http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/21
    * stratified k-fold http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/22
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/4-biais-variance-gradient-stochastique</link>
        <guid isPermaLink="true">http://localhost:4000/4-biais-variance-gradient-stochastique</guid>
        
        
      </item>
    
      <item>
        <title>4) Gradient, Stochastique,  Biais, Variance</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Biais, variance et gradient stochastique&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt; avec scikit-learn&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* Régression logistique
* Approche statistique ou approche Machine Learning
* Métriques de classification:
    * TP, TN, FP, FN
    * Matrice de confusion
    * ROC-AUC
* One hot encoding
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
I: Rapide Rappel Scikit-learn
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# scikit-learn API
Simple et cohérente

1. Instancier un modèle, par exemple une regression linéaire:
    * ```from sklearn.Linear import LinearRegression```
    * ```mdl = LinearRegression( meta-params, loss function, ...)```
2. Entrainer le modèle
    * ```mdl.fit(X, y)```
3. Obtenir des prédictions sur de nouvelles données
    * ```y_hat = mdl.predict(Nouveaux échantillons}```.
    * ```y_hat = mdl.predict_proba(Nouveaux échantillons)```
4. Obtenir un score
    * ```mdl.score(X,y)```

&lt;/div&gt;
&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# paramétrer les modèles

* les meta-paramètres du modèle: \\(\alpha, \epsilon, \beta, \gamma, \cdots \\)
* la regularisation: *penalty, l1, l2*
* la fonction de cout: *loss*
* la gestion des itérations: *max_iter, n_iter*
* data pre-processing: *normalize, shuffle, valeurs manquantes*

[![Lin reg params](/assets/04/scikit_linear_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)
[![SGD reg params](/assets/04/scikit_sgd_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    ## Lab: Titanic la suite
    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:60%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Aujourd'hui

* Gradient et Gradient Stochastique;
* Décomposition Biais - Variance

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:35%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:50%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Methode du gradient

Soit une function \\(  f \\) dont on souhaite trouver le minimum.

Pour \\(  \alpha \\)  assez petit et si f est dérivable

alors

$$  {\bf w}_{t+1} = {\bf w}_t - \alpha_t \hat{\nabla} f({\bf w}_t) $$

\\( {\bf w}_t  \\) converge vers le minima de \\(f\\)

On a

$$ f(\mathbf {w}\_{0})\geq f(\mathbf {w}\_{1}) \geq f(\mathbf {w}\_{2})\geq \cdots , $$


=&amp;gt; *fastest  decrease obtained for the direction of the negative gradient of f*
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/04/700px-Gradient_descent.svg.png width=350px&amp;gt;
&amp;lt;img src=/assets/04/700px-Gradient_ascent_contour.png width=350px&amp;gt;
&amp;lt;img src=/assets/04/Gradient_ascent_surface.png width=350px&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En python

On veut trouver le minimum de la fonction  \\( f(x) =   \\)

    cur_x       = 6 # The algorithm starts at x=6
    gamma       = 0.01 # step size multiplier
    max_iters   = 10000 # maximum number of iterations
    iters       = 0 #iteration counter
    precision   = 0.00001
    previous_step_size = 1

    # dérivée de la fonction à minimiser

    fct = lambda x: 4 * x**3 - 9 * x**2
    x = []
    while (previous_step_size &amp;gt; precision) &amp;amp; (iters &amp;lt; max_iters):
        x.append(cur_x)
        prev_x = cur_x
        cur_x -= gamma * fct(prev_x)
        print(cur_x, previous_step_size)
        previous_step_size = abs(cur_x - prev_x)
        iters+=1

    print(&quot;Le minimum est {:.4f}&quot;, cur_x)

    print(&quot;En x = {:.4f}, la valeur de la fonction est {:.4f}  &quot;.format(cur_x, fct(cur_x)) )


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Stochastic Gradient

The general idea is to aproximate an unknown function through iterations of **unbiased estimates of the function's gradient.**

Knowing that the expectation of the gradient estimates equal the gradient.

On va prendre les erreurs successives entre les vraies valeurs et leur estimée comme estimation du gradient!

Cela marche parce que dans notre contexte: E(gradient) = gradient

&amp;lt;img src=/assets/04/robins_monroe_1951.png style='width: 500px; margin: auto; float:right; '&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Fonction de cout, forme générale

On cherche a minimiser la fonction \\( f(x) = w^T x + b   \\)

$$ E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w) $$

where

* \\(L\\) is a loss function that measures model (mis)fit
* \\(R\\) is a regularization term  that penalizes model complexity
* \\(\alpha\\) is a non-negative hyperparameter.

En fonction du choix de \\(L\\) et de \\(R\\) on obtient différent algorithmes

=&amp;gt; http://scikit-learn.org/stable/modules/sgd.html

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Gradient stochastique
&amp;lt;img src=/assets/04/gradient_ball.png height=300px&amp;gt;

* low on computation,
* has good convergence behavior
* The SGD is implemented in a wide variety of languages (python, R, Java, scala, matlab, ...) and platforms (weka, rapidminer, ...) as well as online services big (Amazon ML, Google, Azure) and small (Dataiku, ....)


The literature related to the SGD algorithm is abundant. With the resurgence of deep learning and neural networks, no less than 6000 academic papers about Stochastic Gradient Descent were published in 2016 according to google scholar (from around a few 100s in the early 2000s).


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# SGD

* learning rate
    * tuning
    * fixe
    * adaptif

* epoch and shuffling

## Mini-Batch Gradient Descent (MB-GD)
milieu entre le GD et le SGD
On utilise K echantillons pour estimer le gradient a chaque iteration

Accroitre K entraine

* on converge plus vite et plus directement. moins de zig zag
* on converge moins profondement. le biais est plus important.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;



Instead, we update the weights after each training sample:

et on passe plusieurs fois sur le dataset en shufflant le dataset a chaque fois (epoch)

“stochastic” comes from the fact that the gradient based on a single training sample is a “stochastic approximation” of the “true” cost gradient

it has been shown that SGD almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: Biais - Variance
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=assets/04/bias-variance-targets.png style='width:500px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:50%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Décomposition Biais - Variance
L'erreur de prédiction peut etre décomposée en 2 termes

$$
\text{Erreur totale} = \text{Erreur du biais} + \text{Erreur de la variance}
$$

**Biais**: la différence entre les predictions du modele et la valeur cible. Le biais mesure la performance du modèle, la distance entre les predictions et les valeurs cibles.

* **Underfitting**: Un biais important indique que le modele n'arrive pas  à comprendre les données qui lui sont fournies

**Variance**: Il s'agit là de la variabilité des prédictions entre différentes *réalisations* du modèle pour un échantillon donné.

La variance mesure la sensibilité du modèle aux données d'apprentissages

* **Overfitting**: Une forte erreur de variance indique que le modèle ne pourra pas extrapoler ses prédictions sur des nouvelles données.
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width : 45%;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* **Biais**: L'espérance de  l'erreur de prédiction

* **Variance**: Variance des prédictions.

    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:45%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Décomposition Biais - Variance
L'erreur quadratique est définie par:

$$ \operatorname{MSE}( \hat{y} ) = \frac {1}{n} \sum_{i=1}^{n}( \hat{y_i}-y_i )^2 = \mathbb{E} \big[ (\hat{y} - y)^2   \big] $$


Et on peut réécrire cette équation de la façon suivante:

$$ \operatorname{MSE}( \hat{y} )   =\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] + \left(\mathbb{E}(\hat{y})-y\right)^2 $$

Soit
$$ \operatorname{MSE}( \hat{y} )= \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2 $$

avec

$$ \operatorname{Var}(\hat{y}) =  \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] $$
$$ \operatorname{Bias}(\hat{y},y)  = \mathbb{E}(\hat{y})-y  $$

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Le calcul
&lt;/div&gt;
$$
\begin{align} \mathbb{E}((\hat{y}-y)^2)&amp;amp;=
 \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})+\mathbb{E}(\hat{y})-y\right)^2\right]
\\ &amp;amp; =
\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2 +2\left((\hat{y}-\mathbb{E}(\hat{y}))(\mathbb{E}(\hat{y})-y)\right)+\left( \mathbb{E}(\hat{y})-y \right)^2\right]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\mathbb{E}\Big[(\hat{y}-\mathbb{E}(\hat{y}))(\overbrace{\mathbb{E}(\hat{y})-y}^{\begin{smallmatrix} \text{This is} \\  \text{a constant,} \\ \text{so it can be} \\  \text{pulled out.} \end{smallmatrix}}) \,\Big] + \mathbb{E}\Big[\,\overbrace{\left(\mathbb{E}(\hat{y})-y\right)^2}^{\begin{smallmatrix} \text{This is a} \\  \text{constant, so its} \\  \text{expected value} \\  \text{is itself.} \end{smallmatrix}}\,\Big]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\underbrace{\mathbb{E}(\hat{y}-\mathbb{E}(\hat{y}))}_{=\mathbb{E}(\hat{y})-\mathbb{E}(\hat{y})=0}(\mathbb{E}(\hat{y})-y)+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2
\end{align}

$$
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
![underfit - overfit](/assets/04/underfit_overfit.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
Pour réduire l'erreur quadratique, il faut minimiser à la fois le **biais** et la **variance**.

* Biais important &amp;lt;-&amp;gt; sous estimation - Underfitting
    * le modele n'est pas bon
    * On obtient de mauvais score

* Variance importante &amp;lt;-&amp;gt; Overfitting
    * Le modele est trop **sensible** au training dataset
    * pouvoir d'extrapolation faible, mauvaises performances sur des nouvelles données.

Mais comment détecter l'overfit ?

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Underftiing


* Ajouter des predicteurs
* Rendre le modele plus complexe
* Attenuer la regularisation.
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Overfitting

* Reduire le nombre de predicteurs
* Utiliser plus de données d'apprentissage
* Accroitre la reguilarisation
* Moyenner plusieurs modèles

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
IV: K-fold cross validation
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# split: train vs test dataset

* On entraine le modele sur un **dataset de training**
* Mais le but est d'avoir un modele capable de traiter des **données nouvelles**, données qu'il n'a pas vu auparavant
* et on veut surtout éviter **l'Overfit**: *Le modele est trop **sensible** au training dataset*

Donc on va mettre de coté  une partie des données comme données nouvelles: dataset de test.

Typiquement : une répartition  80/20 ou 70/30

&amp;lt;img src=/assets/04/split_train_test.png style='width: 500px;  margin:auto; ' &amp;gt;

Et c'est en évaluant le modele sur les données de test que l'on va pouvoir détecter l'overfit

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    [scikit example 1](http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve)

    [scikit example 2](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py)
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:70%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Comment détecter l'overfit ?
### Courbes d'apprentissages - Learning curves

* On met de coté un set d'apprentissage (20% des données)
* on entraine le modele sur un nombre croissant d'echantillons (10%, 20%, ...)
* Pour chaque réalisation on calcule
    * l'erreur sur le set d'apprentissage
    * l'erreur sur le set de test

* En accroissant le set d'apprentissage, le modele a de plus en plus d'info, le modele apprends le set d'apprentissage. On espere que ca va lui permettre de traiter aussi les données sur le set de test.

Ce que l'on observe:
* avec un set d'apprentissage petit, les 2 erreurs sont grandes
* avec plus de données, l'erreur d'apprentissage décroit
    * si l'erreur sur le set de test ne décroit pas: **overfit**!

Si l'erreur sur le set de test ne décroit pas, alors cela veut dire que le modele n'est pas capable d'extrapoler sur des nouvelles données

Note: si l'erreur ne décroit pas sur le set de training en premier lieu, alors cela veut dire que le modele est mauvais, que l'erreur de biais est forte.

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Validation croisée
Si on a peu de données, le split train - test *gaspille* des données pour le test. Données qui pourraient etre utile pour l'apprentissage du modele.

=&amp;gt; on va alterner le découpage train - test, 80% - 20%,

C'est la validation croisée et plus particulièrement **K-FOLD cross validation**

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# K-fold cross-validation

&amp;lt;img src=/assets/04/k-fold-cross-validation.png style='width: 600px;  margin:auto; float:right;' &amp;gt;

1. Mélanger le dataset
2. Puis découper le dataset en K (5) parties
3. Faire K (5) experiences:
    * apprentissage sur 1,2,3,4 et evaluation sur 5
    * apprentissage sur 1,2,3,**5** et evaluation sur **4**
    * ...
    * apprentissage sur 2,3,4,5 et evaluation sur **1**

La moyenne des scores obtenus ainsi est plus robuste qu'un score obtenu sur un unique découpage.

[K-fold cross validation - scikit-learn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Autres méthodes de validation croisée

* **Stratified K-Fold**: (classification) Chaque subset conserve la distribution des classes. Utile lorsque la repartition des classes est déséquilibrée.
* **Leave one out**:  Chaque échantillon est utilisé à son tour comme echantillon de test. Tous les autres sont laissé dans le set d'apprentissage.
* **Shuffle cross validation**: Decoupage aléatoire avec remise en place. rien n'oblige à fixer le découpage au début.


### scikit-learn
* [cross_val_score](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_digits.html#example-exercises-plot-cv-digits-py)
* [cross validation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Overfit - Biais - Variance SGD

Demo sur cars avec SGD

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: Gradient, Stochastique
&amp;lt;/p&amp;gt;
&amp;lt;p class=mitop&amp;gt;
Stochastic Gradient Descent
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/4bis-biais-variance-gradient-stochastique</link>
        <guid isPermaLink="true">http://localhost:4000/4bis-biais-variance-gradient-stochastique</guid>
        
        
      </item>
    
      <item>
        <title>5)  Arbres, Random Forests et XGBoost</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Arbres, Random Forests et XGBoost&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;Ensembling, Bagging, Boosting&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* SGD
* Biais Variance

&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
I: Arbres de décision
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Exemple arbre de décision sur Iris dataset
&amp;lt;img src =/assets/05/L12-tree-iris.png width=900px&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Mais

* **high overfitting** for over-complex trees that do not generalise the data well.
* Decision trees can be **unstable** because small variations in the data might result in a completely different tree being generated.
* no globally optimal decision tree

&lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Avantages

Robustes, rapides et interpretables


* Simple to understand and to interpret. Trees can be visualised.
* Requires little data preparation. (missing values, scaling, dummy variables, ...)
* Can handle both numerical and categorical data.
* Possible to validate a model using statistical tests.
* Uses a white box model. An observed situation can simply be explained by boolean logic.


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Biais - Variance

## Deep

* Low bias, high variance
* Overfitting

## Shallow (short)

* High bias, low variance
* Underfitting


* Shallow decision trees have high bias and low variance.
* Deep decision trees have low bias and high variance.

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Lab: Controlling the tree

Lab: [Simple Decision tree](https://github.com/alexperrier/gads/blob/master/12_decision_trees/py/L12%20Simple%20Decision%20Tree%20-%20Iris%20dataset.ipynb)


Set these params to control the tree complexity

* **max_depth** (pruning): The maximum depth of the tree

* **min_samples_split**: The minimum number of samples required to split an internal node
* **min_samples_leaf**: The minimum number of samples required to be at a leaf node.
* **max_features**: The number of features to consider when looking for the best split


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
II: Bootstrap
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Example

* mean de a

1000 fois on sample a

on tire 200 echantillons avec remplacement

    a = [1,2,3,-1,-2,-3,4,-2,-2]

    m = []
    for i in range(1000):
        m.append(np.mean(random.choice(a, size = 200, replace = True)))

    plt.boxplot(m)

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Bootstrap

**Echantilloner avec remplacement**

N samples au total, N est petit (par ex. ~&amp;lt; 10)

* Comment estimer la moyenne de ces echantillons ?
* Est ce que la moyenne arithmetique classique est un bon estimateur ?

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Essayer sur le titanic

sklearn.tree.DecisionTreeClassifier

* Creer les train et test sets
* comme baseline: arbre de decision simple, not pruned, quel accuracy sur le test set ?
* maintenant prendre 20 arbres, en limitant la taille a 2 niveaux
* pour chaque arbre, predire les probas des echantillons du test set
* puis moyenner les proba et utiliser le resultat pour determiner la classe predite.
* quel accuracy sur le test set ?

=&amp;gt; 20 arbres biaisés valent mieux qu'un arbre *non contraint* qui overfit

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Bagging for trees

Bagging stands for Bootstrap Aggregation,


* Generate B different **bootstrapped** training data sets.
* Train a new tree on each training set

The predictions of all the trees are averaged

=&amp;gt; significantly reduces over fitting for deep trees

=&amp;gt; does it also reduce bias for shallow trees ?

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Bagging Classifier


The key intuition of Bagging is that it reduces the variance of your model class.

http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier


A **Bagging classifier** is an ensemble meta-estimator that fits **base classifiers** each on random subsets (bootstrapped) of the original dataset

The final prediction is aggregated from the models individual predictions  to form a final prediction.

* **voting**: most predicted class
* **averaging**: average of predictions (regression) or predicted probabilities (classification)

Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.

* base_estimator: The base model, decision tree by default, could also be another simple n

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: Random Forests
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Random Forests

Extension of bootstrapping to features

* In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.

* In addition, when splitting a node during the construction of the tree, the split that is picked is the best split among **a random subset of the features**.

=&amp;gt; The **bias slightly increases** compared to the bias of a single non-random tree

=&amp;gt; but, due to averaging, **its variance decreases**,

usually more than compensating for the increase in bias, hence yielding an overall better model.

A **Random Forest is a generalization of Bagging that is specific to Decision Trees**.

At each branch in the decision tree, Random Forest training also subsamples the features in addition to the training examples.

Intuitively, this process further de-correlates the individual trees, which is good for Bagging.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Out Of Bag - OOB

When boostrapping, in each experiment will use only approx. 2/3rd of the available samples.

Which leaves 1/3rd that we can use to estimate the validation error of each tree.

This is called OOB Out of Bag error.

It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Feature importance

* When the *max_features &amp;lt; total number of features*.

    =&amp;gt; Some features are left out of the splitting decision in each node.

* Relative Feature importance can be deduced from the delta in MSE associated to the features included vs left out.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Titanic

Quelles sont les variables les plus importantes ?

# Cars

Quelles sont les variables les plus importantes ?


etc ...
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
IV: XGBoost
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Adaboost

https://en.wikipedia.org/wiki/AdaBoost

Training
AdaBoost refers to a particular method of training a boosted classifier. A boost classifier is a classifier in the form

$$ F\_T(x) = \sum_{t=1}^T f\_t(x) $$


where each \\( f_{t} \\) is a weak learner that takes an object  \\(x\\) as input and returns a value indicating the class of the object.

Each weak learner produces an output hypothesis,  \\(h(x_i)\\), for each sample in the training set. At each iteration \\(t\\), a weak learner is selected and assigned a coefficient \\(\alpha\_t \\) such that the sum training error \\( E_{t}\\) of the resulting  \\(t\\)-stage boost classifier is minimized.

$$ E\_t = \sum\_i E[F\_{t-1}(x\_i) + \alpha\_t h(x\_i)] $$

Here \\( F\_{t-1}(x)\\) is the boosted classifier, \\(E(F)\\) is some error function and \\( f\_t(x) = \alpha\_t h(x) \\) is the new weak learner.

### Weighting
At each iteration of the training process, a weight {\displaystyle w_{t}} w_{t} is assigned to each sample in the training set equal to the current error {\displaystyle E(F_{t-1}(x_{i}))} E(F_{t-1}(x_i)) on that sample. These weights can be used to inform the training of the weak learner, for instance, decision trees can be grown that favor splitting sets of samples with high weights.

Derivation

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Gradient boosting

Keep an overall predictor that is the (weighted) average of a bunch of models.
Train first model on original training data, and initialize overall predictor as just this single model.
Assess the error of the the overall predictor and modify the training data the focus on areas of high error.
For AdaBoost, this means re-weighting the data points so that poorly modeled data points get higher weight.
For Gradient Boosting, this means redefining the supervised prediction target to be some kind of residual between the ground truth and the overall predictor.
Train a new model on the modified training data, and add to the overall predictor.
Repeat Steps 3 &amp;amp; 4.

A Gradient Boosting will take a different approach. It will start with a (usually) not very deep tree (sometimes a decision stump - a decision tree with only one split) and will model the original target. Then it takes the errors from the first round of predictions, and passes the errors as a new target to a second tree. The second tree will model the error from the first tree, record the new errors and pass that as a target to the third tree. And so forth. Essentially it focuses on modelling errors from previous trees. GB is one of the best algorithms available today and it’s almost always outperforming RF on most datasets I’ve tried.

Notice how RF runs trees in parallel, thus making it possible to parallelize jobs on a multiprocessor machine. GB instead uses a sequential approach.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

Random forests bags models, while boosting iteratively averages them with respect to error. XGBoost extends boosting by imposing regression penalties similar to elastic net.


One can interpret boosting as trying to minimize the bias of the overall predictor. So when you use boosting, you’re incentivized to use shallow decision trees because they have low variance and high bias. Using high variance base models in boosting runs a much higher risk of overfitting than approaches like Bagging.
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Fri, 05 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/5-arbres-random-forest-xgboost</link>
        <guid isPermaLink="true">http://localhost:4000/5-arbres-random-forest-xgboost</guid>
        
        
      </item>
    
      <item>
        <title>6) Texte &amp; Word2Vec</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Arbres, Random Forests et XGBoost&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;Ensembling, Bagging, Boosting&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent

&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
I: Text Mining - NLP
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Corpus

## texte brut

* forums, réseaux sociaux (peu structuré)
* plus structuré: discours, news, articles, emails, ...
* plus ou moins long: livres, articles scientifiques, abstracts, ...

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Applications

Simple et directe

* prediction, classification, identification
    * binaire: spam
    * multiclass: sujet du document

Non supervisée

* topic modeling

Avancée: productive

* Résumé
* Traduction automatique
* Chatbots

voir les nouvelles fonctionnalités de gmail

Interpretative

*  Sentiment analysis

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Librairies python

* spacy.io
* nltk
* gensim



Nombreuses librairies open source en R, Java, ...

# Resources

* Livre: Speech and Language Processing https://web.stanford.edu/~jurafsky/slp3/

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Cloud

* AWS Comprehend
* Google NLP
* Speech to text

# Arabic

* Stanford NLP: https://nlp.stanford.edu/projects/arabic.shtml
* Deep learning for Arabic NLP https://www.sciencedirect.com/science/article/pii/S1877750317303757


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right; width:30%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Chomsky
&amp;lt;img src=/assets/06/Syntactic_Structures_Noam_Chomsky_cover.jpg&amp;gt;

1957
    &lt;/div&gt;
&lt;/div&gt;


&lt;div style=&quot;float:left; width:30%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Ferdinand de Saussure
&amp;lt;img src=/assets/06/saussure.jpg&amp;gt;

1916

    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:right; width:30%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Benveniste
&amp;lt;img src=/assets/06/Benveniste-Emile-Problemes-De-Linguistique-Generale-Livre-684848349_L.jpg&amp;gt;
1966

    &lt;/div&gt;
&lt;/div&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Text generation

* https://ml5js.org/docs/lstm-interactive-example

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Transformations

* lemmatization,

    * la voiture est grande
    * Je suis sur un grand bateau

    * est, suis =&amp;gt; etre
    * grande, grand =&amp;gt; grand


* tokens, bi-grams
* stopwords: je, tu, il, et, me, sa, son, mais, donc, par, ....


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Numeriser le texte

Comment passer d'un texte libre a une matrice numérique ?

Approche **Bags of words**

## Tf-idf

Pour un mot donné dans un corpus de plusieurs documents

* Fréquence dans un document / frequence des mots dans les autres documents

* tf-idf means term-frequency times inverse document-frequency

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Cosine distance
&amp;lt;img src=/assets/06/cosine_similarity.png&amp;gt;

# Spacy
https://spacy.io/usage/vectors-similarity

    from gensim.models import Word2Vec

    #loading the downloaded model
    model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)

    #the model is loaded. It can be used to perform all of the tasks mentioned above.

    # getting word vectors of a word
    banana = model['banana']

    #performing king queen magic
    print(model.most_similar(positive=['woman', 'king'], negative=['man']))

    #picking odd one out
    print(model.doesnt_match(&quot;breakfast cereal dinner lunch&quot;.split()))

    #printing similarity index
    print(model.similarity('apple', 'orange'))
    print(model.similarity('car', 'orange'))

see word2vec_demo.py
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## Word2vec et Glove

* Approche très recente qui associe un vecteur de grande dimension (128, 256, ...) a des milliers de mots

* Comme on a des vecteurs on a une distance entre les mots. Cosine distance

* Corpus original: Wikipedia

* Capture du *sens* du mot

    * Reine - femme = Roi - homme
    * Rabat - capitale = Paris - capitale




    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Word2vec
Word2vec is not a single algorithm but a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model.

* Skip – gram : to predict the context given a word
* CBOW tends to predict the probability of a word given a context
* word2vec is a &quot;predictive&quot; model, predict word / context + context / word

http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf

### see also Glove

GloVe is a &quot;count-based&quot; model :Dimensionality reduction on the co-occurrence counts matrix.

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/06/skip_gram_net_arch.png&amp;gt;

Predictive models learn their vectors in order to improve their predictive ability of Loss(target word | context words; Vectors), i.e. the loss of predicting the target words from the context words given the vector representations. In word2vec, this is cast as a feed-forward neural network and optimized as such using SGD,

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# TF-IDF - sklearn

The most intuitive way to do so is to use a bags of words representation:

Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).

For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary.

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Topic modeling

&amp;lt;img src=/assets/06/lsa_decomposition_example_03.png&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
II: Lab : text classification sur bbc dataset
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/6-text-classification</link>
        <guid isPermaLink="true">http://localhost:4000/6-text-classification</guid>
        
        
      </item>
    
      <item>
        <title>7) Support Vector Machines, Imbalanced datasets</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;Support Vector Machines&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;Imbalanced datasets&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
I: Support Vector Machines
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/07/vladimir-vapnik-500x435.jpg&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:40%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;
#  Владимир Наумович Вапник

Vladimir Naumovich Vapnik (Russian: Владимир Наумович Вапник; born 6 December 1936) is one of the main developers of the Vapnik–Chervonenkis theory of statistical learning, and the co-inventor of the support vector machine method (1998).



&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Linearly separable

We have n samples in p dimensions  $$ X = \\{ x_{i}  \\}  \quad i \in [1,n]  $$

which belong to two classes \\( y_i \in \\{-1, +1 \\} \\)


**Def:** The classes are **linearly separable** \\( \iff \\) there is an  hyperplane that fully separates the points according to their classes.


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

## Hyperplane &amp;amp; classification

A hyperplane in 2 dimension is defined by a line equation \\( \beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0  \\)

&amp;lt;img src=/assets/07/hyperplane_3d.png width=300&amp;gt;

In **p dimensions** a hyperplane is defined by: $$\beta_0 + \beta_1 X_1 + ... + \beta_p X_p = 0 \qquad  p \gt 0 $$


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Linear decision boundary
A classifier that is based on a separating hyperplane leads to a **linear decision boundary**.




    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# New point
We predict the class of a new point \\( x^{\prime} \\)

by calculating
$$ f(x^{\prime}) = \beta\_0+ \sum\_{j=1}^{n} \beta\_j x\_{j}^{\prime}  $$

All points \\( x_i \\) such that

* \\( f(x^{\prime})   &amp;lt; 0 \\) belong to **class -1**
* \\( f(x^{\prime})  &amp;gt; 0 \\) belong to **class +1**

## sign

Note that since both \\( y\_i \\) and \\( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j}  \\) have the same sign:

$$ y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) &amp;gt; 0 $$

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Maximal Margin Classifier

We want to find the Hyperplane that will maximise the distance to all the points.
Best separation of the classes


Finding the largest margin \\(M\\) that separates the classes is equivalent to solving:

$$ \max\_{\beta\_i} M \quad  \quad y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) &amp;gt; M \quad \forall i \in [1,..,n] $$

with a constraint on the coefficients \\( \sum\_{j=0}^{p}  \beta^2\_j = 1\\)

## M exists

since for all points

$$ y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) &amp;gt; 0 $$

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Infinity of Hyperplanes

If our data is linearly separable then there exists an infinity of hyperplanes that can separate it

&amp;lt;img src=/assets/07/hyperplane-classification.png height=200&amp;gt;


=&amp;gt; We need to introduce a margin to separate the classes

&amp;lt;img src=/assets/07/Maximal_Margin_Classifier.png height=250&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Only the support vectors
Interestingly, the maximal margin hyperplane depends directly on the support vectors, but not on the other observations.

**A movement to any of the other observations would not affect the separating hyperplane**, provided that the observation’s movement does not cause it to cross the boundary set by the margin.

However changing an observation which is on one of the support vectors impacts the margin a lot =&amp;gt; **over fitting**
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Support Vectors

The observations that are on the margin lines are called **support vectors**

They *support* the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyper- plane would move as well.

&amp;lt;img src=/assets/07/support_vectors.png&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* \\( C\\) is a non negative tuning parameter

$$ \max\_{\beta\_i, \epsilon\_i} M \quad  y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) &amp;gt; M - \epsilon\_i  $$
with

* \\( \sum\_{i} \epsilon\_i \leq C  \\)

* \\( \sum\_j  \beta^2\_j = 1 \\)
* \\(   \epsilon\_i &amp;gt; 0 \\)

&amp;lt;img src=/assets/07/simple-svm.gif&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Support Vector Classifier

We allow some points to be either i) wrongly classified or ii) within the margin boundaries?

Add a tuning parameter C that dictates the **severity of the violation of the margin**. The larger C is the more points are within the margin or misclassified

Adding flexibility to the margin makes the classifier less likely to overfit.

We are now able to address datasets that are not **linearly separable**.

&amp;lt;img src=/assets/07/non_linearly_separable.png&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Demo

07_svm_demo.py

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/07/sgd_loss_svm_hinge.png width=600&amp;gt;

* see scikit-learn doc: [1.5.7. SGD - Mathematical formulation](http://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation)

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# loss functions

### Hinge loss function

$$ L(y_i, x_i) = max(0,1−y_i(w^Tx_i+b) $$

### SGD loss function

$$ E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w) $$

### soft margin SVM tries to minimize

$$ E(w,b) = \frac{1}{n}\sum_{i=1}^{n}  max(0,1−y_i(w^Tx_i+b) + \alpha ||w||^2_2 $$


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Kernels

Here are some classic Kernel functions

* Linear Kernel \\( \quad K(x,x_i) = \langle x, x_i \rangle\\)
* Polynomial Kernel (d): \\( \quad K(x,x\_i) =  (1 + \sum\_{j=1}^{p} x\_{j}x\_{i,j} )^d   \\)
* Radial Kernel \\( \quad K(x,x\_i) =   \exp(-\gamma    \sum\_{j=1}^{p} (x\_{j} - x\_{i,j})^2 )   \\)


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Introducing Kernels

The optimization equation for the linear support vector classifier can be rewritten as such

$$ f(x) =  \beta\_0 + \sum\_{i=1}^{n} \alpha\_i \langle x, x\_i \rangle   $$
$$ f(x) =  \beta\_0 + \sum\_{i=1}^{n} \alpha\_i K(x,x\_i)    $$

with \\( K(x,x\_i) = \langle x, x\_i \rangle \\) the vector dot product.

But we could use a different Kernel function

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
II: Imbalanced datasets
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Strategies

* Collect more data
* Accuracy is not always the best metric
    * Cohen's Kappa
    * F1 score
    * AUC
* Oversample or Undersample
* SMOTE
* Penalized Models
* Decision trees often perform well on imbalanced datasets
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Cas classique: Caravan

[Caravan dataset on kaggle](https://www.kaggle.com/uciml/caravan-insurance-challenge)

Can you predict who would be interested in buying a caravan insurance policy and give an explanation why?

* Identify potential purchasers of caravan insurance policies
* 86 variables
* Highly imbalanced
    * No     5474
    * Yes     348

### simplest classifier

* No one buys
* accuracy = 5474 / (5474+348) = 94% !

Setting all predictions to No gives you a 94% prediction rate!
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# SMOTE:
Oversampling method that creates new samples as interpolations of the minority class.

* Paper [Synthetic Minority Over-sampling Technique](http://www.jair.org/papers/paper953.html)
* [SMOTE in python](https://github.com/scikit-learn-contrib/imbalanced-learn)

&amp;lt;img src=/assets/07/SMOTE_R_visualisation_2.png&amp;gt;
&amp;lt;img src=/assets/07/SMOTE_R_visualisation_3.png&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Oversampling - subsampling
The idea is to balance the ratio of minority \\(n\_{min}\\) vs majority \\(n\_{maj}\\) samples.

$$  \frac{ n\_{min} }{n\_{maj}} \ll 1 $$

becomes

$$  \frac{ n\_{min} }{n\_{maj}} \simeq 1 $$

### Subsampling

* Randomly sub sample the majority class.

### Oversampling

* Bootstrap the minority class \\(K\\) times such that \\( K*n\_{min}  \simeq n\_{maj}\\)

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;!-- ------------------------------------------ --&gt;
&lt;!-- ------------------------------------------ --&gt;
&lt;!-- ------------------------------------------ --&gt;
&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/7-support-vector-machines</link>
        <guid isPermaLink="true">http://localhost:4000/7-support-vector-machines</guid>
        
        
      </item>
    
      <item>
        <title>8) Time series</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;Time series&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent

&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
I: Time series, intro
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/08/time-series-analysis.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Today
* Metrics
* Simple, MA, EWMA
* Forecasting, train, test
* Stationnarity, definition, Dickey Fuller test
* Decomposition: Trend, cycle, seasonality
* Differencing
* ARIMA
    * autocorrelation et partial autocorrelation
* residuals

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Domaines d'application

* **IoT**
* **econometrics**
* mathematical finance / trading / markets
* intelligent transport and trajectory forecasting
* weather forecasting and Climate change research
* earthquake prediction, astronomy
* electroencephalography, control engineering, communications
* **signal processing**

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Time Series [TS]

* A time series is a series of data points listed in time order.
* A sequence taken at successive equally spaced points in time.
* A sequence of **discrete-time data**.

=&amp;gt; The **time interval** is key
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/08/monthly_milk_production.png&amp;gt;

&amp;lt;img src=/assets/08/prediction_with_prophet_01.png&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/08/ts-cheese.png&amp;gt;

&amp;lt;img src=/assets/08/TimeSeriesChart_1.jpg&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Ts metrics


Metrics to compare TS techniques

* Mean Absolute Error:

\\( MAE=mean(|e\_i|) \\)

* [Mean Absolute Deviation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mad.html):

\\( MAD = \frac{1}{n} \sum\_{i=1}^{n}  | e\_i |  \\)

* Root mean squared error:

\\( RMSE = \sqrt{  mean(e\_i^2)  } \\)

* Mean absolute percentage error:

\\( MAPE = \frac{1}{n} \sum\_{i=1}^{n} \frac{ | e\_i | }{ y\_i } \\)

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Prévision la plus simple

* Il fera aujourd'hui le meme temps qu'hier

$$ \hat{y}\_{n+1} = y\_{n} $$

## Forecasting error

$$ e\_i=y\_i−\hat{y}\_i $$

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
II: Modélisation
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Smoothing
&amp;lt;img src=/assets/08/moving-avg-2.png&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Moving Average

Simple smoothing, moyenne sur fenetre

$$  \hat{y}\_{t +1 }   = \frac{1}{n} \sum\_{i=0}^{n-1}y\_{t-i}
= \frac{1}{n} ( y\_{t} + y\_{t-1}+ \cdots + y\_{t-(n-1)}  )
$$


## Avec coefficients

Modèle MA(q): Moving Average d'ordre q

$$  \hat{y}\_{t +1 }   =  \sum\_{i=0}^{q-1} \beta\_{i}  y\_{t-i} $$


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

&amp;lt;img src=/assets/08/exponential_moving_averages.gif&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Exponential Weighted Moving Average
Introduce a Decay

The EWMA for a series Y may be calculated recursively:

* \\( S\_{1}=Y\_{1} \\)
* \\(  S\_{t}=\alpha  Y\_{t}+(1-\alpha ) S\_{t-1}  \ \ \ \  t&amp;gt;1 \\)


Where:

* The coefficient α represents the degree of weighting decrease, a constant smoothing factor between 0 and 1.
* \\(Y\_t\\) is the value at a time period t.
* \\(S\_t\\) is the value of the EWMA at any time period t.

A higher \\( \alpha \\) discounts older observations faster.

http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.ewma.html


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# ARMA(p,q) model
The notation ARMA(p, q) refers to the model with p autoregressive terms and q moving-average terms. This model contains the AR(p) and MA(q) models,

$$ X\_{t}= c + \varepsilon\_{t} + \sum\_{i=1}^{p} \varphi\_{i} X\_{t-i} + \sum\_{i=1}^{q} \theta\_{i} \varepsilon\_{t-i} $$

# ARIMA(p,d,q)

Difference entre evenements successifs.

On considère le processus différencié (d=1):

$$ y\_{t}'=y\_{t}-y\_{t-1} $$

ou second order differencing (d=2):

$$ \begin{aligned}
y\_{t}^{\*} &amp;amp; = y\_{t}'-y\_{t-1}' \\\
y\_{t}^{\*} &amp;amp; = (y\_{t}-y\_{t-1})-(y\_{t-1}-y\_{t-2}) \\\
y\_{t}^{\*} &amp;amp; = y\_{t} - 2y\_{t-1} + y\_{t-2}
\end{aligned} $$

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;


# Autoregressive model

The notation AR(p) refers to the autoregressive model of order p. The AR(p) model is written

$$ X\_{t}=c + \sum\_{i=1}^{p} \varphi\_{i}X\_{t-i} + \varepsilon\_{t} $$

## polynome associé

$$ AR(z) =  z^{p} - \sum\_{i=1}^{p} \varphi\_{i}z^{p-i} $$



# Moving-average model

The notation MA(q) refers to the moving average model of order q:

$$ X\_{t}=\mu +\varepsilon\_{t}+\sum\_{i=1}^{q}\theta\_{i}\varepsilon\_{t-i} $$

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* An ARIMA(0,1,0) model (or I(1) model) is given by \\( X\_{t}=X\_{t-1} + \varepsilon\_{t} \\)  which is simply a random walk.

* An ARIMA(0,1,0) with a constant, given by  \\( X\_{t}=X\_{t-1} + c + \varepsilon\_{t} \\) — which is a random walk with drift.

* An ARIMA(0,0,0) model is a white noise model.

* An ARIMA(0,1,1) model without constant is a basic exponential smoothing model

* An ARIMA(0,2,2) model is given by

$$ X\_{t} = 2X\_{t-1} - X\_{t-2} + ( \alpha + \beta -2) \varepsilon\_{t-1} + (1 - \alpha ) \varepsilon\_{t-2} + \varepsilon\_{t} $$

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Stationnarité

* Comment determiner p,d,q

* Quel modelisation appliquer

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
II: Stationarity
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Interpretation

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Autocorrelation ACF

as

$$  \hat{R}(k)  = \frac{1}{ (n-k) \sigma^{2} } \sum_{t=1}^{n-k} ( X\_t - \mu )
( X\_{t+k} - \mu ) $$


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Interpretation

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Partial Autocorrelation PACF

The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags

Given a time series  the partial autocorrelation of lag k, denoted \\( \alpha(k) \\), is the autocorrelation between \\( z\_{t} \\) and  \\( z\_{t+k} \\) with all the linear dependence of \\( z\_{t} \\) on \\( z\_{t+1}, ....,  z\_{t+k -1} \\) removed;

$$ \alpha(1)= \hat{R} (z\_{t+1},z\_{t}) $$


$$ \alpha(k) = \hat{R} ( z\_{t+k}-P\_{t,k}(z\_{t+k} ) , z\_{t} - P\_{t,k}(z\_{t} ) )$$

where \\( P\_{t,k}(x) \\) denotes the projection of \\(x\\) onto the space spanned by \\( x\_{t+1},\dots ,x\_{t+k-1} \\)   &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

&amp;lt;img src=/assets/08/Mean_nonstationary.png height=150&amp;gt;

&amp;lt;img src=/assets/08/Var_nonstationary.png height=150&amp;gt;

&amp;lt;img src=/assets/08/Cov_nonstationary.png height=150&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Stationarity
Key concept for time series

Do the caracteristics of the TS evolve over time ?

* Trend
* Cycles, Seasonality

A time serie is said to be stationnary if

* **Constant mean**: The mean of the series should be constant with respect to time.

* **Constant variance**: Homoscedasticity: The variance of the series constant with respect to time.

* **Fix lagged covariance**: The covariance of the \\(i^{th}\\) term and the \\( (i+m)^{th}\\) term should only depend on m and not i.

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# PACF - stationnarity
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# ACF - stationnarity

https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# KPSS

# Test random walk
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Test for stationarity

## Dickey Fuller test

* Here the null hypothesis is that the TS is non-stationary.

* The test results comprise of a Test Statistic and some Critical Values for difference confidence levels.

https://datamarket.com/data/set/22ox/monthly-milk-production-pounds-per-cow-jan-62-dec-75#!ds=22ox&amp;amp;display=line

## Augmented Dickey Fuller test

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Transform - Stationary

* difference
* log

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Decomposition

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;!-- ------------------------------------------ --&gt;
&lt;!-- ------------------------------------------ --&gt;
&lt;!-- ------------------------------------------ --&gt;
&lt;section&gt;

* Autocorrelation function
https://analysights.wordpress.com/tag/box-pierce-test/

* Test est ce un random noise / walk
&lt;/section&gt;

&lt;!-- ------------------------------------------ --&gt;
&lt;!-- ------------------------------------------ --&gt;
&lt;!-- ------------------------------------------ --&gt;
&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/8-time-series</link>
        <guid isPermaLink="true">http://localhost:4000/8-time-series</guid>
        
        
      </item>
    
      <item>
        <title>Lesson 18 - R &amp; AWS ML</title>
        <description>&lt;section data-background-color=&quot;#000&quot;&gt;
    &lt;h1 class=&quot;white&quot; style=&quot;border-top: thin solid #DDD;border-bottom: thin solid #DDD;&quot;&gt;
        &lt;img src=&quot;assets/ga_logo_black.png&quot; style=&quot;float:left;top:0px;&quot; /&gt;
        General Assembly
    &lt;/h1&gt;
    &lt;p class=&quot;big_title&quot;&gt;18. R and AWS ML&lt;/p&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Previously

* Decomposition, Trending and seasonality
* Stationarity, Dickey-Fuller test, Autocorrelation, partial auto correlation
* AR(p), MA9q), ARIMA(p,d,q)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Today

* R
    * R studio
    * packages
    * essential code
    * time series forecasting

* AWS ML

    * classification problem

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# R Packages

* caret:  The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations.
* CRAN - Package e1071 - for Naive Bayes, SVMs, Latent Class Analysis
* CRAN - Package randomForest - Random Forests
* CRAN - Package gbm - Generalized boosting models

and many more https://www.quora.com/What-are-the-best-machine-learning-packages-in-R
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# R Code

Directory:

* getwd() get working directory
* setwd() set working directory

Packages:

* install.packages('forecast')
* library('forecast')
* installed.packages()

help:

* ?c
* ?forecast

data:

* **x &amp;lt;- 1 instead of x = 1**
* a &amp;lt;- c(1,2,3,4,5)
* vector &amp;lt;- c(apple = 1, banana = 2, &quot;kiwi fruit&quot; = 3, 4)
* df &amp;lt;- read.csv('iris.csv')

Dataframe:

* dim(df)
* head(df)
* colnames(df)
* summary(df)

plot:

* plot(df)
* hist(df$sepal_length)
* boxplot(df$sepal_width)
* plot(df$sepal_length, df$petal_width)
* qqnorm(df$sepal_length)
* qqline(df$sepal_length)

ML on Iris

* iris$target[iris$Species == 'setosa'] &amp;lt;- 1
* iris$target[iris$Species == 'versicolor'] &amp;lt;- 2
* iris$target[iris$Species == 'virginica'] &amp;lt;- 3

* fit &amp;lt;- lm(target ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris )
* coefficients(fit)
* summary(fit)


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Time series analysis in R

dataset: [Stock Prices for Tui Ag from the Frankfurt Stock Exchange.](https://www.quandl.com/data/FSE/TUI1_X-Tui-TUI1_X)


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Holt Winters

The additive Holt-Winters prediction function (for time series with period length p) is

Yhat[t+h] = a[t] + h * b[t] + s[t - p + 1 + (h - 1) mod p],

where a[t], b[t] and s[t] are given by

a[t] = α (Y[t] - s[t-p]) + (1-α) (a[t-1] + b[t-1])

b[t] = β (a[t] - a[t-1]) + (1-β) b[t-1]

s[t] = γ (Y[t] - a[t]) + (1-γ) s[t-p]

The multiplicative Holt-Winters prediction function (for time series with period length p) is

Yhat[t+h] = (a[t] + h * b[t]) * s[t - p + 1 + (h - 1) mod p],

where a[t], b[t] and s[t] are given by

a[t] = α (Y[t] / s[t-p]) + (1-α) (a[t-1] + b[t-1])

b[t] = β (a[t] - a[t-1]) + (1-β) b[t-1]

s[t] = γ (Y[t] / a[t]) + (1-γ) s[t-p]


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Amazon Machine Learning

https://console.aws.amazon.com/

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Amazon Machine Learning

* Datasources
* Models
* Evaluations and results

&lt;/section&gt;
</description>
        <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/18-r-and-aws-ml.html</link>
        <guid isPermaLink="true">http://localhost:4000/18-r-and-aws-ml.html</guid>
        
        
      </item>
    
  </channel>
</rss>
