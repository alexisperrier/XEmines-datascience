<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Emines - UM6P - Data Science</title>
    <description>Cours de data science.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 29 Sep 2018 15:44:24 +0100</pubDate>
    <lastBuildDate>Sat, 29 Sep 2018 15:44:24 +0100</lastBuildDate>
    <generator>Jekyll v3.7.2</generator>
    
      <item>
        <title>1) Intro et Python</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Data science&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;Analyse prédictive et machine learning &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;&amp;nbsp;&lt;/p&gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;&amp;nbsp;&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:center;&quot;&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/01/presentation_alexis_perrier.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* Concepts et Methodes
    * biais, variance et overfitting
    * transformations des données
    * feature engineering et feature selection
    * métriques et techniques de scoring

* datasets
    * iris, titanic, housing
    * caravan, arbres, ...

* Python
    * notebook jupyter, anaconda
    * pandas, numpy
    * statsmodel et surtout scikit-learn
    &lt;/div&gt;
&lt;/div&gt;


&lt;hr class=&quot;vline&quot; /&gt;

&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Programme

* Machine learning avec scikit-learn
    * analyse prédictive
    * classification et régression

* Approches statistiques classiques:

    * régression linéaire,
    * régression logistique

* Modélisation machine learning
    * Random Forests, XGBoost
    * Support vector machines
    * Gradient stochastique
    * Adaboost, perceptron
    * Naive Bayes

&lt;/div&gt;
&lt;/div&gt;

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Déroulement

* Matin: théories, méthodes et démos

* Après-midi: Lab, workshop =&amp;gt; notebooks jupyter

* Quizzes

* Projet final: Kaggle

    * https://www.kaggle.com/c/house-prices-advanced-regression-techniques/

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;data science - machine learning - predictive analytics - intelligence artificielle - deep learning&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/tweet_when_youre_fundraising.png width = 800&amp;gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![data science sexiest job](/assets/01/data_science_sexiest_job.jpeg)
![demand](/assets/01/data-scientist-demand.jpg)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# data science - machine learning - predictive analytics - intelligence artificielle - deep learning

* **Data Analysis, Data Mining** : Exploration, trouver les tendances, les evolutions, les anomalies, etudier les corrélations.

* **Statistiques** : Trouver le modèle qui explique au mieux les données

* **Machine learning** : Le modèle apprend automatiquement à partir des données. Dimension importante d'apprentissage, de training

* **Analyse prédictive**: Construire ou entrainer des modèles qui peuvent *&quot;prédire&quot;* à partir de données passées.
* **Deep Learning** : Analyse prédictive supervisée avec des réseaux de neurones


[What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data?](https://www.quora.com/What-is-the-difference-between-Data-Analytics-Data-Analysis-Data-Mining-Data-Science-Machine-Learning-and-Big-Data-1)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Stats vs machine learning

# A tiny drop of History

Great article [Forbes: A Very Short History Of Data Science](http://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/#bbea13569fd2)

2001 Leo Breiman, Berkeley, publishes “[Statistical Modeling: The Two Cultures](https://projecteuclid.org/euclid.ss/1009213726)”:

“*There are two cultures in the use of statistical modeling to reach conclusions from data.*

**One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.**

*The statistical community has been committed to the almost exclusive use of data models.*

*This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.*”


&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/ds_meme.jpg&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Data Science: skills
&amp;lt;img src=/assets/01/ven_diagrams.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/data_science_close_up.png width = 650&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Champs d'applications

* **Predictions**: market, demand, supply prices, population, weather, earthquakes, ...

* **Patterns**: customer behavior patterns

* **Detection**: Spam, Fraud, Failures, Cyber attacks

* **Extracting meaning** from large sets of data: handwritten health records, exoplanets

* **NLP**: translation, speech to text, speech recognition, sentiment analysis, topic modeling, spell checking

* **Recommender systems**: Netflix, Spotify, Amazon

* **Ranking systems**: search results

* **Autonomous systems** (reinforcement learning / AI): playing games, self driving cars, drones

* **Time series**: algorithmic trading, signal processing, IoT
* **Image / Video**: automatic captionning, face and object recognition, ...

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Data science workflow&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

![](/assets/01/predicsis_data_science_workflow.png)

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## A) Les données

1.  Définir le problème
    * De quelles données disposent-on ?
    * Sont-elles accessibles ?
    * Que veut-on améliorer ?
    * Comment mesurer l'efficacité de la solution, du modèle ?
    * Choix des métriques

2. ETL: Extraction Transform Load

    * Constituer le dataset
    * Explorer et comprendre

## là commence le travail de modélisation

3. Travailler sur les variables
    * Nettoyer et transformer : outliers, missing values, distributions, correlations, ...
    * feature engineering
    * feature selection
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/aws_ml.png width=200&amp;gt;
&amp;lt;img src=/assets/01/gcloud_ml.png width=200&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## B) Machine Learning

4) Outils et plateforme
* Cloud (AWS, Google Cloud, Azure) ou local
* python (scikit-learn) ou R ou ...
* Modèles classiques
* Deep learning (TF, Keras, pytorch, ...)

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/ml_map.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## B) Machine Learning

5) Modélisation

* Choisir la bonne approche, le bon type de modèle
* *Train* le modèle
* Evaluer le modèle, scoring, ....
* Sélectionner les meilleurs parametres du modèle

6) Appliquer sur de nouvelles données

* on quitte un environnement controlé (laptop / labo) pour le monde réel

* le modèle generalise t il bien ?
* les données ont-elles changées ?


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;



## D) Mise en production

* ingénierie logicielle
* API
* streaming

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

## C) Nouvelle itération

7) Présentation des résultats

* cycle itératif court
* communication
* data visualization

8) reprendre le problème au niveau des données
* il en faut plus
* il faut de nouvelles variables
* ....

ou au niveau de la définition du problème

* qu'est ce qu'on veut optimiser
* comment le mesurer
* accessibilité des données
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;!-- ======================================================== --&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/predicsis_data_science_workflow.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## A) Les données

* 1)  Définir le problème

* 2) ETL: Extraction Transform Load

* 3) Travailler sur les variables

## B) Machine Learning

* 4) Outils et plateforme
* 5) Modélisation
* 6) Le test des nouvelles données

## C) Nouvelle itération

* 7) Présentation des résultats
* 8) reprendre le problème


## D) Mise en production

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Data science - machine learning - predictive analytics

[Can I learn Machine Learning completely with Kaggle?](https://www.quora.com/Can-I-learn-Machine-Learning-completely-with-Kaggle)

*While modeling is the sexy part of any machine learning project, it is also one of the parts that you will actually spend the least amount of time on.*

*In a business environment 80–90% of the time will be spent on defining problems worthwhile solving, defining **evaluation metrics**, procuring access to the **raw data**, **understanding** the data, generating **features**, presenting findings, and working with engineers to deploy the model to production via API or other automated approaches.*

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/your_plan_vs_reality.jpeg height=500&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Analyse prédictive

![](/assets/01/predictive_analytics.png)

[Predictive analytics](https://en.wikipedia.org/wiki/Predictive_analytics) : analyze current and historical facts to make predictions about **future or otherwise unknown events**. Predictive analytics provides a predictive score (probability).

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Non Supervisée

Le dataset d'apprentissage n'inclut pas de variable cible. Il n'y a pas de **ground truth**

* logique de clustering, de classification automatique des échantillons  sans connaitre a priori le nombre de classes
* notion de similarité et de distance entre les échantillons
* K-means, K-NN, ...


&amp;lt;img src=/assets/01/ch1_unsupervised_learning.png&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Supervisée

Le dataset d'apprentissage inclut la variable  à prédire  [cible]. On a un certain nombres d'exemples sur lesquels on peut entrainer un modèsle

* logique de scoring, de classification et de prediction
* Random forest, Regression lineaire ou logistique, SVM, ...
* Classification: On connait le nombre de classes

&amp;lt;img src=/assets/01/ch1_supervised_learning.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Classification

La variable cible est discrete, une catégorie, une classe

### Cas binaire
* Achat, resiliation, click
* Survie, maladie, succes examen, admission,
* Positif ou negatif
* Spam, fraude

### Multi class - multinomiale
* Catégories, types (A,B,C),
* Positif, neutre ou negatif
* Espèces de plantes d'animaux, ...
* Pays, planetes

### Ordinale

* Notes, satisfaction, ranking


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Regression

La variable cible est continue

* Age, taille, poids,

* nombre d'appels, de clicks, volume de vente, consommation

* Température, Salaire, ...

* Probabilité d'une action

* Temps, délai, retard

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Classification à regression

* Prédire une probabilité au lieu d'une classe

$$
0 &amp;lt; P( x \in A) &amp;lt; 1
$$


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Regression à classification

* discrétiser la variable

Age =&amp;gt;

* 0 - 12
* 12 - 24
* 25 - 49
* 50 - 65
* plus de 65



    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;Environnement&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;Python, anaconda et jupyter &lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/Python-Logo-PNG-Image.png width=400&amp;gt;
&amp;lt;img src=/assets/01/guidovanrossum.jpg width=400&amp;gt;


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Python

* Beaucoup d’applications: web, data science, scientific, …

* Créé en 1991 par Guido von Rossum! 30 ans déjà!

* 130.000 packages et librairies

* Duck typing, pas de compilation, pas de ; ou de {}

* Indentation =&amp;gt; le code est lisible

* Performances

* Mais il y a des surprises, des incoherences, des idioms, …

* Python 2.7 ou python 3.6


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/python_projections_on_stackoverflow.png height=650&amp;gt;
&lt;/section&gt;

&lt;section&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Python

* list comprehension

```liste_a = [n for n in range(100) if n % 2 ==0 ]```


* pandas dataframe

``` df = pd.read_csv(filename) ```

```df = df.groupby(by = 'age' ).reset_index(inplace = True)```

```df = df.age.apply(lambda a : une_fonction(a) )
```

* notebook jupyter


    &amp;gt; jupyter notebook


* exemple

[Python_Pandas_Demo.ipynb](Python_Pandas_Demo.ipynb)

&lt;/div&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Quel Python avez-vous?

Dans un terminal

```&amp;gt; python --version
```

![](/assets/01/python_version.png)

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/01/anaconda_logo.png&amp;gt;

[https://www.anaconda.com/](https://www.anaconda.com/)


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Anaconda

* Distribution Anaconda et package manager conda

```conda install package_name
```

* Data science en python:

    * Dataframe: pandas, dask

    * Math, science: numpy, scipy, statsmodel,

    * Dataviz: matplotlib, plot.ly, bokeh

    * Deep learning: Tensorflow, Keras, Mxnet, …

    * Text: Gensim, NLTK, Spacy.io

    * scikit-learn: http://scikit-learn.org/

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# [http://jupyter.org/](http://jupyter.org/)
&amp;lt;img src=/assets/01/jupyter_logo_ecosystem.png&amp;gt;
&amp;lt;img src=/assets/01/jupyter_architecture.png&amp;gt;


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Jupyter notebook

* Executer du code dans le navigateur
* Partage et reproductabilité
* Calcul et visualisation

* Multilingue: R, python, ...
* Local ou cloud
    * $ Jupyter notebook
    * AWS Sagemaker, Google datalab, Kaggle kernels
* A base de cellules
    * Documentation: markdown et latex
    * Kernels: Python, R, Julia, Scala, …
    * Shell terminal

* Alt: Beaker, Apache zeppelin

* Mais: Le code est séquentiel + State problems

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Editeurs de texte

&amp;lt;img src=/assets/01/atom_io-card.png height=150px&amp;gt;

&amp;lt;img src=/assets/01/sublime_text_logo.png height=150px&amp;gt;

&amp;lt;img src=/assets/01/spyder_readme_banner.png height=150px&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;Récapitulatif&amp;lt;/p&amp;gt;
&amp;lt;p class=top&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Récapitulatif

* Programme des 2 semaines
* Révisions de python
* Différence entre Data Science, Machine Learning et analyse prédictive
* Approche statistique vs approche machine learning
* Déroulement d'un projet de Data science
* Supervisée vs non-supervisée
* Regression vs Classification
* Anaconda, Python et Jupyter

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Notebook d'exploration et Pandas

* load dataset dans une dataframe pandas
* visualisation des variables
* statistiques des variables numériques et occurences des catégories
* trouver les outliers et les enlever

Arrondissements:

* quels arrondissements ont
    * le plus d'arbres
    * le plus de variétés d'arbres
    * les arbres les plus hauts, les plus larges
* hauteur et circonférence en fonctions des espèces d'arbres
* Comment sont définis les arbres dit remarquables?
    * comment traiter les valeurs manquantes de cette colonne

Domanialité
    * memes questions que pour les arrondissements: variétés, hauteur, ...

En joignant le dataset arrondissement qui donne la superficie des arrondissements

Creer une variable code_postale dans le dataset arbres, qui permette de joindre les 2 fichiers: PARIS 2E ARRDT =&amp;gt; 75102

Joindre les 2 fichiers

Calculer le nombre d'arbres par arrondissement en utilisant groupby et count()
enlever les arrondissement qui ne sont pas dans Paris



    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Lab

2 datasets

* 200.000 arbres de Paris
    * Espèces, genres, famille
    * Adresse, geolocalisation
    * Environnement: rue, jardin, ..
    * hauteur et circonférence

* Arrondissement de Paris
    * Superficie

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/1-intro-python</link>
        <guid isPermaLink="true">http://localhost:4000/1-intro-python</guid>
        
        
      </item>
    
      <item>
        <title>2) Régression linéaire</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
Régression Linéaire
&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;et Python&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent

* Révisions de python
* Différence entre Data Science, Machine Learning et analyse prédictive
* Approche statistique vs approche machine learning
* Déroulement d'un projet de Data science
* Supervisée vs non-supervisée
* Régression vs Classification
* Anaconda, Python et Jupyter

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Lab

Lab pandas et exploration sur le dataset les-arbres.csv

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Régression linéaire

* Régression linéaire
    * OLS, Moindres carrés
    * Modélisation
    * Univariable &amp;amp; multivariables
* Interprétation des résultats
    * Mean Square Error (MSE)
    * P-value, Interval de confiance, \\(R^2\\), \\(R^2_{adj}\\)
    * Confonders et multi-collinearité

* Hypothèses et vérification
    * Linéarité: Définition et tests

* Statsmodel

## Projet Kaggle

## Python

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
https://www.kaggle.com/c/house-prices-advanced-regression-techniques
&amp;lt;img src=/assets/02/kaggle_competition.png height=600&amp;gt;

&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

## Regression: Qualitatif

La variable à prédire est **continue**

* Age, taille, poids,

* nombre d'appels, de clicks, volume de vente, consommation

* Température, Salaire, ...

* Probabilité d'une action

* Temps, délai, retard

&lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:60%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

## Classification: Quantitatif
La variable à prédire est **discrète**

### Cas binaire
* Achat, resiliation, click
* Survie, maladie, succes examen, admission,
* Positif ou negatif
* Spam, fraude

### Multi class - multinomiale
* Catégories, types (A,B,C),
* Positif, neutre ou negatif
* Espèces de plantes d'animaux, ...
* Pays, planetes

### Ordinale

* Notes, satisfaction, ranking

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data=&quot;&quot;&gt;

&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

&amp;lt;img src=/assets/02/height_vs_weight.png style='float:right; width:400px;border:0px'&amp;gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

## Taille en fonction de l'age des enfants

On mesure la taille des enfants dans une ecole et leur age.
La taille croit avec l'age. On peut écrire

$$ \text{Taille} = f(\text{Age})  $$


## Regression univariable


On modélise cette fonction par une relation linéaire de la forme:

$$ \hat{\text{T}}\text{aille} = a * \text{Age} + b $$

où \\(\hat{\text{T}}\text{aille}\\) est la taille estimée.


On cherche à connaitre les paramètres \\((a,b)\\) qui donnent la meilleure approximation de la réalité entre la taille et l'age.

Pour trouver ces paramètres on utilise une méthode dite des **moindres carrés**  ou  **Ordinary Least-Squares (OLS)**.

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data=&quot;&quot;&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
Les résidus  \\( e_i  \\)  représentent une **distance** entre les vrais valeurs  \\( y_i  \\) et leur estimations \\( \hat{y_i}  \\). On veut réduire cette distance.

Pour cela on chercher à minimiser la somme des carrés des résidus (aussi appelé norme  \\( L^2  \\).)

$$  || y - \hat{y} ||^2 =  \sum_{i=0}^n (y_i - (a*x_i + b))^2   $$

&amp;lt;img src=/assets/02/Ordinary_Least_Squares_OLS.jpg&amp;gt;


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Regression linéaire
Nous avons \\( n  \\) échantillons:

* Une variable prédictrice \\( x = [x_1, ... , x_n]  \\)

* Et une variable cible \\( y = [y_1, ... , y_n]  \\)

On veut trouver les *meilleurs*  \\(a\\) et \\(b\\) pour lesquels

$$ \hat{y_i} = a * x_i +b  $$

l'erreur de prédiction \\( e_i  \\)  soit minimale:

$$ e_i = \vert  y_i - \hat{y_i} \vert  = \vert  y_i - (a * x_i +b)\vert  $$
&lt;/div&gt;
&lt;/div&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Petit rappel des normes

### Norme quadratique  \\( L^2  \\)
$$  ||x|| = \sqrt{  x_1^2 + .... + x_n^2  } $$

### Norme \\( L^1  \\) ou norme en valeur absolue
$$  |x| =  |x_1| + .... + |x_n|  $$

### Norme infinie \\( L^\infty  \\)

$$  |x|\_{\infty} = max [ |x_1|, ... , |x_n| ] $$

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

Cela donne 2 équations à 2 inconnues  dont la solution exacte est:

$$ \hat{\beta} =  (x^T.x)^{-1} x^T y   $$

avec

* \\( \hat{\beta} = \\{ a,b \\}^T \\)

*  \\( x = [x_1, ... , x_n]  \\)

*  \\( y = [y_1, ... , y_n]  \\)

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Fonction de cout

On a ce qu'on appelle une **fonction de cout** \\(L(a,b) \\):

$$ L(a,b)  = || y - \hat{y} ||^2 =  \sum_{i=0}^n [y_i - (a*x_i + b)]^2   $$

C'est fonction quadratique donc convexe.

Par conséquent pour trouver son minima, il faut trouver les valeurs de \\( a \\) et \\( b \\) qui annule la dérivée \\( 0 \\) .

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
On veut trouver les n+1 coefficients

$$ \beta = [\beta_0, \beta_1, ...., \beta_n] $$

qui minimisent la fonction de cout:

$$  L(\beta) = || y - X\beta ||  $$

La solution de cette équation est : $$ \hat{\beta} =  (X^T.X)^{-1} X^T y  $$
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;


# Regression multinomial

## plusieurs predicteurs

On a  maintenant \\(m\\) predicteurs et toujours \\(n\\)  échantillons.

Pour chaque échantillon, on a la modélisation suivante:
$$ \hat{y}_i  = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_m x_m $$
ou plus simplement
$$ \hat{y}  = \beta X  $$

avec

* \\( X = \[ (x_{i,j}) \]  \\) est une matrice de taille  \\(n\\) par \\(m\\)

* \\( y = [y_1, ... , y_n]  \\) vecteur de \\( n\\) échantillons



    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Python

A)

    X, y = make_regression(n_samples=N, n_features=M, noise =10)


B)

    beta_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

C)

    yhat = X[:, 0]* beta[0] + X[:,1] * beta[1]


* ou si \\(M &amp;gt; 2\\):


    yhat = [0 for i in range(N)]

    for k in range(M):
        yhat += X[:, k]* beta[k]


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Regression linéaire


A) N samples avec M variables:

$$ y_i = \sum_k \beta_k * X[i,k]  + \sigma^2 $$

$$ y =  \beta * X  + \sigma^2 $$

B) Regression weights:

$$\quad \hat{\beta} = (X^T . X)^{-1} X^T y $$

C) Prédiction

$$ \hat{y}_i = \sum_k \beta_k * X[i,k]  + \sigma^2 $$



    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Notebook - demo

02 Linear Regression Exact.ipynb

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Metriques de scoring
## Erreur absolu (MAE) (L1)

Valeur absolue de la difference entre la prédiction et les vraies valeurs


$$  MAE = \sum\_{i=1}^n \| \hat{y\_i} - y\_i \| $$

        e = np.mean( np.abs(y - yhat) )

## Erreur quadratique (MSE) (L2)

$$  MSE = \sum_{i=1}^n (\hat{y_i} - y_i)^2 $$

        ```e = np.mean( (y - yhat)**2 )
        ```

&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

Sur un vrai dataset: **Mileage per gallon performances of various cars** disponible sur https://www.kaggle.com/uciml/autompg-dataset

A prédire:
* mpg: continuous

Les variables

* cylinders: multi-valued discrete
* displacement: continuous
* horsepower: continuous
* weight: continuous
* acceleration: continuous

On ne prends pas en compte:

* model year: multi-valued discrete
* origin: multi-valued discrete
* car name: string (unique for each instance)

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Régression linéaire avec Statsmodel

On va estimer les coefficients non plus directement mais avec la méthode OLS.



On aura plus d'information sur les coefficients de régression:

* leur importance relative
* leur fiabilité
* leur impact quantitatif

On utilise la librairie

* [Statsmodel](http://www.statsmodels.org/stable/index.html) librairie Python
pour une approche statistique de l'analyse de données.

* Intégrée avec pandas et numpy


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# statsmodel


![](assets/02/statsmodel_functions.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Notebook python
    import pandas as pd
    import statsmodels.formula.api as smf

    df = pd.read_csv('../data/autos_mpg.csv')
    lm = smf.ols(formula='mpg ~ cylinders + displacement + horsepower + weight + acceleration + origin ', data=df).fit()
    lm.summary()

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Résultats
* **Dep. Variable**: La variable à prédire
* **Model**: Le modèle
* **Method**: La méthode utilisée
* **No. Observations**: Le nombre d'observations / échantillons
* **DF Residuals**: Degré de liberté des résidus = nombre d'échantillons - nombre de variables
* **DF Model**: Nombre de prédicteurs

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/02_linreg_autompg_01-left.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Goodness of fit

* **R-squared**: The [coefficient of determination](http://en.wikipedia.org/wiki/Coefficient_of_determination). A statistical measure of how well the regression line approximates the real data points

* **Adj. R-squared**: The above value adjusted based on the number of observations and the degrees-of-freedom of the residuals

* F-statistic: A measure how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals

* Prob (F-statistic): The probability that you would get the above statistic, given the null hypothesis that they are unrelated

* Log-likelihood: The log of the likelihood function.

* AIC: The [Akaike Information Criterion](http://en.wikipedia.org/wiki/Akaike_information_criterion). Adjusts the log-likelihood based on the number of observations and the complexity of the model.

* BIC: The [Bayesian Information Criterion](http://en.wikipedia.org/wiki/Bayesian_information_criterion). Similar to the AIC, but has a higher penalty for models with more parameters.&amp;lt;/td&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/02_linreg_autompg_01-right.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    # Définition

\\(R^2\\) est la proportion des variations de la variable cible qui est prédite grace aux prédicteurs.

On définit  \\(R^2\\) par

$$ R^{2} = 1-{SS\_{\text{res}} \over SS\_{\text{tot}}}   $$

On a

$$ 0 &amp;lt; R^2 &amp;lt; 1$$



    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    # \\(R^2\\)

Soit la moyenne de la variable cible :

$$ \bar{y} = \frac{1}{n} \sum\_{i} y_{i} $$

et la somme des carrés de la variable cible centrée :

$$ SS\_{\text{tot}} = \sum\_{i} (y\_{i} - \bar{y} )^2 $$

La somme des carrés des résidus :
$$ SS\_{\text{res}} = \sum\_{i}e\_{i}^{2} = \sum\_{i} (y\_{i} - \hat{y}\_{i} )^2  $$


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# R2 does not indicate whether:

* the independent variables are a cause of the changes in the dependent variable;
* omitted-variable bias exists;
* the correct regression was used;
* the most appropriate set of independent variables has been chosen;
* there is collinearity present in the data on the explanatory variables;
* the model might be improved by using transformed versions of the existing set of independent variables;
* there are enough data points to make a solid conclusion.

et surtout
* plus on ajoute de variable plus \\(R^2\\) augmente meme quand les variables ne sont pas vraiment significative.

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
En accroissant le nombre de predicteurs, on accroit souvent R^2.

Mais \\(R^{2}_{adj}\\) compense la complexité du modele
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# \\(R^2_{adj}\\)

On ajuste pour prendre en compte la complexité du modele:

$$ R^{2}_{adj} = {1-(1-R^{2}){n-1 \over n-p-1}} $$

avec

* \\(p\\) le nombre de prédicteurs
* \\(n\\) le nombre d'échantillons

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;


# Coefficients et p-value
La deuxième partie des résultats porte sur les coefficients et leur fiabilité.


* **coef**: La valeur estimée des coefficients
* **P &amp;gt; |t|**: la probabilité que l'on observe cette estimation alors qu'en fait le coefficient est nulle (=0) .
* **[95.0% Conf. Interval]**: l'interval de confiance de l'estimation du coefficient.
* **std err**: l'erreur d'estimation
* **t-statistic**: une mesure de l'importance (significant) statistique de chaque coefficient.

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/02_linreg_autompg_02.png&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;width : 40%;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

&amp;lt;img src=/assets/02/p_value.png height=250px &amp;gt;


The p-value represents the probability that the coefficient is actually zero

* Si \\( P_{value} &amp;gt; 0.05 \\) alors il y a plus de 5% de chance que l'hypothèse NULL soit vraie:=&amp;gt; **on ne peut pas la rejeter**.

* si \\( P_{value} &amp;lt; 0.05 \\) a lors il y a moins de 5% de chance pour que l'hypothèse NULL soit vraie: =&amp;gt; **on  peut la rejeter**
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# P-value

On a 2 hypothèses:

1. [NULL] ce que l'on observe est du au hasard
2. [ALT] ce que l'on observe n'est pas du au hasard (il y a une relation)

La p-value est la probabilité que ce que l'on observe est du au hasard.

Si la p-value est faible, on rejete l'hypothèse NULL.

Ce qui ne veut pas dire que la valeur du coefficient est la  bonne. (ca serait trop simple) mais simplement que il y a bien une relation entre le predicteur et la variable cible.

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/xkcd_p_value.png style='width:250px'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/02/xkcd_p_value_02.png style='width:300px'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Multinomiale

Que se passe t il quand on filtre certains predicteurs ?

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Conditions sur les données
Pour qu'une régression linéaire soit possible et fiable, il faut que les données vérifient les conditions suivantes:

* **Linearite**: la relation entre les predicteurs et la cible est lineaire
    * On peut tester avec des scatter plots

* **Normality**: Les variables ont une distribution normale
    * test: [QQ plot](https://en.wikipedia.org/wiki/Q–Q_plot#Interpretation)
    * ou Kolmogorov-Smirnov test
    * correction: log ou box-cox

* **Independence**: no or little multicollinearity between variables
    * test: Correlation matrix

* **Homoscedasticity**: for a given variable the low and high range have the same statistical properties with respect to the residuals
    * test: Chunk data and Check Variance

* All **Confounders** accounted for

http://www.statisticssolutions.com/assumptions-of-linear-regression/

https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/

https://www.kaggle.com/questions-and-answers/57571
What does Linear Relationship means? :- You can check linearity with scatter plots, if there is little to no linearity in the scatter plot between your dependent and Independent variables then the assumption doesn't hold.
Linearity regression assumption requires all variables to be normal, how to check? :- You can check this assumption with a Q-Q plot, if your data deviates substantially from the line on the Q-Q plot, then this assumption doesn't hold.
check for little to no multicollinearity, why is multicollinearity a problem? :- Multicollinearity generally occurs when there are high correlations between two or more predictor variables. A principal danger of such data redundancy is that of overfitting in regression analysis models. The best regression models are those in which the predictor variables each correlate highly with the dependent (outcome) variable but correlate at most only minimally with each other. Such a model is often called &quot;low noise&quot; and will be statistically robust (that is, it will predict reliably across numerous samples of variable sets drawn from the same statistical population).

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
Correlation
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Rappel pearson coefficient

Etudier la corrélation entre deux ou plusieurs variables aléatoires ou statistiques numériques, c’est étudier l'intensité de la liaison qui peut exister entre ces variables.

Il y a différentes façon de calculer la corrélation de 2 variables.

La plus commune est [Pearson Correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient)

Qui se calcule suivant :

$$r=\frac{\sum\_{i=1}^{n} (x\_{i}-{\bar{x}}) (y\_{i}-{\bar{y}})} { {\sqrt {\sum\_{i=1}^{n}(x\_{i}-{\bar {x}})^{2}}}{\sqrt {\sum\_{i=1}^{n}(y\_{i}-{\bar {y}})^{2}}}}$$

où :

* \\(n\\) nombre d'échantillons

* \\(x\_{i},y\_{i}\\) les échantillons

* \\( \bar{x} = \frac{1}{n} \sum\_{i=1}^{n} x\_{i} \\) la moyenne; de meme pour  \\({\bar {y}}\\)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation
![Correlation](/assets/02/pearson-correlation-coefficient-illustration.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation

On va regarder l'influence de la correlation entre les predicteurs

    df.corr()

Les prédicteurs ```horsepower``` et ```weight``` sont très corrélés, ```displacement``` et ```cylinders``` aussi.

&amp;lt;img src=/assets/02/autompg-correlation.png  height=400&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Correlation \\(\neq\\) Causalité
http://www.tylervigen.com/spurious-correlations

&amp;lt;img src=/assets/02/spurious_correlations.png&amp;gt;


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Regression and Causation:


For regression coefficients to have a causal interpretation we need both that
* the linear regression assumptions hold: linearity, normality,
independence, homoskedasticity
* and that all confounders of, e.g., the relationship between treatment A and Y be in the model.

Not the same thing

Calculating Correlation: easy

Demonstrating and Quantifying Causation: Causal Inference: Not so easy


=&amp;gt; However most common strategy is to find not causality but correlation through linear regression which can be interpreted as causality under strong assumptions on the covariates.

**Works under VERY strong assumptions**

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Confonders

facteurs potentiels de confusion

https://www.r-bloggers.com/how-to-create-confounders-with-regression-a-lesson-from-causal-inference/

http://www.statisticshowto.com/experimental-design/confounding-variable/

![Confounders](/assets/02/confounder.png)

* Relationship between **ice-cream consumption** and number of **drowning deaths** for a given period

**Confounding: ?**
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

![xkcd](/assets/02/xkcd_correlation.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;Récapitulatif&amp;lt;/p&amp;gt;
&amp;lt;p class=top&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Récapitulatif

* Regression lineaire, simple et explicite
* Attention à ce que les predicteurs soient decorrélés
* R^2 ajusté au lieu de R^2

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Lab de cette apres midi
Regression lineaire sur le  dataset *advertising*

![advertising](/assets/02/advertisingscatterplots.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Questions

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Liens et resources

* [Régression linéaire en python](http://www.xavierdupre.fr/app/mlstatpy/helpsphinx/notebooks/regression_lineaire.html) sur le site de Xavier Dupré

* [OLS sur wikipedia](https://en.wikipedia.org/wiki/Ordinary_least_squares): tres complet

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/2-regression-lineaire</link>
        <guid isPermaLink="true">http://localhost:4000/2-regression-lineaire</guid>
        
        
      </item>
    
      <item>
        <title>3) Régression Logistique</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
Régression Logistique
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/03/questions.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* Régression linéaire - OLS
* Interpretation
    * p-value
    * R^2

* Correlation
* Statsmodel
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    ## Lab: titanic
    &amp;lt;img src=/assets/03/titanic_photo.jpg style='width:300px; border:0'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Programme

* Regression logistique
* odds ratio, log odds ratio
* Maximum de vraisemblance
* encoding categorical values
* Metriques de classification
    * confusion matrix
    * AUC and ROC
* Outliers, detection and impact
* Skewness: Box cox and Kurtosis

&lt;/div&gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Classification ou Régression

Voici une variable à prédire, une condition médicale:

* 0: Attaque cardiaque
* 1: Crise d'épilepsie
* 2: Overdose

Pourquoi ne pas utiliser une regression linéaire pour prédire cette variable ?

L'encodage de la variable (ordre et continuité) implique que

* il y a un ordre entre les catégories: Attaque &amp;lt; Crise &amp;lt; Overdose
* Toutes ces catégories sont équidistantes

# Dans le cas binaire

* 0: Attaque cardiaque
* 1: Crise d'épilepsie

On pourrait utiliser une regression lineaire comme substitut de probabilité mais on obtiendrait peut etre des valeurs en dehors de [0,1]

Donc utiliser des modèles de classification est plus approprié!

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Regression ou Classification?
Review the following situations and decide if each one is a regression problem, classification problem, or neither:

* Using the total number of explosions in a movie, predict if the movie is by JJ Abrams or Michael Bay.
* Determine how many tickets will be sold to a concert given who is performing, where, and the date and time.
* Given the temperature over the last year by day, predict tomorrow's temperature outside.
* Using data from four cell phone microphones, reduce the noisy sounds so the voice is crystal clear to the receiving phone.
* With customer data, determine if a user will return or not in the next 7 days to an e-commerce website.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Régression Logistique


Aussi appelée **logit regression**, **maximum-entropy classification (MaxEnt)** ou log-linear classifier.

Au lieu de prédire la catégorie de la variable cible, on va prédire la probabilité que cette variable appartienne à la catégorie en question :


$$ P(Y = 1 \bigg{/} X) $$

que l'on note \\( p(X) \\)

et comme pour la regression linéaire on vuet avoir un modlèle linéaire simple pour estimer cette probabilité.

$$ P(Y=1 / X) =  p(X) = \beta_0 + \beta_1 X $$

mais pour que \\(p(X)\\) soit une probabilité il faut que ses valeurs soient comprises dans \\( [0, 1] \\) ce qui n
est pas forcement le cas avec la formule ci-dessus.

&lt;/section&gt;

&lt;section&gt;

&lt;div style=&quot;float:right; width:45%; &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    # Fonction Sigmoide
&amp;lt;img src=/assets/03/sigmoid.svg style='width:300px; border:0'&amp;gt;

Cette fonction réduit \\( \mathbb{R} \\)  à l'intervale \\( [0,1] \\)
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Régression logistique

Le modèle linéaire \\(p(X) = \beta\_0 + \beta\_1 X\\)  est pris comme attribut de la fonction sigmoide.

$$ f(z) = \frac{e^{z} }{1 + e^{z}} =  \frac{1 }{1 + e^{-z}} $$

ce qui donne

$$ p(X) = \frac{e^{(\beta\_0 + \beta\_1 X)} }{1 + e^{(\beta\_0 + \beta\_1 X)}}  $$


&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# logistique regression en python
avec statsmodel

### appliqué au *default* dataset

4 colonnes

* student: étudiant?
* balance: compte en banque
* income: revenues

prédiction : va défaulter sur son crédit ou non

En utilisant :

1. default vs balance
2. default vs balance, income et student

* calculer la probabilité de default pour

    * Un etudiant avec un solde debiteur de 1500 et un revenude 40000
    * un non etudiant avec le meme solde et meme revenue

Pourquoi est ce que le coefficient relatif a la variable student est positive quand student est la seule variable alors qu'elle est negative dans le cas multinomial ?


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Catégorie et évènement

Question de vocabulaire:

* un évènement = la variable appartient à la catégorie

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Odds ratio

Aussi appelé  rapport des chances, **rapport des cotes** ou **risque relatif rapproché**

Comment quantifier l'impact d'une variable predicteur sur la probabilité de la catégorie ?

On a:

$$ p(X) = \frac{e^{\beta\_0 + \beta\_1 X} }{1 + e^{\beta\_0 + \beta\_1 X}}  $$

Le **odds ratio**: est le rapport entre la probabilité de l'évènement sur la probabilité du non évènement.

$$
\frac{p(X)}{ 1 -p(X)} = e^{\beta\_0 + \beta\_1*X}
$$

* Odds ratio  \\( \in [0, +\infty] \\)
* Odds ratio proche de  0: probabilité faible que l'évènement survienne
* Odds ratio s'approchant de  \\( +\infty \\) : probabilité forte que l'évènement survienne

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Log-Odds ratio
Si on prends le log du *odds-ratio* on a le **log odds ratio**

$$
log(\frac{p(X)}{ 1 -p(X)}) = \beta\_0 + \beta\_1 X
$$

Cela mesure l'influence d'une variable sur la cible.

Si \\( \beta\_1 \\)  augmente, alors  \\( p(X)\\) augmente aussi.

C'est moins direct que dans le cas de la régression linéaire.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Log-Odds ratio: application
Sur le data set default:

* On accroit / décroit le compte en banque de 10k
* On accroit / décroit le revenu de 10k

## Exemple in the default dataset

\\( p(X) = 0.2 \iff  \frac{0.2}{1 -0.2} = 0.25 \\)

* 1/5 people with ods 1/4 will default

\\( p(X) = 0.9 \iff  \frac{0.9}{1 -0.9} = 9 \\)

* 9 out of 10 people (90%)  with odds 9 will default


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Maximum de vraisemblance
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Métriques de classification
&amp;lt;img src=/assets/03/classification_metrics.png style='width:450px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Métriques de régression
&amp;lt;img src=/assets/03/regression_metrics.png style='width:450px;'&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Faux Positifs et Faux Négatifs
&amp;lt;img src=/assets/03/pregnant.jpg style='width:400px;border:0'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Métrique: Accuracy ou Précision

Correctement identifiés

* TP = True Positive - Vrai positif
* TN = True Negatives  - Vrai négatifs

Incorrectement identifiés

* FP = False Positive
* FN = False Negatives

## Accuracy

On définit la précision par

$$ Accuracy = \frac{ TP + TN  }{TP + FP + TN + FN}   $$

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Confusion matrix

![confusion matrix](/assets/03/confusion_matrix.png)
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# classification
![cats and dogs](/assets/03/cats_dogs.png)


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Ca se complique assez rapidement

&amp;lt;img src=/assets/03/confusion_matrix_wikipedia.png style='border:0'&amp;gt;

[https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Confusion matrix
Avec scikit:

        from sklearn.metrics import confusion_matrix
        y_true = [0,0,0,0,0,1,1,1,1,1]
        y_pred = [0,0,0,1,1,0,1,1,1,1]
        confusion_matrix(y_true, y_pred)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
Machine learning
&amp;lt;/p&amp;gt;
&amp;lt;p class=mitop&amp;gt;Au revoir les statistiques :)&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:30%&quot;&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/03/stats-vs_ml.jpeg&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left;width:50%;&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;


# Des stats au machine learning

* Statsmodel est dans une approche **statistique classique** qui favorise l'interprétabilité et l'analyse des prédicteurs

* Scikit-learn est dans une approche **machine learning** plus orientée vers la  robustesse et la prédiction

Au niveau de l'implémentation de la regression logistique dans les 2 librairies, la difference est que scikit ajoute une **contrainte** sur le modele au niveau de la fonction de cout.

Cette contrainte est appelé **régularisation** et sert à accroitre la capacité du modele a extrapoler sur des donnees nouvelles. On verra cela en detail dans 2 jours.

On est donc dans une transition de la modélisation statistique vers la modélisation machine learning.
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Regression avec scikit-learn

On va avoir des meta parametres.

Par exemple:

* accès à differents algorithmes  pour trouver les coefficients + un certain controle sur leur fonctionnement
* différentes façon de traiter le multi-class: ovr, multinomial
* différents mode de régularisation

et en output

* un modele que l'on peut appliquer a de nouvelles donnees
* les intervals de confiance
* la ou les categories prédites
* les probabilités de prédiction (appartenance a la classe)



On n'aura plus:
- les p-value
- les tests statistiques
- le R^2 (pas directement en tout cas)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Scikit-learn LogisticRegression

http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

![scikit LogisticRegression](/assets/03/scikit-logistic-regression.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![](/assets/03/sklearn_01.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![](/assets/03/sklearn_02.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![](/assets/03/iris_screen_shot_01.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
![](/assets/03/iris_screen_shot_02.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Demo Scikit-learn LogisticRegression

* Iris dataset
* Score
* ROC AUC
* Trouver le meilleur threshold (Acc, max P, min Neg, TPR, ...)
* Use predict_proba and a different threshold =&amp;gt; you should find a different confusion matrix


    import pandas as pd
    from sklearn import datasets, metrics
    from sklearn.linear_model import LogisticRegression


    iris = datasets.load_iris()
    clf = LogisticRegression()
    clf.fit(iris.data, iris.target)

    metrics.accuracy_score(y_test, y_hat )


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# AUX and ROC Curve

* TPR = \\( \frac{  TP }{ P}  = \frac{  TP }{ TP + FN} \\) aka **Sensitivity** or **Recall**
* FPR = \\( \frac{  FP }{ N}  = \frac{  FP }{ FP + TN} \\)  aka **Fall-out**

Le TPR / recall et le FPR varient en fonction du seuil. on obtient donc

### Receiver operating characteristic

ROC = TPR vs FPR pour different seuils

        sklearn.metrics.roc_curve returns TPR, FPR

plot to get the ROC Curve

### AUX and ROC Curve
The AUX is Area under the Curve

        sklearn.metrics.roc_auc_score

So what's your best model LR according to AUC?

voir aussi F1-score

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Exemples

* Marque de voiture: Audi, Renault, Ford, Fiat

Si on assigne un numero arbitraire à chaque marque de voiture on crée une hiérarchie:

Audi =&amp;gt;1 , Renault =&amp;gt; 2, Ford =&amp;gt; 3, Fiat =&amp;gt; 4

De meme:

* chien, chat, souris, poulet =&amp;gt; {1,2,3,4}

pourquoi le poulet serait *4* fois le chien ? Ca ne fait pas sens.


Parfois on peut quand meme assigner un chiffre à chaque categorie, catégories ordonnées

* enfant, jeune, adulte, vieux =&amp;gt; {1,2,3,4}
* negatif, neutre, positif =&amp;gt; {-1, 0, 1}


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# One hot encoding et label encoding

Comment traduire les variables quantitative en variables numeriques

Binaires
* Oui / Non ; 1 /0
* Homme / Femme
* Spam / legit
* Action: Achete, enregistre,
* Identification


Multinomiales
* liste de villes, pays, destinations,
* tranche d'age
* niveau d'etude
* marques de voiture

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# One hot encoding

[One Hot Encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), ou pandas.get_dummies

Si on a **N** classes, on crée **N-1** variables binaires:

par exemple la variable ```animal_type: chien, chat, souris, poulet``` sera transformée en 3 variables binaires

* est_ce_chien : 1/0
* est_ce_chat : 1/0
* est_ce_souris : 1/0

La variable *est_ce_poulet* étant redondante et automatiquement déduite des 3 autres.

# LabelEncoder
[LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) associe un chiffre a chaque classe, on garde l'ordonnancement

* enfant, jeune, adulte, vieux =&amp;gt; {1,2,3,4}

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Recap
* regression logistique
* approche stats vs approche ML
* matrice de confusion
* ROC-AUC
* One hot encoding
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Resources
* Logistic Regression: Why sigmoid function?
https://github.com/rasbt/python-machine-learning-book/blob/master/faq/logistic-why-sigmoid.md

* scikit-learn documentation: Logistic regression,
http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression

* No, Machine Learning is not just glorified Statistics
https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3

* on stackexchange When to use One Hot Encoding vs LabelEncoder vs DictVectorizor? https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor


&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/3-regression-logistique</link>
        <guid isPermaLink="true">http://localhost:4000/3-regression-logistique</guid>
        
        
      </item>
    
      <item>
        <title>4) Biais, variance et gradient stochastique</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Biais, variance et gradient stochastique&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt; avec scikit-learn&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* Régression logistique
* Approche statistique ou approche Machine Learning
* Métriques de classification:
    * TP, TN, FP, FN
    * Matrice de confusion
    * ROC-AUC
* One hot encoding
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    ## Lab:
    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:60%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Aujourd'hui

* Scikit-learn
* Gradient Stochastique; Stochastic Gradient Descent
* Décomposition Biais - Variance

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;

    * [scikit-learn.org](http://scikit-learn.org/stable/)
    * [Eco-système](http://scikit-learn.org/stable/related_projects.html)

    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:60%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Scikit-learn

Libraries open source pour le machine learning créée 2010 dans le cadre du summer of code de Google
* Librairie basée sur numpy, scipy
* Contributeurs principaux :  Olivier Grisel, Andreas Muller, Gael Varoquaux, Jake Vanderplas =&amp;gt; [lien github](github.com)
* Un projet soutenu par
    * [INRIA](http://www.inria.fr),
    * [Telecom ParisTech ](http://www.telecom-paristech.fr/),
    * NYU
* Largement utilisé dans la communauté ML
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Algorithmes et modèles:
Grand choix d'algorithmes et de modèles

## Supervisé
* Classification:
    * SVM, nearest neighbors, random forest, XGBoost, AdaBoost, ...

* Regression:
    * SVM, SGD, ridge et Lasso, regression lineaire, naive bayes, ...

## Non supervisé
* Clustering: Grouper des échantillons *similaires* ou *proches*
    * k-Means, spectral clustering, mean-shift, ...

* Reduction de dimension: Réduire le nombre de variables
    * Applications: Visualization, Performance
    * PCA, feature selection, non-negative matrix factorization.
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:70%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Scikit learn

Mais aussi :
* Sélectionner les modèles: comparer, valider et choisir les paramètres et modèles
    * But: trouver les paramètres qui offrent les meilleurs performances
    * Modules: grid search, cross validation, metrics.

* Pre-processing: Transformation des variables.
    * But: Transformer les variables brutes pour améliorer leur pertinence et les numériser.
    * Modules: preprocessing, feature extraction.

* Documentation très complete avec de nombreux exemples
* Capable de traiter différents types de données: images, textes, données numeriques

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/logo-scikit.png style='width:300px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# scikit-learn API
Simple et cohérente

1. Instancier un modèle, par exemple une regression linéaire:
    * ```from sklearn.Linear import LinearRegression```
    * ```mdl = LinearRegression( meta-params, loss function, ...)```
2. Entrainer le modèle
    * ```mdl.fit(X, y)```
3. Obtenir des prédictions sur de nouvelles données
    * ```y_hat = mdl.predict(Nouveaux échantillons}```.
    * ```y_hat = mdl.predict_proba(Nouveaux échantillons)```
4. Obtenir un score
    * ```mdl.score(X,y)```

&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# paramétrer les modèles

* les meta-paramètres du modèle: \\(\alpha, \epsilon, \beta, \gamma, \cdots \\)
* la regularisation: *penalty, l1, l2*
* la fonction de cout: *loss*
* la gestion des itérations: *max_iter, n_iter*
* data pre-processing: *normalize, shuffle, valeurs manquantes*

[![Lin reg params](/assets/04/scikit_linear_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)
[![SGD reg params](/assets/04/scikit_sgd_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: Biais - Variance
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=assets/04/bias-variance-targets.png style='width:500px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:50%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Décomposition Biais - Variance
L'erreur de prédiction peut etre décomposée en 2 termes

$$
\text{Erreur totale} = \text{Erreur du biais} + \text{Erreur de la variance}
$$

**Biais**: la différence entre les predictions du modele et la valeur cible. Le biais mesure la performance du modèle, la distance entre les predictions et les valeurs cibles.

* **Underfitting**: Un biais important indique que le modele n'arrive pas  à comprendre les données qui lui sont fournies

**Variance**: Il s'agit là de la variabilité des prédictions entre différentes *réalisations* du modèle pour un échantillon donné.

La variance mesure la sensibilité du modèle aux données d'apprentissages

* **Overfitting**: Une forte erreur de variance indique que le modèle ne pourra pas extrapoler ses prédictions sur des nouvelles données.
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width : 45%;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* **Biais**: L'espérance de  l'erreur de prédiction

* **Variance**: Variance des prédictions.

    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:45%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Décomposition Biais - Variance
L'erreur quadratique est définie par:

$$ \operatorname{MSE}( \hat{y} ) = \frac {1}{n} \sum_{i=1}^{n}( \hat{y_i}-y_i )^2 = \mathbb{E} \big[ (\hat{y} - y)^2   \big] $$


Et on peut réécrire cette équation de la façon suivante:

$$ \operatorname{MSE}( \hat{y} )   =\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] + \left(\mathbb{E}(\hat{y})-y\right)^2 $$

Soit
$$ \operatorname{MSE}( \hat{y} )= \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2 $$

avec

$$ \operatorname{Var}(\hat{y}) =  \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] $$
$$ \operatorname{Bias}(\hat{y},y)  = \mathbb{E}(\hat{y})-y  $$

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Le calcul
&lt;/div&gt;
$$
\begin{align} \mathbb{E}((\hat{y}-y)^2)&amp;amp;=
 \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})+\mathbb{E}(\hat{y})-y\right)^2\right]
\\ &amp;amp; =
\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2 +2\left((\hat{y}-\mathbb{E}(\hat{y}))(\mathbb{E}(\hat{y})-y)\right)+\left( \mathbb{E}(\hat{y})-y \right)^2\right]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\mathbb{E}\Big[(\hat{y}-\mathbb{E}(\hat{y}))(\overbrace{\mathbb{E}(\hat{y})-y}^{\begin{smallmatrix} \text{This is} \\  \text{a constant,} \\ \text{so it can be} \\  \text{pulled out.} \end{smallmatrix}}) \,\Big] + \mathbb{E}\Big[\,\overbrace{\left(\mathbb{E}(\hat{y})-y\right)^2}^{\begin{smallmatrix} \text{This is a} \\  \text{constant, so its} \\  \text{expected value} \\  \text{is itself.} \end{smallmatrix}}\,\Big]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\underbrace{\mathbb{E}(\hat{y}-\mathbb{E}(\hat{y}))}_{=\mathbb{E}(\hat{y})-\mathbb{E}(\hat{y})=0}(\mathbb{E}(\hat{y})-y)+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2
\end{align}

$$
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
![underfit - overfit](/assets/04/underfit_overfit.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
Pour réduire l'erreur quadratique, il faut minimiser à la fois le **biais** et la **variance**.

* Biais important &amp;lt;-&amp;gt; sous estimation - Underfitting
    * le modele n'est pas bon
    * On obtient de mauvais score

* Variance importante &amp;lt;-&amp;gt; Overfitting
    * Le modele est trop **sensible** au training dataset
    * pouvoir d'extrapolation faible, mauvaises performances sur des nouvelles données.

Mais comment détecter l'overfit ?

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Underftiing


* Ajouter des predicteurs
* Rendre le modele plus complexe
* Attenuer la regularisation.
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Overfitting

* Reduire le nombre de predicteurs
* Utiliser plus de données d'apprentissage
* Accroitre la reguilarisation
* Moyenner plusieurs modèles

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# split: train vs test dataset

* On entraine le modele sur un **dataset de training**
* Mais le but est d'avoir un modele capable de traiter des **données nouvelles**, données qu'il n'a pas vu auparavant
* et on veut surtout éviter **l'Overfit**: *Le modele est trop **sensible** au training dataset*

Donc on va mettre de coté  une partie des données comme données nouvelles: dataset de test.

Typiquement : une répartition  80/20 ou 70/30

&amp;lt;img src=/assets/04/split_train_test.png style='width: 500px;  margin:auto; ' &amp;gt;

Et c'est en évaluant le modele sur les données de test que l'on va pouvoir détecter l'overfit

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# train - test split

&amp;gt; Demo sur iris dataset

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    [scikit example 1](http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve)

    [scikit example 2](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py)
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:70%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;




# Comment détecter l'overfit ?
### Courbes d'apprentissages - Learning curves



* On met de coté un set d'apprentissage (20% des données)
* on entraine le modele sur un nombre croissant d'echantillons (10%, 20%, ...)
* Pour chaque réalisation on calcule
    * l'erreur sur le set d'apprentissage
    * l'erreur sur le set de test

* En accroissant le set d'apprentissage, le modele a de plus en plus d'info, le modele apprends le set d'apprentissage. On espere que ca va lui permettre de traiter aussi les données sur le set de test.

Ce que l'on observe:
* avec un set d'apprentissage petit, les 2 erreurs sont grandes
* avec plus de données, l'erreur d'apprentissage décroit
    * si l'erreur sur le set de test ne décroit pas: **overfit**!

Si l'erreur sur le set de test ne décroit pas, alors cela veut dire que le modele n'est pas capable d'extrapoler sur des nouvelles données

Note: si l'erreur ne décroit pas sur le set de training en premier lieu, alors cela veut dire que le modele est mauvais, que l'erreur de biais est forte.

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Illustration

### learning curve

* underfit
* overfit


&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Detecter l'overfit -

Demo sur ames housing avec SGD

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Validation croisée
Si on a peu de données, le split train - test *gaspille* des données pour le test. Données qui pourraient etre utile pour l'apprentissage du modele.

=&amp;gt; on va alterner le découpage train - test, 80% - 20%,

C'est la validation croisée et plus particulièrement **K-FOLD cross validation**

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# K-fold cross-validation

&amp;lt;img src=/assets/04/k-fold-cross-validation.png style='width: 600px;  margin:auto; float:right;' &amp;gt;

1. Mélanger le dataset
2. Puis découper le dataset en K (5) parties
3. Faire K (5) experiences:
    * apprentissage sur 1,2,3,4 et evaluation sur 5
    * apprentissage sur 1,2,3,**5** et evaluation sur **4**
    * ...
    * apprentissage sur 2,3,4,5 et evaluation sur **1**

La moyenne des scores obtenus ainsi est plus robuste qu'un score obtenu sur un unique découpage.

[K-fold cross validation - scikit-learn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Autres méthodes de validation croisée

* **Stratified K-Fold**: (classification) Chaque subset conserve la distribution des classes. Utile lorsque la repartition des classes est déséquilibrée.
* **Leave one out**:  Chaque échantillon est utilisé à son tour comme echantillon de test. Tous les autres sont laissé dans le set d'apprentissage.
* **Shuffle cross validation**: Decoupage aléatoire avec remise en place. rien n'oblige à fixer le découpage au début.


### scikit-learn
* [cross_val_score](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_digits.html#example-exercises-plot-cv-digits-py)
* [cross validation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)


&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# K-fold cross validation
Exercise

On the diabetes dataset, find the optimal regularization parameter alpha.

Bonus: How much can you trust the selection of alpha?

http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold

http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: Gradient Stochastique
&amp;lt;/p&amp;gt;
&amp;lt;p class=mitop&amp;gt;
Stochastic Gradient Descent
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Illustration
&amp;lt;img src=/assets/04/robins_monroe_1951.png style='width: 500px; margin: auto; float:right; '&amp;gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Methode du gradient

**1951!** Robbins - Monroe: A Stochastic approximation Method

Soit une function \\(  f \\) dont on souhaite trouver le minimum.

Sous certaines conditions sur  \\(  \alpha \\) et \\(\hat{\nabla} f\\) (gradient de f), alors

$$  {\bf w}_{t+1} = {\bf w}_t - \alpha_t \hat{\nabla} f({\bf x}_t) $$

si \\(  \alpha \\) assez petit et si f est différentiable

Alors \\( {\bf w}_t  \\) converge vers le / un minima de \\(f\\)

*decreases fastest in the direction of the negative gradient of f*

$$ f(\mathbf {w}\_{0})\geq f(\mathbf {w}\_{1}) \geq f(\mathbf {w}\_{2})\geq \cdots , $$

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En python

On veut trouver le minimum de la fonction  \\( f(x) =   \\)

    cur_x       = 6 # The algorithm starts at x=6
    gamma       = 0.01 # step size multiplier
    max_iters   = 10000 # maximum number of iterations
    iters       = 0 #iteration counter
    precision   = 0.00001
    previous_step_size = 1

    # dérivée de la fonction à minimiser

    fct = lambda x: 4 * x**3 - 9 * x**2
    x = []
    while (previous_step_size &amp;gt; precision) &amp;amp; (iters &amp;lt; max_iters):
        x.append(cur_x)
        prev_x = cur_x
        cur_x -= gamma * fct(prev_x)
        print(cur_x, previous_step_size)
        previous_step_size = abs(cur_x - prev_x)
        iters+=1

    print(&quot;Le minimum est {:.4f}&quot;, cur_x)

    print(&quot;En x = {:.4f}, la valeur de la fonction est {:.4f}  &quot;.format(cur_x, fct(cur_x)) )


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Stochastic Gradient

The general idea is to aproximate an unknown function through iterations of **unbiased estimates of the function's gradient.**

Knowing that the expectation of the gradient estimates equal the gradient.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Gradient Descent

**1951!** Robbins - Monroe: A Stochastic approximation Method

&amp;lt;img src=/assets/04/robins_monroe_1951.png style='width: 500px; margin: auto; float:right; '&amp;gt;


Soit une function \\(  f \\) que l'on souhaite approximer.
Sous certaines conditions sur  \\(  \alpha \\) et \\(\hat{\nabla} f\\) (gradient de f), alors \\(  \\)

$$  {\bf w}_{t+1} = {\bf w}_t - \alpha_t \hat{\nabla} f({\bf x}_t) $$
$$ {\bf w}_t -&amp;gt; f  $$

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

Dans notre contexte, la fonction \\(f\\) est une fonction de régression linéaire d'ordre N avec les coefficients \\(w_k \text{with} k \in [0..N]\\)

$$f(x) = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_N x_N $$

On veut donc minimiser l'erreur

$$ e = y - f(X) = t - W^T X $$

Cette equation admet une solution exacte que l'on a vu precedemment

$$\hat{W} = ()() $$

Au lieu de calculer la solution directement on va l'estimer par la methode du gradient:

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Methode du gradient

Equation

Neanmoins cela nécessite de calculer le gradient sur tous les échantillons disponibles a la fois. Pour un dataset grand, c'est couteux et long.

Donc on va utiliser le fait que sous certaines conditions

le gradient peut etre estimé par la moyenne des

The SGD algorithm is low on computation, has good convergence behavior and is applicable to many different situations through its many available variants. The SGD is implemented in a wide variety of languages (python, R, Java, scala, matlab, ...) and platforms (weka, rapidminer, ...) as well as online services big (Amazon ML, Google, Azure) and small (Dataiku, ....)

The idea of iterative stochastic approximation Robbins and Monro in 1951 in a seminal paper titled A Stochastic approximation Method

The literature related to the SGD algorithm is abundant. With the resurgence of depp learning and neural networks, no less than 6000 academic papers about Stochastic Gradient Descent were published in 2016 according to google scholar (from around a few 100s in the early 2000s).



According to the Gauss-Markov theorem, the model fit by the Ordinary Least Squares (OLS) is the least biased estimator of all possible estimators. In other words, it fits the data it has seen better than all possible models.

Calculer la solution a partir de l'equation ci dessus est couteuse en calcul

On va donc approximer la solution de facon iterative

* on choisit un vercteor W_0 pour initialiser l'algo
et a chaque iteration on corrige W par le gradient de la fonction de cout


see [raschka](https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html)
The cost function J(⋅), the sum of squared errors (SSE), can be written as:

The magnitude and direction of the weight update is computed by taking a step in the opposite direction of the cost gradient

where η is the learning rate. The weights are then updated after each epoch via the following update rule:

=&amp;gt; learning rate

~[gradient as ball](/assets/04/gradient_ball.png)

see file:///Users/alexis/amcp/packt-B05028/B05028_07_draft.html

In case of very large datasets, using GD can be quite costly since we are only taking a single step for one pass over the training set – thus, the larger the training set, the slower our algorithm updates the weights and the longer it may take until it converges to the global cost minimum

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

La dimenion stichastique consiste a actualiser le W non plus avec l'integralité du gradient sur toutes les donnees mais avec une estimation du gradient echantillon par echantillon.
Cela marche parce que dans notre contexte: E(gradient) = gradient

Instead, we update the weights after each training sample:

et on passe plusieurs fois sur le dataset en shufflant le dataset a chaque fois (epoch)

“stochastic” comes from the fact that the gradient based on a single training sample is a “stochastic approximation” of the “true” cost gradient

it has been shown that SGD almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)


## Mini-Batch Gradient Descent (MB-GD)
milieu entre le GD et le SGD
On utilise K echantillons pour estimer le gradient a chaque iteration

Accroitre K entraine

* on converge plus vite et plus directement. moins de zig zag
* on converge moins profondement. le biais est plus important.


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Outliers, detection and impact

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Skewness: Box cox and Kurtosis

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# resources
https://towardsdatascience.com/predicting-housing-prices-using-advanced-regression-techniques-8dba539f9abe



* Data Split
    train, test, valid http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/18
    k-fold http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/20
    http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/21
    * stratified k-fold http://127.0.0.1:4000/07-sampling-bias-variance-sgd.html#/22
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/4-biais-variance-gradient-stochastique</link>
        <guid isPermaLink="true">http://localhost:4000/4-biais-variance-gradient-stochastique</guid>
        
        
      </item>
    
      <item>
        <title>4) Gradient, Stochastique,  Biais, Variance</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Biais, variance et gradient stochastique&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt; avec scikit-learn&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* Régression logistique
* Approche statistique ou approche Machine Learning
* Métriques de classification:
    * TP, TN, FP, FN
    * Matrice de confusion
    * ROC-AUC
* One hot encoding
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
I: Rapide Rappel Scikit-learn
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# scikit-learn API
Simple et cohérente

1. Instancier un modèle, par exemple une regression linéaire:
    * ```from sklearn.Linear import LinearRegression```
    * ```mdl = LinearRegression( meta-params, loss function, ...)```
2. Entrainer le modèle
    * ```mdl.fit(X, y)```
3. Obtenir des prédictions sur de nouvelles données
    * ```y_hat = mdl.predict(Nouveaux échantillons}```.
    * ```y_hat = mdl.predict_proba(Nouveaux échantillons)```
4. Obtenir un score
    * ```mdl.score(X,y)```

&lt;/div&gt;
&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# paramétrer les modèles

* les meta-paramètres du modèle: \\(\alpha, \epsilon, \beta, \gamma, \cdots \\)
* la regularisation: *penalty, l1, l2*
* la fonction de cout: *loss*
* la gestion des itérations: *max_iter, n_iter*
* data pre-processing: *normalize, shuffle, valeurs manquantes*

[![Lin reg params](/assets/04/scikit_linear_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)
[![SGD reg params](/assets/04/scikit_sgd_reg_params.png)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:40%&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    ## Lab: Titanic la suite
    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:60%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Aujourd'hui

* Gradient et Gradient Stochastique;
* Décomposition Biais - Variance

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:35%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:50%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Methode du gradient

Soit une function \\(  f \\) dont on souhaite trouver le minimum.

Pour \\(  \alpha \\)  assez petit et si f est dérivable

alors

$$  {\bf w}_{t+1} = {\bf w}_t - \alpha_t \hat{\nabla} f({\bf w}_t) $$

\\( {\bf w}_t  \\) converge vers le minima de \\(f\\)

On a

$$ f(\mathbf {w}\_{0})\geq f(\mathbf {w}\_{1}) \geq f(\mathbf {w}\_{2})\geq \cdots , $$


=&amp;gt; *fastest  decrease obtained for the direction of the negative gradient of f*
    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/04/700px-Gradient_descent.svg.png width=350px&amp;gt;
&amp;lt;img src=/assets/04/700px-Gradient_ascent_contour.png width=350px&amp;gt;
&amp;lt;img src=/assets/04/Gradient_ascent_surface.png width=350px&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En python

On veut trouver le minimum de la fonction  \\( f(x) =   \\)

    cur_x       = 6 # The algorithm starts at x=6
    gamma       = 0.01 # step size multiplier
    max_iters   = 10000 # maximum number of iterations
    iters       = 0 #iteration counter
    precision   = 0.00001
    previous_step_size = 1

    # dérivée de la fonction à minimiser

    fct = lambda x: 4 * x**3 - 9 * x**2
    x = []
    while (previous_step_size &amp;gt; precision) &amp;amp; (iters &amp;lt; max_iters):
        x.append(cur_x)
        prev_x = cur_x
        cur_x -= gamma * fct(prev_x)
        print(cur_x, previous_step_size)
        previous_step_size = abs(cur_x - prev_x)
        iters+=1

    print(&quot;Le minimum est {:.4f}&quot;, cur_x)

    print(&quot;En x = {:.4f}, la valeur de la fonction est {:.4f}  &quot;.format(cur_x, fct(cur_x)) )


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Stochastic Gradient

The general idea is to aproximate an unknown function through iterations of **unbiased estimates of the function's gradient.**

Knowing that the expectation of the gradient estimates equal the gradient.

On va prendre les erreurs successives entre les vraies valeurs et leur estimée comme estimation du gradient!

Cela marche parce que dans notre contexte: E(gradient) = gradient

&amp;lt;img src=/assets/04/robins_monroe_1951.png style='width: 500px; margin: auto; float:right; '&amp;gt;

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Fonction de cout, forme générale

On cherche a minimiser la fonction \\( f(x) = w^T x + b   \\)

$$ E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w) $$

where

* \\(L\\) is a loss function that measures model (mis)fit
* \\(R\\) is a regularization term  that penalizes model complexity
* \\(\alpha\\) is a non-negative hyperparameter.

En fonction du choix de \\(L\\) et de \\(R\\) on obtient différent algorithmes

=&amp;gt; http://scikit-learn.org/stable/modules/sgd.html

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Gradient stochastique
&amp;lt;img src=/assets/04/gradient_ball.png height=300px&amp;gt;

* low on computation,
* has good convergence behavior
* The SGD is implemented in a wide variety of languages (python, R, Java, scala, matlab, ...) and platforms (weka, rapidminer, ...) as well as online services big (Amazon ML, Google, Azure) and small (Dataiku, ....)


The literature related to the SGD algorithm is abundant. With the resurgence of deep learning and neural networks, no less than 6000 academic papers about Stochastic Gradient Descent were published in 2016 according to google scholar (from around a few 100s in the early 2000s).


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# SGD

* learning rate
    * tuning
    * fixe
    * adaptif

* epoch and shuffling

## Mini-Batch Gradient Descent (MB-GD)
milieu entre le GD et le SGD
On utilise K echantillons pour estimer le gradient a chaque iteration

Accroitre K entraine

* on converge plus vite et plus directement. moins de zig zag
* on converge moins profondement. le biais est plus important.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;



Instead, we update the weights after each training sample:

et on passe plusieurs fois sur le dataset en shufflant le dataset a chaque fois (epoch)

“stochastic” comes from the fact that the gradient based on a single training sample is a “stochastic approximation” of the “true” cost gradient

it has been shown that SGD almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: Biais - Variance
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=assets/04/bias-variance-targets.png style='width:500px;'&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:50%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Décomposition Biais - Variance
L'erreur de prédiction peut etre décomposée en 2 termes

$$
\text{Erreur totale} = \text{Erreur du biais} + \text{Erreur de la variance}
$$

**Biais**: la différence entre les predictions du modele et la valeur cible. Le biais mesure la performance du modèle, la distance entre les predictions et les valeurs cibles.

* **Underfitting**: Un biais important indique que le modele n'arrive pas  à comprendre les données qui lui sont fournies

**Variance**: Il s'agit là de la variabilité des prédictions entre différentes *réalisations* du modèle pour un échantillon donné.

La variance mesure la sensibilité du modèle aux données d'apprentissages

* **Overfitting**: Une forte erreur de variance indique que le modèle ne pourra pas extrapoler ses prédictions sur des nouvelles données.
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width : 45%;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* **Biais**: L'espérance de  l'erreur de prédiction

* **Variance**: Variance des prédictions.

    &lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;float:left; width:45%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Décomposition Biais - Variance
L'erreur quadratique est définie par:

$$ \operatorname{MSE}( \hat{y} ) = \frac {1}{n} \sum_{i=1}^{n}( \hat{y_i}-y_i )^2 = \mathbb{E} \big[ (\hat{y} - y)^2   \big] $$


Et on peut réécrire cette équation de la façon suivante:

$$ \operatorname{MSE}( \hat{y} )   =\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] + \left(\mathbb{E}(\hat{y})-y\right)^2 $$

Soit
$$ \operatorname{MSE}( \hat{y} )= \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2 $$

avec

$$ \operatorname{Var}(\hat{y}) =  \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right] $$
$$ \operatorname{Bias}(\hat{y},y)  = \mathbb{E}(\hat{y})-y  $$

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div data-markdown=&quot;&quot;&gt;
# Le calcul
&lt;/div&gt;
$$
\begin{align} \mathbb{E}((\hat{y}-y)^2)&amp;amp;=
 \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})+\mathbb{E}(\hat{y})-y\right)^2\right]
\\ &amp;amp; =
\mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2 +2\left((\hat{y}-\mathbb{E}(\hat{y}))(\mathbb{E}(\hat{y})-y)\right)+\left( \mathbb{E}(\hat{y})-y \right)^2\right]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\mathbb{E}\Big[(\hat{y}-\mathbb{E}(\hat{y}))(\overbrace{\mathbb{E}(\hat{y})-y}^{\begin{smallmatrix} \text{This is} \\  \text{a constant,} \\ \text{so it can be} \\  \text{pulled out.} \end{smallmatrix}}) \,\Big] + \mathbb{E}\Big[\,\overbrace{\left(\mathbb{E}(\hat{y})-y\right)^2}^{\begin{smallmatrix} \text{This is a} \\  \text{constant, so its} \\  \text{expected value} \\  \text{is itself.} \end{smallmatrix}}\,\Big]
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+2\underbrace{\mathbb{E}(\hat{y}-\mathbb{E}(\hat{y}))}_{=\mathbb{E}(\hat{y})-\mathbb{E}(\hat{y})=0}(\mathbb{E}(\hat{y})-y)+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \mathbb{E}\left[\left(\hat{y}-\mathbb{E}(\hat{y})\right)^2\right]+\left(\mathbb{E}(\hat{y})-y\right)^2
\\ &amp;amp; = \operatorname{Var}(\hat{y})+ \operatorname{Bias}(\hat{y},y)^2
\end{align}

$$
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
![underfit - overfit](/assets/04/underfit_overfit.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# En résumé
Pour réduire l'erreur quadratique, il faut minimiser à la fois le **biais** et la **variance**.

* Biais important &amp;lt;-&amp;gt; sous estimation - Underfitting
    * le modele n'est pas bon
    * On obtient de mauvais score

* Variance importante &amp;lt;-&amp;gt; Overfitting
    * Le modele est trop **sensible** au training dataset
    * pouvoir d'extrapolation faible, mauvaises performances sur des nouvelles données.

Mais comment détecter l'overfit ?

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Underftiing


* Ajouter des predicteurs
* Rendre le modele plus complexe
* Attenuer la regularisation.
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

# Overfitting

* Reduire le nombre de predicteurs
* Utiliser plus de données d'apprentissage
* Accroitre la reguilarisation
* Moyenner plusieurs modèles

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
IV: K-fold cross validation
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# split: train vs test dataset

* On entraine le modele sur un **dataset de training**
* Mais le but est d'avoir un modele capable de traiter des **données nouvelles**, données qu'il n'a pas vu auparavant
* et on veut surtout éviter **l'Overfit**: *Le modele est trop **sensible** au training dataset*

Donc on va mettre de coté  une partie des données comme données nouvelles: dataset de test.

Typiquement : une répartition  80/20 ou 70/30

&amp;lt;img src=/assets/04/split_train_test.png style='width: 500px;  margin:auto; ' &amp;gt;

Et c'est en évaluant le modele sur les données de test que l'on va pouvoir détecter l'overfit

&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    [scikit example 1](http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve)

    [scikit example 2](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py)
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;float:left; width:70%&quot;&gt;
&lt;div data-markdown=&quot;&quot;&gt;

# Comment détecter l'overfit ?
### Courbes d'apprentissages - Learning curves

* On met de coté un set d'apprentissage (20% des données)
* on entraine le modele sur un nombre croissant d'echantillons (10%, 20%, ...)
* Pour chaque réalisation on calcule
    * l'erreur sur le set d'apprentissage
    * l'erreur sur le set de test

* En accroissant le set d'apprentissage, le modele a de plus en plus d'info, le modele apprends le set d'apprentissage. On espere que ca va lui permettre de traiter aussi les données sur le set de test.

Ce que l'on observe:
* avec un set d'apprentissage petit, les 2 erreurs sont grandes
* avec plus de données, l'erreur d'apprentissage décroit
    * si l'erreur sur le set de test ne décroit pas: **overfit**!

Si l'erreur sur le set de test ne décroit pas, alors cela veut dire que le modele n'est pas capable d'extrapoler sur des nouvelles données

Note: si l'erreur ne décroit pas sur le set de training en premier lieu, alors cela veut dire que le modele est mauvais, que l'erreur de biais est forte.

&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Validation croisée
Si on a peu de données, le split train - test *gaspille* des données pour le test. Données qui pourraient etre utile pour l'apprentissage du modele.

=&amp;gt; on va alterner le découpage train - test, 80% - 20%,

C'est la validation croisée et plus particulièrement **K-FOLD cross validation**

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# K-fold cross-validation

&amp;lt;img src=/assets/04/k-fold-cross-validation.png style='width: 600px;  margin:auto; float:right;' &amp;gt;

1. Mélanger le dataset
2. Puis découper le dataset en K (5) parties
3. Faire K (5) experiences:
    * apprentissage sur 1,2,3,4 et evaluation sur 5
    * apprentissage sur 1,2,3,**5** et evaluation sur **4**
    * ...
    * apprentissage sur 2,3,4,5 et evaluation sur **1**

La moyenne des scores obtenus ainsi est plus robuste qu'un score obtenu sur un unique découpage.

[K-fold cross validation - scikit-learn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Autres méthodes de validation croisée

* **Stratified K-Fold**: (classification) Chaque subset conserve la distribution des classes. Utile lorsque la repartition des classes est déséquilibrée.
* **Leave one out**:  Chaque échantillon est utilisé à son tour comme echantillon de test. Tous les autres sont laissé dans le set d'apprentissage.
* **Shuffle cross validation**: Decoupage aléatoire avec remise en place. rien n'oblige à fixer le découpage au début.


### scikit-learn
* [cross_val_score](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_digits.html#example-exercises-plot-cv-digits-py)
* [cross validation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Overfit - Biais - Variance SGD

Demo sur cars avec SGD

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: Gradient, Stochastique
&amp;lt;/p&amp;gt;
&amp;lt;p class=mitop&amp;gt;
Stochastic Gradient Descent
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/4bis-biais-variance-gradient-stochastique</link>
        <guid isPermaLink="true">http://localhost:4000/4bis-biais-variance-gradient-stochastique</guid>
        
        
      </item>
    
      <item>
        <title>5)  Arbres, Random Forests et XGBoost</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Arbres, Random Forests et XGBoost&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;Ensembling, Bagging, Boosting&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent
* SGD
* Biais Variance

&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
I: Arbres de décision
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Exemple arbre de décision sur Iris dataset
&amp;lt;img src =/assets/05/L12-tree-iris.png width=900px&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Mais

* **high overfitting** for over-complex trees that do not generalise the data well.
* Decision trees can be **unstable** because small variations in the data might result in a completely different tree being generated.
* no globally optimal decision tree

&lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Avantages

Robustes, rapides et interpretables


* Simple to understand and to interpret. Trees can be visualised.
* Requires little data preparation. (missing values, scaling, dummy variables, ...)
* Can handle both numerical and categorical data.
* Possible to validate a model using statistical tests.
* Uses a white box model. An observed situation can simply be explained by boolean logic.


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Biais - Variance

## Deep

* Low bias, high variance
* Overfitting

## Shallow (short)

* High bias, low variance
* Underfitting


* Shallow decision trees have high bias and low variance.
* Deep decision trees have low bias and high variance.

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Lab: Controlling the tree

Lab: [Simple Decision tree](https://github.com/alexperrier/gads/blob/master/12_decision_trees/py/L12%20Simple%20Decision%20Tree%20-%20Iris%20dataset.ipynb)


Set these params to control the tree complexity

* **max_depth** (pruning): The maximum depth of the tree

* **min_samples_split**: The minimum number of samples required to split an internal node
* **min_samples_leaf**: The minimum number of samples required to be at a leaf node.
* **max_features**: The number of features to consider when looking for the best split


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
II: Bootstrap
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Example

* mean de a

1000 fois on sample a

on tire 200 echantillons avec remplacement

    a = [1,2,3,-1,-2,-3,4,-2,-2]

    m = []
    for i in range(1000):
        m.append(np.mean(random.choice(a, size = 200, replace = True)))

    plt.boxplot(m)

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Bootstrap

**Echantilloner avec remplacement**

N samples au total, N est petit (par ex. ~&amp;lt; 10)

* Comment estimer la moyenne de ces echantillons ?
* Est ce que la moyenne arithmetique classique est un bon estimateur ?

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Essayer sur le titanic

sklearn.tree.DecisionTreeClassifier

* Creer les train et test sets
* comme baseline: arbre de decision simple, not pruned, quel accuracy sur le test set ?
* maintenant prendre 20 arbres, en limitant la taille a 2 niveaux
* pour chaque arbre, predire les probas des echantillons du test set
* puis moyenner les proba et utiliser le resultat pour determiner la classe predite.
* quel accuracy sur le test set ?

=&amp;gt; 20 arbres biaisés valent mieux qu'un arbre *non contraint* qui overfit

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Bagging for trees

Bagging stands for Bootstrap Aggregation,


* Generate B different **bootstrapped** training data sets.
* Train a new tree on each training set

The predictions of all the trees are averaged

=&amp;gt; significantly reduces over fitting for deep trees

=&amp;gt; does it also reduce bias for shallow trees ?

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Bagging Classifier


The key intuition of Bagging is that it reduces the variance of your model class.

http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier


A **Bagging classifier** is an ensemble meta-estimator that fits **base classifiers** each on random subsets (bootstrapped) of the original dataset

The final prediction is aggregated from the models individual predictions  to form a final prediction.

* **voting**: most predicted class
* **averaging**: average of predictions (regression) or predicted probabilities (classification)

Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.

* base_estimator: The base model, decision tree by default, could also be another simple n

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
II: Random Forests
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Random Forests

Extension of the bootstrapping to features

* In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.

* In addition, when splitting a node during the construction of the tree, the split that is picked is the best split among **a random subset of the features**.

=&amp;gt; The bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.

(see RandomForestClassifier and RandomForestRegressor classes),

A Random Forest is a generalization of Bagging that is specific to DTs. At each branch in the decision tree, Random Forest training also subsamples the features in addition to the training examples. Intuitively, this process further de-correlates the individual trees, which is good for Bagging, since the main limitation of Bagging is that bootstrapping is not the same as drawing fresh samples from the true data distribution.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Out Of Bag - OOB

When boostrapping, in each experiment will use only approx. 2/3rd of the available samples.

Which leaves 1/3rd that we can use to estimate the validation error of each tree.

This is called OOB Out of Bag error.

It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Feature importance

* When the *max_features &amp;lt; total number of features*.

    =&amp;gt; Some features are left out of the splitting decision in each node.

* Relative Feature importance can be deduced from the delta in MSE associated to the features included vs left out.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Titanic

Quelles sont les variables les plus importantes ?

# Cars

Quelles sont les variables les plus importantes ?


etc ...
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
III: XGBoost
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Adaboost



&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Gradient boosting

Keep an overall predictor that is the (weighted) average of a bunch of models.
Train first model on original training data, and initialize overall predictor as just this single model.
Assess the error of the the overall predictor and modify the training data the focus on areas of high error.
For AdaBoost, this means re-weighting the data points so that poorly modeled data points get higher weight.
For Gradient Boosting, this means redefining the supervised prediction target to be some kind of residual between the ground truth and the overall predictor.
Train a new model on the modified training data, and add to the overall predictor.
Repeat Steps 3 &amp;amp; 4.

A Gradient Boosting will take a different approach. It will start with a (usually) not very deep tree (sometimes a decision stump - a decision tree with only one split) and will model the original target. Then it takes the errors from the first round of predictions, and passes the errors as a new target to a second tree. The second tree will model the error from the first tree, record the new errors and pass that as a target to the third tree. And so forth. Essentially it focuses on modelling errors from previous trees. GB is one of the best algorithms available today and it’s almost always outperforming RF on most datasets I’ve tried.

Notice how RF runs trees in parallel, thus making it possible to parallelize jobs on a multiprocessor machine. GB instead uses a sequential approach.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

Random forests bags models, while boosting iteratively averages them with respect to error. XGBoost extends boosting by imposing regression penalties similar to elastic net.


One can interpret boosting as trying to minimize the bias of the overall predictor. So when you use boosting, you’re incentivized to use shallow decision trees because they have low variance and high bias. Using high variance base models in boosting runs a much higher risk of overfitting than approaches like Bagging.
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Fri, 05 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/5-arbres-random-forest-xgboost</link>
        <guid isPermaLink="true">http://localhost:4000/5-arbres-random-forest-xgboost</guid>
        
        
      </item>
    
      <item>
        <title>6) Texte &amp; Word2Vec</title>
        <description>&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt; Arbres, Random Forests et XGBoost&amp;lt;/p&amp;gt;
&lt;p style=&quot;font-size:28px;&quot;&gt;Ensembling, Bagging, Boosting&lt;/p&gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right;&quot;&gt;
    &lt;h1&gt;Questions ?&lt;/h1&gt;

    &lt;div data-markdown=&quot;&quot;&gt;
    &amp;lt;img src=/assets/04/questions_04.gif&amp;gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div data-markdown=&quot;&quot;&gt;
# Cours précédent

&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
I: Text Mining - NLP
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Corpus

## texte brut

* forums, réseaux sociaux (peu structuré)
* plus structuré: discours, news, articles, emails, ...
* plus ou moins long: livres, articles scientifiques, abstracts, ...

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Applications

Simple et directe

* prediction, classification, identification
    * binaire: spam
    * multiclass: sujet du document

Non supervisée

* topic modeling

Avancée: productive

* Résumé
* Traduction automatique
* Chatbots

voir les nouvelles fonctionnalités de gmail

Optimiste (aka fumeuse): interpretation

*  Sentiment analysis


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Librairies python

* spacy.io
* nltk
* gensim



Nombreuses librairies open source en R, Java, ...

# Resources

* Livre: Speech and Language Processing https://web.stanford.edu/~jurafsky/slp3/

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Cloud

* AWS Comprehend
* Google NLP
* Speech to text

# Arabic

* Stanford NLP: https://nlp.stanford.edu/projects/arabic.shtml
* Deep learning for Arabic NLP https://www.sciencedirect.com/science/article/pii/S1877750317303757


    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Transformations

* lemmatization,

    * la voiture est grande
    * Je suis sur un grand bateau

    * est, suis =&amp;gt; etre
    * grande, grand =&amp;gt; grand


* tokens, bi-grams
* stopwords: je, tu, il, et, me, sa, son, mais, donc, par, ....


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Numeriser le texte

Comment passer d'un texte libre a une matrice numérique ?

Approche **Bags of words**

## Tf-idf

Pour un mot donné dans un corpus de plusieurs documents

* Fréquence dans un document / frequence des mots dans les autres documents

* tf-idf means term-frequency times inverse document-frequency

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Cosine distance
&amp;lt;img src=/assets/06/cosine_similarity.png&amp;gt;

# Spacy
https://spacy.io/usage/vectors-similarity

    from gensim.models import Word2Vec

    #loading the downloaded model
    model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)

    #the model is loaded. It can be used to perform all of the tasks mentioned above.

    # getting word vectors of a word
    banana = model['banana']

    #performing king queen magic
    print(model.most_similar(positive=['woman', 'king'], negative=['man']))

    #picking odd one out
    print(model.doesnt_match(&quot;breakfast cereal dinner lunch&quot;.split()))

    #printing similarity index
    print(model.similarity('apple', 'orange'))
    print(model.similarity('car', 'orange'))


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
## Word2vec et Glove

* Approche très recente qui associe un vecteur de grande dimension (128, 256, ...) a des milliers de mots

* Comme on a des vecteurs on a une distance entre les mots. Cosine distance

* Corpus original: Wikipedia

* Capture du *sens* du mot

    * Reine - femme = Roi - homme
    * Rabat - capitale = Paris - capitale




    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# Word2vec
Word2vec is not a single algorithm but a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model.

* CBOW work is that it tends to predict the probability of a word given a context
* Skip – gram : to predict the context given a word

http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

# vs Glove

https://www.quora.com/How-is-GloVe-different-from-word2vec

* word2vec is a &quot;predictive&quot; model, predict word / context + context / word


* GloVe is a &quot;count-based&quot; model.

dimensionality reduction on the co-occurrence counts matrix.

    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
&amp;lt;img src=/assets/06/skip_gram_net_arch.png&amp;gt;

Predictive models learn their vectors in order to improve their predictive ability of Loss(target word | context words; Vectors), i.e. the loss of predicting the target words from the context words given the vector representations. In word2vec, this is cast as a feed-forward neural network and optimized as such using SGD,

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;div style=&quot;float:right; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;

* http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html


    &lt;/div&gt;
&lt;/div&gt;
&lt;hr class=&quot;vline&quot; /&gt;
&lt;div style=&quot;float:left; width:45%;  &quot;&gt;
    &lt;div data-markdown=&quot;&quot;&gt;
# TF-IDF - sklearn

The most intuitive way to do so is to use a bags of words representation:

Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).

For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary.

    &lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Topic modeling

&amp;lt;img src=/assets/06/lsa_decomposition_example_03.png&amp;gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
&amp;lt;div class=centerbox&amp;gt;
&amp;lt;p class=top&amp;gt;
II: Lab : text classification sur bbc dataset
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/section&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/6-text-classification</link>
        <guid isPermaLink="true">http://localhost:4000/6-text-classification</guid>
        
        
      </item>
    
      <item>
        <title>Lesson 18 - R &amp; AWS ML</title>
        <description>&lt;section data-background-color=&quot;#000&quot;&gt;
    &lt;h1 class=&quot;white&quot; style=&quot;border-top: thin solid #DDD;border-bottom: thin solid #DDD;&quot;&gt;
        &lt;img src=&quot;assets/ga_logo_black.png&quot; style=&quot;float:left;top:0px;&quot; /&gt;
        General Assembly
    &lt;/h1&gt;
    &lt;p class=&quot;big_title&quot;&gt;18. R and AWS ML&lt;/p&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Previously

* Decomposition, Trending and seasonality
* Stationarity, Dickey-Fuller test, Autocorrelation, partial auto correlation
* AR(p), MA9q), ARIMA(p,d,q)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Today

* R
    * R studio
    * packages
    * essential code
    * time series forecasting

* AWS ML

    * classification problem

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# R Packages

* caret:  The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations.
* CRAN - Package e1071 - for Naive Bayes, SVMs, Latent Class Analysis
* CRAN - Package randomForest - Random Forests
* CRAN - Package gbm - Generalized boosting models

and many more https://www.quora.com/What-are-the-best-machine-learning-packages-in-R
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# R Code

Directory:

* getwd() get working directory
* setwd() set working directory

Packages:

* install.packages('forecast')
* library('forecast')
* installed.packages()

help:

* ?c
* ?forecast

data:

* **x &amp;lt;- 1 instead of x = 1**
* a &amp;lt;- c(1,2,3,4,5)
* vector &amp;lt;- c(apple = 1, banana = 2, &quot;kiwi fruit&quot; = 3, 4)
* df &amp;lt;- read.csv('iris.csv')

Dataframe:

* dim(df)
* head(df)
* colnames(df)
* summary(df)

plot:

* plot(df)
* hist(df$sepal_length)
* boxplot(df$sepal_width)
* plot(df$sepal_length, df$petal_width)
* qqnorm(df$sepal_length)
* qqline(df$sepal_length)

ML on Iris

* iris$target[iris$Species == 'setosa'] &amp;lt;- 1
* iris$target[iris$Species == 'versicolor'] &amp;lt;- 2
* iris$target[iris$Species == 'virginica'] &amp;lt;- 3

* fit &amp;lt;- lm(target ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris )
* coefficients(fit)
* summary(fit)


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Time series analysis in R

dataset: [Stock Prices for Tui Ag from the Frankfurt Stock Exchange.](https://www.quandl.com/data/FSE/TUI1_X-Tui-TUI1_X)


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Holt Winters

The additive Holt-Winters prediction function (for time series with period length p) is

Yhat[t+h] = a[t] + h * b[t] + s[t - p + 1 + (h - 1) mod p],

where a[t], b[t] and s[t] are given by

a[t] = α (Y[t] - s[t-p]) + (1-α) (a[t-1] + b[t-1])

b[t] = β (a[t] - a[t-1]) + (1-β) b[t-1]

s[t] = γ (Y[t] - a[t]) + (1-γ) s[t-p]

The multiplicative Holt-Winters prediction function (for time series with period length p) is

Yhat[t+h] = (a[t] + h * b[t]) * s[t - p + 1 + (h - 1) mod p],

where a[t], b[t] and s[t] are given by

a[t] = α (Y[t] / s[t-p]) + (1-α) (a[t-1] + b[t-1])

b[t] = β (a[t] - a[t-1]) + (1-β) b[t-1]

s[t] = γ (Y[t] / a[t]) + (1-γ) s[t-p]


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Amazon Machine Learning

https://console.aws.amazon.com/

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Amazon Machine Learning

* Datasources
* Models
* Evaluations and results

&lt;/section&gt;
</description>
        <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/18-r-and-aws-ml.html</link>
        <guid isPermaLink="true">http://localhost:4000/18-r-and-aws-ml.html</guid>
        
        
      </item>
    
      <item>
        <title>Lesson 17 - Time Series Modeling</title>
        <description>&lt;section data-background-color=&quot;#000&quot;&gt;
    &lt;h1 class=&quot;white&quot; style=&quot;border-top: thin solid #DDD;border-bottom: thin solid #DDD;&quot;&gt;
        &lt;img src=&quot;assets/ga_logo_black.png&quot; style=&quot;float:left;top:0px;&quot; /&gt;
        General Assembly
    &lt;/h1&gt;
    &lt;p class=&quot;big_title&quot;&gt;17. Time Series Modeling and Forecasting&lt;/p&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Previously

* Time series
* Smoothing
* Trending and seasonality
* Stationarity, Dickey-Fuller test
* Autocorrelation, partial auto correlation
* Forecasting 101 &amp;amp; Metrics

### Today

* ARMA Modeling
* ARMA Modeling

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Types of TS

[Different types of TS](/assets/17/stationary_time_series.png)


* White noise

* Trend

* Seasonality

* Cycle

* **Seasonality != Cycle**

*Some cases can be confusing — a time series with cyclic behaviour (but not trend or seasonality) is NOT stationary. That is because the cycles are not of fixed length, so before we observe the series we cannot be sure where the peaks and troughs of the cycles will be.*


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Stationarity

Required for most models.

* Mean is constant \\(  \ \ \ \operatorname{E}[Y\_{t}]  = \mu \\)
* Variance is constant  \\(  \ \ \ \operatorname{Var}(Y\_t) = \operatorname{E}[ (Y\_{t} - \mu)^2 ]  = \sigma^2 \\)
* Autocorrelation is lag dependent

$$  R(\tau) = \frac {\operatorname{E} [ (Y\_{t}-\mu )(Y\_{t+\tau }-\mu )]} {\sigma ^{2}} $$

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Testing for stationarity

### Dickey-Fuller test

**Null hypothesis**: TS is NOT stationary

[Demo in Notebook](https://github.com/alexperrier/gads/blob/master/17_ts2/py/L17%20Time%20Series%20Demo.ipynb)

* Dickey Fuller test does not test for seasonality stationarity

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# (Partial) Autocorrelation

### ACF
Correlation between \\( Y\_t \\) and  \\( Y\_{t-s} \\)

### PACF

Correlation between \\( Y\_t \\) and  \\( Y\_{t-s} \\)

*  without the cumulative correlation between \\( Y\_t \\) and \\( Y\_{t-1} \cdots Y\_{t-s+1}  \\)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Simple Forecasting

* next sample = last sample
$$ \hat{Y}\_{t+1} = Y\_{t} $$
* Moving average $$ \hat{Y}\_{t+1} = \frac{1}{n} \sum^{n}\_{i = 0} Y\_{t-1-i}  $$
* EWMA $$ \ \ \ \ \hat{Y}\_{t}= \alpha \cdot Y\_{t}+(1-\alpha )\cdot \hat{Y}\_{t-1} $$
* Linear Regression OLS

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Today

* Transform TS  into a Stationary TS
* Test is the TS predictable? Is it **white noise**?
* Decomposition: Trend, Seasonality, Residuals
* Is my forecast reliable?
* Is the Dow Jones a **Random Walk**?
* AutoRegressive modeling (AR) and Moving Average (MA)

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Differencing

Create a new TS by taking the difference shifted by 1

$$  X\_t =  Y\_t - Y\_{t-1} $$

! [Try it out](https://github.com/alexperrier/gads/blob/master/17_ts2/py/L17%20Time%20Series%20Demo.ipynb) on the milk production ts

What happens to the seasonality? to the trend?

What is the result of the Dickey Fuller test on the difference?
Is the difference series stationary?
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# White noise

### What is white noise?

Time series data that shows **no auto correlation** is called **white noise**.

Formally, \\( X(t) \\) is a white noise process if

* \\( E[X(t)]=0 \\)
* \\( E[X(t)^2]=\sigma^2  \\)
* and  \\( E[X(t)X(h)]=0 \ \  \text{for} \ \ t \neq h \\)

The autocorrelation matrix of a white noise TS is a diagonal matrix

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# How to detect white noise

### 1. ACF and PACF

Rule of thumb:

* A Time series is white noise if 95% of the spikes in the Auto-correlation Function lie within  \\( \pm \frac{2}{ \sqrt{N} } \\) with N the length of the time series.

=&amp;gt; Plot the PACF for the milk volume and difference TS and the tree rings series

Which one is a white noise?

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Testing for white noise: the Ljung-Box test

The Ljung–Box test may be defined as:

* H0: The data are independently distributed
* Ha: The data are not independently distributed; they exhibit serial correlation.

The test statistic is

$$ Q = n (n+2) \sum\_{k=1}^{h} \frac{ \hat{\rho }\_{k}^{2}}{n-k} $$

where

* n is the sample size,
* \\( \hat{\rho }\_{k} \\) is the sample autocorrelation at lag k,
* **h** is the number of lags being tested.
&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Testing for white noise: the Ljung-Box test

[Rule of thumb](http://robjhyndman.com/hyndsight/ljung-box-test/) for h

* h = 10 for non-seasonal data
* h = 2m for seasonal data, where m is the period of seasonality.
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Residual diagnostics on forecasting
A good forecasting method will yield residuals with the following properties:

* The residuals are uncorrelated: *If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.*

* The residuals have zero mean : *If the residuals have a mean other than zero, then the forecasts are biased.*

It is useful to also have the following two properties which make the calculation of prediction intervals easier

* The residuals have constant variance.
* The residuals are normally distributed.

These two properties make the calculation of **prediction intervals** easier

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Prediction Intervals

95% prediction interval: \\( \ \ \ \hat{Y\_{t}}  \pm 1.96 \sigma^2 \ \ \ \\)  with \\(\sigma \\)  an estimate of the standard deviation of the forecast distribution.

When the residuals are **normally distributed and uncorrelated** and when **forecasting one-step ahead**

=&amp;gt;  the standard deviation of the *forecast distribution* is almost the same as the standard deviation of the *residuals*.

When conditions are not met, there are more complex ways to estimate confidence intervals

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# TS Decomposition

### Additive Model
$$ Y\_t = S\_t + T\_t + E\_t $$

where  \\( S\_t \\)  is the seasonal component,   \\( S\_t \\) is the trend-cycle component and  \\( E\_t \\) is the residual


        import statsmodels.api as sm
        res = sm.tsa.seasonal_decompose(milk_prod.volume, model = 'additive')
        resplot = res.plot()

### Multiplicative Model
$$ Y\_t=S\_t \cdot T\_t \cdot E\_t $$


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Forecast with decomposition

* forecast seasonality, trend and residuals separately
* add back together

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# LAB:  IBM dataset
Consider the daily closing IBM stock prices (data set ibmclose).

https://datamarket.com/data/set/2322/ibm-common-stock-closing-prices-daily-17th-may-1961-2nd-november-1962#!ds=2322&amp;amp;display=line

* Produce some plots of the data in order to become familiar with it.
* Split the data into a training set of 300 observations and a test set of 69 observations.
* Try various simple methods to forecast the training set and compare the results on the test set.
* Which method did best?

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# LAB: House sales
https://datamarket.com/data/set/22q8/monthly-sales-of-new-one-family-houses-sold-in-th-e-usa-since-1973#!ds=22q8&amp;amp;display=line

Consider the sales of new one-family houses in the USA, Jan 1973 – Nov 1995 (data set hsales).

* Produce some plots of the data in order to become familiar with it.
* Split the data into a training set of 300 observations and a test set of 69 observations.
* Try various simple methods to forecast the training set and compare the results on the test set.
* Which method did best?

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Break 5mn
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Next session

* R
* SQL
* AWS Machine Learning
* ?

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# LAB Is the DJ a random walk?
Why you cannot beat the market

$$ \$\$\$ $$

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# What's a Random Walk
When the differenced series is white noise, the model for the original series can be written as

\\( y\_t−y\_{t−1}=e\_t \ \ \
or  \ \ \  y\_t= y\_{t−1}+e\_t \\)

A random walk model is very widely used for non-stationary data, particularly finance and economic data. Random walks typically have:

* long periods of apparent trends up or down
* sudden and unpredictable changes in direction.


http://python-for-signal-processing.blogspot.com/2014/04/random-walks-and-stumbles.html

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Notebook: The Dow Jones is a random walk
http://www.johnwittenauer.net/a-simple-time-series-analysis-of-the-sp-500-index/

* plot DJ
* plot diff
* transform with log
* plot rolling variance original + log
* plot diff of log =&amp;gt; stationary time series model of daily changes to the S&amp;amp;P 500 index
* lag variables scatter plot =&amp;gt; all centered and normal
* acf and pacf =&amp;gt; no correlation =&amp;gt; increment is white noise =&amp;gt; we have a random walk
* decomposition of diff =&amp;gt; look at the residuals white noise ?
* AR model, look at the residuals =&amp;gt; much smaler values predicted than actual changes


* look at histogram of residuals
    * skewed =&amp;gt; not great for confidence intervals
* autocorrelation plot of residuals
* test with Ljung-Box

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# AR(p) model
In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable.


$$ AR(1): \ \ \  X\_{t}= c + \varphi X\_{t-1} + \varepsilon\_{t} $$
$$ AR(p): \ \ \  X\_{t}= c + \sum\_{i=1}^{p} \varphi\_{i}X\_{t-i} + \varepsilon\_{t} $$


where
* \\( \varphi\_{i} \\) are the parameters of the model
* \\(  \varepsilon\_{t} \\) is a white noise process with zero mean and constant variance \\( \sigma\_{\varepsilon }^{2}\\)
* c is a constant

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Special cases
For an AR(1) model:

* When  \\( \varphi\_1 = 0, \ \ X\_t  \\) is equivalent to white noise.
* When  \\( \varphi\_1 = 1, \ \ X\_t  \\) is equivalent to a random walk.
* When  \\( \varphi\_1 = 0, c \neq 0 \ \ X\_t  \\)  is equivalent to a random walk with drift
* When \\( \varphi\_1 \lt 0 \ \ X\_t \\)  tends to oscillate between positive and negative values.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# MA(q) models
Rather than use past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.

Moving Average model of order q:

$$   X\_{t}=\mu + \varepsilon\_{t}+\theta\_{1}\varepsilon\_{t-1}+\cdots +\theta\_{q}\varepsilon\_{t-q} $$

where

* \\(\mu \\) is the mean of the series
*  \\(\theta\_{1} \cdots \theta\_{q}\\) are the parameters of the model
* \\( \varepsilon\_{t} \cdots \varepsilon\_{t-q} \\) are white noise error terms

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# ARIMA(p,d,q) model

We combine the AR(p) and the MA(q) and add i^th differencing

$$   X\_{t}=c + \sum\_{i=1}^{p} \varphi\_{i}X\_{t-i} + \varepsilon\_{t}+\theta\_{1}\varepsilon\_{t-1}+\cdots +\theta\_{q}\varepsilon\_{t-q} $$


We call this an ARIMA(p,d,q) model, where

* p: order of the autoregressive part;
* d: degree of first differencing involved;
* q: order of the moving average part.


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Estimating p, d, q

The *squirrel* approach

http://people.duke.edu/~rnau/411arim3.htm

The ML approach: Brute Force and Grid Search

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Criteria for model selection

* Akaike Information Criterion (AIC)
* Schwarz Bayesian Information Criterion (BIC)
* Hannan-Quinn Information Criterion (HQIC)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# White Noise - Normality test
### The Durbin Watson test

The Durbin-Watson statistic ranges in value from 0 to 4.

* A value near 2 indicates non-autocorrelation;

* A value toward 0 indicates positive autocorrelation;

* A value toward 4 indicates negative autocorrelation.

### Agostino and Pearson for normality

Null hypothesis: the sample comes from a normal distribution

        scipy.stats.normaltest(ts)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Lab on sunspots

[Wolf's Sunspot Numbers. 1700 – 1988](https://datamarket.com/data/set/22wg/wolfs-sunspot-numbers-1700-1988#!ds=22wg&amp;amp;display=line)

from https://bicorner.com/2015/11/16/time-series-analysis-using-ipython/
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Time Series Classification and Clustering

[Time Series Classification and Clustering](http://nbviewer.jupyter.org/github/alexminnaar/time-series-classification-and-clustering/blob/master/Time%20Series%20Classification%20and%20Clustering.ipynb)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

### Dickey Fuller test

http://stats.stackexchange.com/questions/44647/which-dickey-fuller-test-should-i-apply-to-a-time-series-with-an-underlying-mode
http://stats.stackexchange.com/questions/225087/seasonal-data-deemed-stationary-by-adf-and-kpss-tests



### Random Walk

http://python-for-signal-processing.blogspot.com/2014/04/random-walks-and-stumbles.html
http://fedc.wiwi.hu-berlin.de/xplore/tutorials/xegbohtmlnode39.html

### Ljung-Box test
[Thoughts on the Ljung-Box test](http://robjhyndman.com/hyndsight/ljung-box-test/)
http://stats.stackexchange.com/questions/18135/testing-normality-and-independence-of-time-series-residuals

https://www.otexts.org/fpp/2/6
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Train Test and Cross validation
https://www.otexts.org/fpp/2/5

&lt;/section&gt;
</description>
        <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/17-time-series-forecast.html</link>
        <guid isPermaLink="true">http://localhost:4000/17-time-series-forecast.html</guid>
        
        
      </item>
    
      <item>
        <title>Lesson 16 - Time Series</title>
        <description>&lt;section data-background-color=&quot;#000&quot;&gt;
    &lt;h1 class=&quot;white&quot; style=&quot;border-top: thin solid #DDD;border-bottom: thin solid #DDD;&quot;&gt;
        &lt;img src=&quot;assets/ga_logo_black.png&quot; style=&quot;float:left;top:0px;&quot; /&gt;
        General Assembly
    &lt;/h1&gt;
    &lt;p class=&quot;big_title&quot;&gt;16. Time series&lt;/p&gt;
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Previously

* Topic Modeling
* LDA, LSA
* Gensim, NLTK

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Today

![](assets/16/time-series-analysis.png)

* Time series
* Modeling
* Stationarity
* Trending and seasonality
* Forecasting 101

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Python

* Python Koans https://github.com/gregmalcolm/python_koans
* Hackerrank: https://www.hackerrank.com/domains

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Time Series [TS]

A time series is a series of data points listed in time order.

A sequence taken at successive equally spaced points in time.

A sequence of **discrete-time data**.

=&amp;gt; The **time interval** is key

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# Time Series [TS]

Any domain of applied science and engineering which involves temporal measurements.

* **IoT**
* **econometrics**
* mathematical finance / trading / markets
* intelligent transport and trajectory forecasting
* weather forecasting and Climate change research
* earthquake prediction, astronomy
* electroencephalography, control engineering, communications
* **signal processing**



&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# TS

* Modeling
* Forecasting
* Pattern detection
* Detection of a change in the parameters of a static or dynamic system

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Time vs Frequency domain: Fourier Transform

The Fourier transform decomposes a function of time (a signal) into the frequencies that make it up. A Fourier transform takes a time series or a function of continuous time, and maps it into a frequency spectrum.

* Jean-Baptiste Joseph Fourier, 1807 Treatise on the *propagation of heat in solid bodies*.

![](assets/16/sFFT.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# So many TS!

* [Great source of Time series](https://datamarket.com/data/list/?q=provider:tsdl)
* [Google trends](https://www.google.com/trends/explore?date=all&amp;amp;q=data%20science,data%20mining,Machine%20learning)

![](assets/16/TimeSeriesChart_1.jpg)


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# TS: Components

* [Trend](https://datamarket.com/data/set/22s6/weekly-closing-price-of-att-common-shares-1979#!ds=22s6&amp;amp;display=line):
    * Gradual long term evolution
    * Easiest to detect

* [Cycle](https://datamarket.com/data/set/235d/mean-daily-temperature-fisher-river-near-dallas-jan-01-1988-to-dec-31-1991#!ds=235d&amp;amp;display=line) or Seasonal Variation
    * Up and down repetitive movement
    * Repeats itself over a long period of time

* Random Variations
    * Erratic movements that do not follow a pattern
    * Not predictable

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# TS: Examples



* https://datamarket.com/data/set/22ox/monthly-milk-production-pounds-per-cow-jan-62-dec-75#!ds=22ox&amp;amp;display=line
* https://datamarket.com/data/set/22vd/quarterly-production-of-clay-bricks-million-units-mar-1956-sep-1994#!ds=22vd&amp;amp;display=line
* https://datamarket.com/data/set/235d/mean-daily-temperature-fisher-river-near-dallas-jan-01-1988-to-dec-31-1991#!ds=235d&amp;amp;display=line


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Loading and indexing in Pandas

Simple loading

        ts = pd.read_csv('../data/Dow-Jones.csv', parse_dates = ['Date'], infer_datetime_format = True)
        ts[ts.Date &amp;gt; '2010-01-01']

Make the date the index:

        ts = pd.read_csv('../data/Dow-Jones.csv', parse_dates=['Date'], index_col='Date', infer_datetime_format = True)
        ts['2010-12-31':'2010-01-01']
        ts[:'2010-01-01']


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Forecasting 101

* Smoothing the data
* Simple forecasting technique
* Tree rings dataset

        ts = pd.read_csv('../data/tree-rings.csv', parse_dates = ['year'], index_col = 'year', infer_datetime_format = True)

&lt;/section&gt;
&lt;section data-markdown=&quot;&quot;&gt;
# plot the tree rings

[Notebook - Smoothing and Forecast 101](https://github.com/alexperrier/gads/blob/master/16_time_series/py/Smoothing%20and%20Forecast%20101.ipynb)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Very simple forecasting technique

$$ \hat{Y}\_{n+1} = Y\_n $$

* load the ts
* create a new column 1 period gap

How good is that predictor?
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Ts metrics

Forecasting error: \\( e\_i=y\_i−\hat{y}\_i \\)


Metrics to compare TS techniques

* Mean Absolute Error: \\( MAE=mean(|e\_i|) \\)
* [Mean Absolute Deviation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mad.html): \\( MAD = \frac{1}{n} \sum\_{i=1}^{n}  | e\_i |  \\)
* Root mean squared error: \\( RMSE = \sqrt{  mean(e\_i^2)  } \\)
* Mean absolute percentage error: \\( MAPE = \frac{100}{n} \sum\_{i=1}^{n} \frac{ | e\_i | }{ y\_i } \\)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Moving Average


Smoothed over a window of n samples

$$ \begin{aligned} SMA = \frac{p\_{M} + p\_{M-1}+ \cdots +p\_{M-(n-1)}}{n} = \frac{1}{n} \sum\_{i=0}^{n-1}p\_{M-i}
\end{aligned} $$

and center

$$ \begin{aligned} SMA = \frac{p\_{M+n/2} + \cdots +   p\_{M+1} +   p\_{M} + p\_{M-1}+ \cdots +p\_{M-(n/2)}}{n} = \frac{1}{n} \sum\_{i=-\frac{n}{2}}^{\frac{n}{2}}p\_{M+i}
\end{aligned} $$



&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Moving Average
Use SMA to forecast

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Exponential weighted moving average


![](assets/16/exponential_moving_average_weights.png)

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Exponential weighted moving average
Introduce a Decay

The EWMA for a series Y may be calculated recursively:

* \\( S\_{1}=Y\_{1} \\)
* \\(      t&amp;gt;1, \ \ S\_{t}=\alpha \cdot Y\_{t}+(1-\alpha )\cdot S\_{t-1} \\)


Where:

* The coefficient α represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. A higher α discounts older observations faster.
* Yt is the value at a time period t.
* St is the value of the EWMA at any time period t.

http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.ewma.html
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Auto Correlation

A measure of how much is the current value influenced by the previous values in a time series.

Autocorrelation measures the linear relationship between lagged values of a time series.

        pandas autocorrelation_plot()
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;

# Partial Auto Correlation

Let's say you have a TS with a high correlation 1 sample apart

* 1) \\( Y\_{n+1}  \\) very correlated with \\( Y\_{n}  \\)
* 2) \\( Y\_{n}  \\) very correlated with \\( Y\_{n-1}  \\)
* etc ...

That correlation impacts the correlation between samples  that are further apart

* \\( Y\_{n+1}  \\) appears very correlated with \\( Y\_{n-1}  \\) since 1_ and 2)

The partial autocorrelation function (PACF) can be thought of as the correlation between two points that are separated by some number of periods n, BUT with the effect of the intervening correlations removed.

The PACF removes the intervening correlation between the samples

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Statsmodels
http://statsmodels.sourceforge.net/stable/tsa.html

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Properties: Stationarity

The concept of **stationarity** is very important in modeling non-independent data

Many results which holds for independent random variables (law of large numbers, central limit theorem, ...) hold for stationary random variables.


A time serie is said to be stationnary if

* **Constant mean**: The mean of the series should not be a function of time rather should be a constant.

* **Constant variance**: Homoscedasticity: The variance of the series should not a be a function of time.

* **Fix lagged covariance**: The covariance of the i th term and the (i + m) th term should only depend on i and not m.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Properties: Stationarity
**Constant mean**

![](assets/16/Mean_nonstationary.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Properties: Stationarity

**Constant variance**

![](assets/16/Var_nonstationary.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Properties: Stationarity
**Fix lagged covariance**

![](assets/16/Cov_nonstationary.png)
&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Properties: Stationarity
* Tree rings?
* Dow Jones?
* Average Water Temp?


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Stationarity test

* Dickey-Fuller Test: This is one of the statistical tests for checking stationarity.
* Here the null hypothesis is that the TS is non-stationary.

* The test results comprise of a Test Statistic and some Critical Values for difference confidence levels.

* If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary.

    For instance: the Dickey-Fuller test statistic is less than the 10% critical value, thus the TS is stationary with 90% confidence.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Dickey Fuller test

* from statsmodels.tsa.stattools import adfuller
* dftest = adfuller(timeseries, autolag='AIC')

Returns: ['Test Statistic','p-value','#Lags Used','Number of Observations Used']

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# How to make a TS stationary
There are 2 major reasons behind non-stationarity of a TS:

1. Trend – varying mean over time. On average, the number of passengers was growing over time.

2. Seasonality – variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals.

So remove trend and seasonality and apply prediction on resulting TS

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Trend

Estimating &amp;amp; Eliminating Trend

One of the first tricks to reduce trend can be transformations such as log, square root, cube root, etc. Lets take a log transform here for simplicity:

### Notebook

* Load the [Monthly milk production](https://datamarket.com/data/set/22sn/monthly-milk-production-pounds-per-cow-jan-62-dec-75-adjusted-for-month-length#!ds=22sn&amp;amp;display=line) dataset
* Fit a linear regression line
* Remove Moving Average
* Fit a linear regression line
* plot before after

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Differencing
Differencing – taking the difference with a particular time lag

One of the most common methods of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the observation at a particular instant with that at the previous instant. This mostly works well in improving stationarity.

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Decomposing
Both trend and seasonality are modeled separately

        from statsmodels.tsa.seasonal import seasonal_decompose
        decomposition = seasonal_decompose(ts_log)

        trend = decomposition.trend
        seasonal = decomposition.seasonal
        residual = decomposition.resid

        # Then plot
        plt.subplot(411)
        plt.plot(ts_log, label='Original')
        plt.legend(loc='best')
        plt.subplot(412)
        plt.plot(trend, label='Trend')
        plt.legend(loc='best')
        plt.subplot(413)
        plt.plot(seasonal,label='Seasonality')
        plt.legend(loc='best')
        plt.subplot(414)
        plt.plot(residual, label='Residuals')
        plt.legend(loc='best')
        plt.tight_layout()

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Transformations
https://www.otexts.org/fpp/2/4

* Mathematical: Box plot
* Calendar Adjustments
    monthly milk production on a farm,

* Population adjustments
    per 1000
    you remove the effect of population changes by considering number of beds per thousand people
* Inflation adjustments
        for money related series
         a price index is used. If ztzt denotes the price index and ytyt denotes the original house price in year tt, then xt=yt/zt∗z2000xt=yt/zt∗z2000 gives the adjusted house price at year 2000 dollar values. Price indexes are often constructed by government agencies. For consumer goods, a common price index is the Consumer Price Index (or CPI).


&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Residuals

A good forecasting method will yield residuals with the following properties:

* The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.

* The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties.

* The residuals have constant variance.

* The residuals are normally distributed.

Look at

* Histograms of residuals
* QQ plots
* ACF of the residuals
* Box-Pierce test
* Ljung-Box

&lt;/section&gt;

&lt;section data-markdown=&quot;&quot;&gt;
# Links

* [Datasets](https://datamarket.com/data/list/?q=provider:tsdl)
* [Seasonal ARIMA with Python](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/)
* [Time Series Analysis using iPython](https://bicorner.com/2015/11/16/time-series-analysis-using-ipython/)
* [Complete guide to create a Time Series Forecast ](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)
* [A Complete Tutorial on Time Series Modeling in R](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/)
* [A Simple Time Series Analysis Of The S&amp;amp;P 500 Index](http://www.johnwittenauer.net/a-simple-time-series-analysis-of-the-sp-500-index/)
* [Identifying the order of differencing in an ARIMA model](http://people.duke.edu/~rnau/411arim2.htm)

* [fecon235 : Computational data tools for financial economics](https://github.com/rsvp/fecon235)

Others:

* https://www.otexts.org/fpp/2/5

&lt;/section&gt;

</description>
        <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/16-time-series.html</link>
        <guid isPermaLink="true">http://localhost:4000/16-time-series.html</guid>
        
        
      </item>
    
  </channel>
</rss>
