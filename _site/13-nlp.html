<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>
      
        Lesson 13 - Natural Language Processing | Emines - UM6P - Data Science
      
    </title>

    <meta name="author" content="" />

    <!-- Description -->
    
      <meta name="description" content="none" />
    

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="/reveal.js/css/reveal.css"/>
    
      <link rel="stylesheet" href="/reveal.js/css/theme/white.css" id="theme"/>
    
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="/reveal.js/lib/css/zenburn.css"/>

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? '/reveal.js/css/print/pdf.css' : '/reveal.js/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />

    <link rel="canonical" href="http://localhost:4000/13-nlp.html">

    <script src="https://cdn.mathjax.org/mathjax/2.4-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>


    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">
      <div class="slides">
        <section data-background-color="#000">
    <h1 class="white" style="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;" />
        General Assembly
    </h1>
    <p class="big_title">13. Natural Language Processing</p>
</section>

<section data-markdown="">
# Lesson 13
## LEARNING OBJECTIVES


* NLP Processing
* Feature extraction
* Pipeline in scikit
* Hashing Trick

</section>

<!-- Prework and review -->
<section data-background-color="#DA0A13">
    <h1>Lesson #13</h1>
    <p class="big_title">Review of Lesson 12</p>
</section>
<section data-markdown="">
# Last Lesson Review

* Random Forests
* SVM

</section>

<!-- Today -->
<section data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class="big_title">NLP</p>
</section>

<section data-markdown="">

# HUGE DOMAIN

* Classify documents,
* Detect (depression, ...)
* Translate, Summarize, ....
* Named entity recognition
* Survey analysis
* Automatic Speech Recognition (Siri, Alexa, ...)
* Unsupervised: infer sentiment or topics, find associations and links, summarize

</section>

<section data-markdown="">
# Feature Extraction

From raw text (html) to vectors used by ML libraries


* Tokenization
* Counting occurences of words / frequencies
* Numerical representation of documents

</section>

<section data-markdown="">
# Tokenize

### Tokens
Tokenization is the process of breaking a stream of text up into syllables, letters, words, n-grams, phrases, symbols, or other meaningful elements called tokens. The list of tokens becomes input for further processing such as parsing or text mining.


### n-grams

Contiguous sequence of n items from a given sequence of text or speech

* unigram
* bigram
* n-gram

</section>

<section data-markdown="">
# Vectorizing text

First we need to transform the raw text into vectors of numerical values

Extract numerical features from text

* tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.
* counting the occurrences of tokens in each document.
* weighting with diminishing importance tokens that occur in the majority of samples / documents.

</section>

<section data-markdown="">
# Text feature extraction

Features and samples are defined as follows:

* each individual token occurrence frequency is treated as a feature.
* the vector of all the token frequencies for a given document is considered a multivariate sample.
* A corpus of documents can thus be represented by
    * a matrix with one row per document
    * one column per token (e.g. word) occurring in the corpus.

</section>

<section data-markdown="">
# Bag of Words

Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.

This approach (tokenization, counting and normalization) is called the **Bag of Words
or Bag of n-grams representation**.

</section>

<section data-markdown="">
# NLP Vocabulary

* **Corpus**: ensemble of documents
* **Token**: the element, the word, the atom
* **Unigrams, bi-grams, n-grams**: sequence of 1, 2 or n words taken as the basic element
* **Stopwords**: small words that are discarded as not meaningful: *a, an, the, my, get, ...*

</section>

<section data-markdown="">
# Lemmatization and Stemming

Lemmatization and stemming are special cases of normalization.
They identify a canonical representative for a set of related word forms. The purpose of both stemming and lemmatization is to reduce morphological variation

'to walk' may appear as 'walk', 'walked', 'walks', 'walking'.

### Stemming

Crude direct approach that chops off the endings of the word to limit variations

'walk', 'walked', 'walks', 'walking' =&gt; walk

[NLTK Stemmer](http://textanalysisonline.com/nltk-snowball-stemmer)
</section>

<section data-markdown="">

# Lemmatization

Returns the closest meaningful *root* word.

In [NLTK](http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization)

        wordnet_lemmatizer.lemmatize(‘is’, pos=’v’)
        ’be’
        wordnet_lemmatizer.lemmatize(‘are’, pos=’v’)
        ’be’

        wordnet_lemmatizer.lemmatize(‘are’, pos=’n’)
        ’are’

[NLTK Lemmatizer](http://textanalysisonline.com/nltk-wordnet-lemmatizer)

</section>

<section data-markdown="">
# POS - Tagging

Given a text, assigns roles to each word: noun, verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection.

[Demo](http://parts-of-speech.info/)

</section>

<section data-markdown="">
# Libraries

* [NLTK](http://www.nltk.org/)
* [Spacy](spacy.io/) (POS)
* [Gensim](https://radimrehurek.com/gensim/) (Latent Dirichlet Allocation - Topic Modeling)

and
* [Scikit](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)

</section>

<section data-markdown="">
# Demo 1: Vectorizing a text

[Vectoring a text](https://github.com/alexperrier/gads/blob/master/13-nlp/py/L13%20N1%20Feature%20Extraction.ipynb)

</section>

<section data-markdown="">
# Normalizing: TF-IDF

What if you have several documents and a word is very frequent in just one of them.
It's relative frequency should not be important.

TF-IDF: term frequency–inverse document frequency

### TF:
* Boolean *frequency*: \\(f\_{t,d} = 1\\) if t occurs in d and 0 otherwise;
* Logarithmically scaled frequency: \\( f\_{t,d} = 1 + log f\_{t,d}\\), or zero if \\(f\_{t,d} = 0\\)  is zero;
* Agmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document:
$$ \mathrm{tf} (t,d)=0.5+0.5\cdot {\frac {f\_{t,d}}{\max\{f\_{t',d}:t'\in d\}}} $$

</section>

<section data-markdown="">
# TF-IDF
### IDF
**inverse document frequency** is a measure of how much information the word provides, that is, whether the term is common or rare across all documents

IDF = the total number of documents divided by the number of documents containing the term, and then taking the logarithm of that quotient.

$$  \mathrm{idf}(t, D) =  \log \frac{N}{ 1+ |\{d \in D: t \in d\}|}$$

* N: total number of documents in the corpus \\( N={|D|}\\)
* \\( |\{d\in D:t\in d\}| \\)  : number of documents with term \\( t \\qquad \mathrm {tf} (t,d)\neq 0\\).

</section>

<section data-markdown="">
# TF-IDF
Then tf–idf is calculated as
$$ \mathrm{tf-idf}(t,d,D)=\mathrm{tf} (t,d) \cdot \mathrm{idf}(t,D) $$

### in Scikit
[TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)

</section>

<section data-markdown="">
# Hashing Trick

* Documents don't have the same length
* You may not have all the documents available (streaming)
* Too many words =&gt; too many dimensions
* highly dimensional very sparse input matrix.

=&gt; Dimensionality reduction with **The Hashing Trick**



</section>
<section data-markdown="">
# Hashing Trick
Ex: Transform any sentences into value 0 to 99

Simple Hash function:

* *a* to 1, *b* to 2, *c* to 3 and so on, up to *z* being 26
* result modulo 100


Sentence: "Beware the hobby that eats"

        * (beware) 2 + 5 + 23 + 1 + 18 + 5 +
        * (the) 20 + 8 + 5 +
        * (hobby) 8 + 15 + 2 + 2 + 25 +
        * (that) 20 + 8 + 1 + 20 +
        * (eats) 5 + 1 + 20 + 19
        * = 233

result = 33
</section>
<section data-markdown="">
# Hashing Trick

Good hash function

* Uniformity: translate your input into each number in its output range with same probability
* Cascading: a small change in your input data will cause a big change in the output

=&gt; limit collisions
=&gt; lose interpretability

* [How do you build a language model with a million dimensions?](http://blog.someben.com/2013/01/hashing-lang/)
* [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)


</section>

<section data-markdown="">
# Install a few things

* conda install BeautifulSoup4
* nltk

</section>

<section data-markdown="">
# 1st Lab predicting sentiment


[N2 Imdb Reviews](https://github.com/alexperrier/gads/blob/master/13-nlp/py/L13%20N2%20Sentiment%20Prediction.ipynb)

* Get the data
* Clean up the text: remove punctuation, html markup, numbers, stop words, tokenize
    and return one long paragraph per document
* Process all the reviews, store into an array
* Train a RF
* Assess on the test set
* AUC curve

Use scikit CountVectorizer and Try to improve the score

</section>

<section data-markdown="">
# Pipeline in scikit

The ability to chain operations

For instance:
* HashingVectorizer
* Classifier

Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.

[Example](http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html)

</section>

<section data-markdown="">
# 2nd lab: classification

[Classification on the twenty Newsgroup dataset](https://github.com/alexperrier/gads/blob/master/13-nlp/py/L13%20N3%20Text%20Classification.ipynb)

</section>

<section data-markdown="">
# Similarity between documents

### Cosine similarity

Given two vectors of attributes, A and B, the cosine similarity, cos(θ), is represented using a dot product and magnitude as

![](assets/13/cosine_similarity.png)

[ex in scikit learn](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)
</section>

<section data-markdown="">
#  Clustering
* with HashingVectorizer
* Silhouette Coefficient

http://scikit-learn.org/stable/auto_examples/text/document_clustering.html

</section>

<section data-markdown="">
# Links

* [Standford NLP Group](http://nlp.stanford.edu/)
* [Kaggle - Bag of Words](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words)
* [How do you build a language model with a million dimensions?](http://blog.someben.com/2013/01/hashing-lang/)
</section>

<section data-markdown="">
# Questions


</section>

      </div>
    </div>

    <script src="/reveal.js/lib/js/head.min.js"></script>
    <script src="/reveal.js/js/reveal.js"></script>
    <script>
      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        width: 1200,
        controls: true,
        progress: true,
        history: true,
        center: false,
        slideNumber: true,
        
          transition: 'slide',
        

        // Optional reveal.js plugins
        dependencies: [
          { src: '/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: '/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: '/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: '/reveal.js/plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: '/reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: '/reveal.js/plugin/notes/notes.js', async: true }
        ]
      });

    </script>

  </body>
</html>
